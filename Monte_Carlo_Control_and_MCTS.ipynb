{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eecc88c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'git+https://github.com/sarah-keren/multi-taxi'\"\n"
     ]
    }
   ],
   "source": [
    "!pip install -q 'git+https://github.com/sarah-keren/multi-taxi'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88a017c5",
   "metadata": {},
   "source": [
    "# Our Problem\n",
    "The problem that we will focus on today is to make a taxi move towards a single passenger and to pick it up, without dropping it at its destination."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f954d24b",
   "metadata": {},
   "source": [
    "![Taxi_domain_show](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/Taxi_domain_show.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "503b615e",
   "metadata": {},
   "source": [
    "The only guidance that we will provide to our agent is a **reward function** that prizes the agent for picking up the passenger and that penalizes the agent for taking too many steps to reach the goal.\n",
    "\n",
    "In particular, the reward function is defined as follows:\n",
    "- +100 points for a successful drop-off\n",
    "- -5 point for every bad pick-up\n",
    "- -2 points for hitting an obstacle (e.g. a wall)\n",
    "- -1 for every step taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7716d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_taxi import Event\n",
    "\n",
    "customized_reward = {\n",
    "    Event.PICKUP: 100,\n",
    "    Event.BAD_PICKUP: -5,\n",
    "    Event.HIT_OBSTACLE: -2,\n",
    "    Event.STEP: -1\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb119675",
   "metadata": {},
   "source": [
    "# Our Tools\n",
    "In our case we are **not** given access to a model, i.e. we don't know the transition function $ T: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S} $ and the reward function $ R: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S} $, where $\\mathcal{S}$ is the set of states and $\\mathcal{A}$ is the set of actions. \n",
    "\n",
    "We can only interact with the environment by taking actions and observing the resulting state and reward.\n",
    "This approach is thus based on Monte Carlo simulations and it is called **model-free**.\n",
    "\n",
    "## Monte Carlo Simulation\n",
    "To understand how Monte Carlo simulations work, let's see it in action.\n",
    "First, we will create a helper function which will allow us to create an environment by specifying the position of the taxi and the passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9818b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_taxi import multi_taxi_v0\n",
    "from multi_taxi.world.maps import DEFAULT_MAP, BIG_MAP\n",
    "\n",
    "def new_environment_creator(taxi_pos, passenger_pos, \n",
    "                            num_taxis=1,num_passengers=1, \n",
    "                            pickup_only=True, can_see_other_taxi_info=False, \n",
    "                            domain_map=DEFAULT_MAP,reward_table=customized_reward):\n",
    "    \"\"\"\n",
    "    A helper function to setup a new environment with predefined taxi and passenger locations\n",
    "    :param taxi_pos: The location of the taxi\n",
    "    :param passenger_pos: The location of the passenger\n",
    "    :param num_taxis: The number of taxis in the environment (default: 1)\n",
    "    :param num_passengers: The number of passengers in the environment (default: 1)\n",
    "    :param pickup_only: Whether the passenger can be dropped off or not (default: True)\n",
    "    :param can_see_other_taxi_info: Whether the taxi can see the other taxis' locations (default: False)\n",
    "    :param domain_map: The map of the environment (default: DEFAULT_MAP)\n",
    "    :param reward_table: The reward table of the environment (default: customized_reward)\n",
    "    :return: The new environment\n",
    "    \"\"\"\n",
    "    new_env = multi_taxi_v0.env(num_taxis=num_taxis, num_passengers=num_passengers, pickup_only=pickup_only, domain_map=domain_map,\n",
    "                                can_see_other_taxi_info=can_see_other_taxi_info,reward_table=reward_table, \n",
    "                                render_mode='human', allow_arrived_passengers_on_reset=False)\n",
    "    new_env.reset()\n",
    "\n",
    "    state = new_env.state()\n",
    "    state.taxis[0].location = taxi_pos\n",
    "    state.passengers[0].location = passenger_pos\n",
    "    new_env.unwrapped.set_state(state)\n",
    "\n",
    "    return new_env\n",
    "\n",
    "\n",
    "# Create the environment from which we will run the Monte Carlo Simulations\n",
    "taxi_position = (0,0)\n",
    "passenger_position = (3,3)\n",
    "env = new_environment_creator(taxi_position, passenger_position)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e787dcf4",
   "metadata": {},
   "source": [
    "Next, we will create a function that will allow us to simulate a single episode, i.e. a single run of the environment, by taking random actions until the episode ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fdfb4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from copy import deepcopy\n",
    "\n",
    "def run_monte_carlo_simulation(environment, gamma, max_number_of_steps: int =100_000, verbose: bool = False, render: bool = False):\n",
    "    \"\"\"\n",
    "    Runs a Monte Carlo simulation on a given environment\n",
    "    :param environment: The environment on which to run the simulation\n",
    "    :param gamma: The discount factor\n",
    "    :param max_number_of_steps: The maximum number of steps to run the simulation for (default: 100_000)\n",
    "    :param verbose: Whether to print the results of the simulation (default: False)\n",
    "    :param render: Whether to render the environment (default: False)\n",
    "    :return: The accumulated reward\n",
    "    \"\"\"\n",
    "    # Copy the environment so we don't change the original\n",
    "    env = deepcopy(environment)\n",
    "\n",
    "    number_of_actions = len(env.unwrapped.get_action_map('taxi_0').values())\n",
    "    accumulated_reward = 0\n",
    "    # Run the simulation for at most max_number_of_steps steps\n",
    "    for step in range(max_number_of_steps):\n",
    "        # Choose a random action\n",
    "        action = np.random.choice(number_of_actions)\n",
    "        # Take the action\n",
    "        env.step(action)\n",
    "        # Get the last state\n",
    "        obs, reward, done, trunc, info = env.last()\n",
    "        # Accumulate the reward\n",
    "        accumulated_reward += reward * (gamma ** step)\n",
    "        if render:\n",
    "            clear_output(wait=True)\n",
    "            if verbose:\n",
    "                print(f\"Step {step}: Accumulated reward: {accumulated_reward}\")\n",
    "            env.render()\n",
    "        # If the simulation is done, stop\n",
    "        if done:\n",
    "            break\n",
    "    del env\n",
    "\n",
    "    return accumulated_reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6cb3f56",
   "metadata": {},
   "source": [
    "Now we are ready to simulate a single episode.\n",
    "\n",
    "Try to run the next cell multiple times and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d184b8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 90: Accumulated reward: -97.69748053388537\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 91, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-97.69748053388537"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "run_monte_carlo_simulation(env, gamma=GAMMA, render=True, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7a3d1af",
   "metadata": {},
   "source": [
    "It is possible to notice that some simulations are really long and some are short. This shows the **variance** to which Monte Carlo simulations are exposed. This variance will be propagated to our estimates and it is the reason why we will need to run many Monte Carlo simulations to obtain a good estimate.\n",
    "\n",
    "Another important thing that we need to pay attention to is the impact of the **discount factor** $\\gamma$. Try to see what happens as you lower the value of $\\gamma$.\n",
    "\n",
    "Run the next cell multiple times and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe49c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 533: Accumulated reward: -9.719267545561829\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 534, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-9.719267545561829"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAMMA = 0.70\n",
    "run_monte_carlo_simulation(env, gamma=GAMMA, render=True, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6402daae",
   "metadata": {},
   "source": [
    "As you can see, with a **lower value** of $\\gamma$ the actions taken after a **high number of steps** have a very **limited impact** on the accumulated reward. This is because the reward is discounted by a factor of $\\gamma$ at each step. For this reason, the agent will care less about the actions taken after a high number of steps, limiting the horizon of its planning. For example, the agent will not care too much if it reached the passenger in 1_000 steps or in 1_100 steps. This will lead to a more **short-sighted** behaviour. \n",
    "\n",
    "Now we will see how to use Monte Carlo simulations to solve our problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba05337f",
   "metadata": {},
   "source": [
    "# Monte Carlo Control\n",
    "\n",
    "The most immediate way to exploit Monte Carlo simulations to solve our problem is to use them to estimate the **q-value function** $Q(s,a)$, i.e. the expected reward that we will get by executing action $a$ in the state $s$.\n",
    "\n",
    "## Algorithm Structure\n",
    "Monte Carlo Control follows the paradigm of Generalised Policy Iteration (GPI), which is composed of two main steps:\n",
    "1. **Policy Evaluation** (Monte Carlo Estimation for Action Values):\n",
    "   - Use Monte Carlo simulations to estimate $Q(s,a)$\n",
    "2. **Policy Improvement** (Greedy Policy Improvement):\n",
    "   - Improve the policy $\\pi$ by acting greedily with respect to $Q(s,a)$\n",
    "3. Repeat until convergence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b2b713c",
   "metadata": {
    "id": "4b2b713c"
   },
   "source": [
    "![monte_carlo_control_schema](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/Monte_Carlo_Control_small.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b0d407c",
   "metadata": {},
   "source": [
    "On top of this basic algorithm, we will add a **decaying $\\epsilon$-greedy policy** to allow the agent to explore the environment.\n",
    "\n",
    "Here is the complete algorithm:\n",
    "\n",
    "P.S. Ignore the line \"Unless the pair $S_t, A_t$ appears in $S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}$\". This is just a technicality that can be ignored (this is a detail of the first-visit version of Monte Carlo Contorl, the every-visit version, which is the one we are going to use, does not check if this condition is verified)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20a23c14",
   "metadata": {},
   "source": [
    "# Monte Carlo Control algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beef176e",
   "metadata": {},
   "source": [
    "![monte_carlo_control_algorithm](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/Monte_carlo_control_algorithm_book.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82d8ee6f",
   "metadata": {},
   "source": [
    "Now that we have seen the pseudocode, let's see how to implement it.\n",
    "\n",
    "We have chosen to make the agent run a certain number of episodes with which the policy is updated and then the agent will take a step in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1929d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "class MonteCarloControl:\n",
    "    def __init__(self, state, gamma:float = 0.9, train_episode_number:int=1_000,\n",
    "                 max_steps_per_episode:int = 1_000) -> None:\n",
    "        '''\n",
    "        :args state: environment with the initial state\n",
    "        :args gamma: discount factor\n",
    "        :args train_episode_number: number of episodes used to train the agent (it is only used in the function train)\n",
    "        :args max_steps_per_episode: maximum number of steps \n",
    "        '''\n",
    "        self.state = state\n",
    "        self.Q = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.N = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.visited = set()\n",
    "        self.episode_number = 0\n",
    "        self.gamma = gamma\n",
    "        self.train_episode_number = train_episode_number\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "\n",
    "    def _policy(self, state, epsilon = None):\n",
    "        \"\"\"\n",
    "        Compute the action to take in a given state using the epsilon-greedy policy\n",
    "        :param state: The state in which to take the action\n",
    "        :param epsilon: The epsilon value to use for the epsilon-greedy policy (default: 1/sqrt(episode_number + 1))\n",
    "        :return: The action to take\n",
    "        \"\"\"\n",
    "        # epsilon-greedy policy\n",
    "        if epsilon is None:\n",
    "            epsilon = 1/np.sqrt(self.episode_number + 1)\n",
    "        else:\n",
    "            epsilon = epsilon\n",
    "        number_of_actions = len(self.state.unwrapped.get_action_map('taxi_0').values())\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(number_of_actions)\n",
    "        else:\n",
    "            # convert state to use as key in Q\n",
    "            state = state.unwrapped.state().taxis[0].location\n",
    "            \n",
    "            if len(self.Q[state]) == 0:\n",
    "                action = np.random.choice(number_of_actions)\n",
    "            else:\n",
    "                action = max(self.Q[state], key=self.Q[state].get)\n",
    "        return action\n",
    "    \n",
    "    def _update(self, episode):\n",
    "        \"\"\"\n",
    "        Update the Q values using the episode\n",
    "        :param episode: The episode to use to update the Q values\n",
    "        \"\"\"\n",
    "        G = 0\n",
    "        for i, (state, action, reward) in enumerate(reversed(episode)):\n",
    "            G = (self.gamma**i) * G + reward\n",
    "            # convert state to use as key in Q\n",
    "            state = state.unwrapped.state().taxis[0].location\n",
    "            self.visited.add(state)\n",
    "            self.N[state][action] += 1\n",
    "            self.Q[state][action] += (G - self.Q[state][action]) / self.N[state][action]\n",
    "    \n",
    "    \n",
    "    def _train(self, starting_state):\n",
    "        \"\"\"\n",
    "        Generate an episode from the starting state and update the Q values\n",
    "        :args starting_state: environment with the initial state\n",
    "        \"\"\"\n",
    "        self.episode_number += 1\n",
    "        current_episode_state = deepcopy(starting_state)\n",
    "        episode = []\n",
    "        for _ in range(self.max_steps_per_episode):\n",
    "\n",
    "            action = self._policy(current_episode_state)\n",
    "            old_state = deepcopy(current_episode_state)\n",
    "            current_episode_state.step(action)\n",
    "            obs, reward, done, trunc, info = current_episode_state.last()\n",
    "            episode.append((old_state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "        self._update(episode)\n",
    "    \n",
    "    def _step(self, current_state, render:bool, step_number:int):\n",
    "        \"\"\"\n",
    "        Make a step in the environment using the current policy\n",
    "        :args current_state: environment with the current state\n",
    "        :args render: if True render the environment after the step (default False)\n",
    "        :args verbose: if True print the action executed and the Q values (default False)\n",
    "        \"\"\"\n",
    "        action = self._policy(current_state, epsilon=0)\n",
    "        old_state = current_state.unwrapped.state().taxis[0].location\n",
    "        current_state.step(action)\n",
    "        obs, reward, done, trunc, info = current_state.last()\n",
    "        if render:\n",
    "            time.sleep(1)\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Step number: {step_number}\")\n",
    "            print(f\"Action executed: {current_state.unwrapped.get_action_meanings('taxi_0')[action]}\")\n",
    "            print(\"Q values:\")\n",
    "            for act, action_name in current_state.unwrapped.get_action_meanings('taxi_0').items():\n",
    "                print(f\"{action_name}: {round(self.Q[old_state][act], 3)}\")\n",
    "            current_state.unwrapped.render()\n",
    "        return current_state, done\n",
    "\n",
    "    def run_and_update(self, number_of_simulated_episodes:int=100, max_number_of_steps:int=150, render:bool=False)-> int: \n",
    "        \"\"\"\n",
    "        Continuosly train the policy for number_of_simulated_episodes and then make a step until the simulation is done\n",
    "        or until max_number_of_steps is reached\n",
    "        :args number_of_simulated_episodes: number of episodes used to train the agent during each step (default 100)\n",
    "        :args max_number_of_steps: maximum number of steps allowed (default 150)\n",
    "        :return: the number of steps executed to reach the end of the simulation\n",
    "        \"\"\"\n",
    "        current_state = deepcopy(self.state)\n",
    "        steps = 0\n",
    "        for i in range(1,max_number_of_steps+1):\n",
    "            steps = i\n",
    "            # TRAINING STEP\n",
    "            self.episode_number = 0\n",
    "            for _ in range(number_of_simulated_episodes):\n",
    "                self._train(current_state)\n",
    "            \n",
    "            # ACTUAL STEP\n",
    "            current_state, done = self._step(current_state, render=render, step_number=i)\n",
    "            if done:\n",
    "                break\n",
    "        return steps\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7ad491d",
   "metadata": {},
   "source": [
    "Let's see the agent in action.\n",
    "First, instantiate the agent. Try to play with GAMMA and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8aa1c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = new_environment_creator(taxi_pos=(0,0), passenger_pos=(3,3))\n",
    "GAMMA = 0.99\n",
    "monte_carlo_control = MonteCarloControl(env, gamma=GAMMA, max_steps_per_episode=1_000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b426569",
   "metadata": {},
   "source": [
    "Run multiple times the next cell to see how the agent behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "420dd37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step number: 7\n",
      "Action executed: pickup\n",
      "Q values:\n",
      "south: 39.996\n",
      "north: 54.098\n",
      "east: 33.555\n",
      "west: 59.944\n",
      "pickup: 99.0\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 7, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte_carlo_control.run_and_update(number_of_simulated_episodes=10, render=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7a76d6e",
   "metadata": {},
   "source": [
    "### Considerations\n",
    "As you can see Monte Carlo Control behaves really well! Indeed it often **learns** **the optimal policy** or a policy that is really close to the optimal one (the optimal policy takes 7 steps).\n",
    "\n",
    "If so, are we done? Can we always use Monte Carlo Control to solve any problem?\n",
    "\n",
    "Unfortunately, the answer is no. Monte Carlo Control is not always applicable. In particular, it needs to store all the visited states and actions in memory. This is not always possible. For example, if we are playing a game like chess or Go where the state space and the action space are too big to fit in memory, we cannot store the Q-table that contains of all the states and actions.\n",
    "\n",
    "For this reason, we will need to use a different algorithm, which is more scalable and that collects trajectories in a wiser way: **Monte Carlo Tree Search**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10cac94c",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d2b174d",
   "metadata": {
    "id": "4d2b174d"
   },
   "source": [
    "# The 4 steps of MCTS\n",
    "\n",
    "MCTS consists of four main steps: selection, expansion, simulation, and backpropagation.\n",
    "\n",
    "1. **Selection.** Select a leaf node using **tree policy**.\n",
    "2. **Expansion.** **Add children** to the selected leaf using unexplored actions\n",
    "3. **Rollout.** From the selected child **simulate** an **episode** using the **rollout policy**\n",
    "4. **Backpropagation**. **Update** the average **value** of the nodes starting **from** the selected **child** up **to** the **root** using the results of the rollout episode\n",
    "    - ATTENTION: No values are saved for the states and actions visited by the rollout policy beyond the tree! \n",
    "\n",
    "![mcts_4_steps](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/mcts_4_steps.png)\n",
    "\n",
    "MCTS **repeats** this cycle until **no time** is left (starting at the root node each time). **Finally**, MCTS **chooses** the **action** to make from the root node."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05be701a",
   "metadata": {
    "id": "05be701a"
   },
   "source": [
    "# Rollout policy and Tree policy\n",
    "\n",
    "* **Rollout policy**: a simple policy generating actions in the **simulated trajectories**\n",
    "\n",
    "* **Tree policy**: policy for traversing the tree and **selecting** a **child node** that is most promising according to a selection policy, which balances exploration and exploitation of the search space. \n",
    "    * examples : $\\epsilon$-greedy, UCT\n",
    "    \n",
    "    * $ \\text{UCT} = \\underbrace{Q}_{\\textbf{Exploitation}} + \\underbrace{C \\sqrt{\\frac{\\log(N)}{n}}}_{\\textbf{Exploration}} $\n",
    "   \n",
    "   Where: \n",
    "   - $Q$ is the average Q-value of the considered state-action pair\n",
    "   - $C$ is a constant that balances exploration and exploitation\n",
    "   - $N$ is the total number of times that the current node has been visited\n",
    "   - $n$ is the number of times that the considered child node has been visited\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b13865a9",
   "metadata": {},
   "source": [
    "Below is MCTS implementation.\n",
    "MonteCarloTreeSearchNode class implements all the functionality.\n",
    "\n",
    "**best_action** is responsible for running the simulations and returning the best move to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4fc385bf",
   "metadata": {
    "id": "4fc385bf"
   },
   "outputs": [],
   "source": [
    "class MonteCarloTreeSearchNode():\n",
    "    def __init__(self, state, n_simulations=100, parent=None, parent_action=None, gamma = 0.9):\n",
    "        # environment holding the current state\n",
    "        self.state = state\n",
    "        # None for the root node, otherwise it is equal to the node it is derived from.\n",
    "        self.parent = parent\n",
    "        # None for the root node, otherwise it is equal to the action which its parent carried out.\n",
    "        self.parent_action = parent_action \n",
    "        # Contains all possible actions from the current node.\n",
    "        self.children = []\n",
    "        # Number of times current node is visited.\n",
    "        self.number_of_visits = 0 \n",
    "        self.average_reward = 0\n",
    "        # Set of all of the possible actions\n",
    "        self._untried_actions = self._get_legal_actions()\n",
    "        self.terminal_state = False\n",
    "        self.gamma = gamma\n",
    "        # Number of loops with the four stages\n",
    "        self.n_simulations = n_simulations\n",
    "    \n",
    "    def run_and_update(self, max_number_of_steps:int = 10_000, render:bool=False)-> int:\n",
    "        \"\"\"\n",
    "        Run Monte Carlo Tree Search for at most max_number_of_steps steps.\n",
    "        :args max_number_of_steps: maximum number of steps allowed (default 10_000)\n",
    "        :args render: if True render the environment after the step (default False)\n",
    "        \"\"\"\n",
    "        if render:\n",
    "            self.state.unwrapped.render()\n",
    "        steps = 0\n",
    "        selected_node = self._best_action()\n",
    "        for i in range(1,max_number_of_steps+1):\n",
    "            steps = i\n",
    "            selected_node = selected_node._best_action()\n",
    "            if render:\n",
    "                time.sleep(1)\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Step number: {i}\")\n",
    "                print(f\"Action executed: {self.state.unwrapped.get_action_meanings('taxi_0')[selected_node.parent_action]}\")\n",
    "                print(\"Q values:\")\n",
    "                for child, action_name in zip(selected_node.children, list(self.state.unwrapped.get_action_meanings('taxi_0').values())):\n",
    "                    print(f\"{action_name}: {round(child.average_reward, 3)}\")\n",
    "                selected_node.state.unwrapped.render()\n",
    "            \n",
    "            if selected_node.terminal_state:\n",
    "                break\n",
    "        \n",
    "        return steps\n",
    "    \n",
    "    def _best_action(self):\n",
    "        \"\"\"\n",
    "        Execute the four stages to find the best action from the current state.\n",
    "        \"\"\"\n",
    "        for _ in range(self.n_simulations):\n",
    "            node = self._tree_policy()\n",
    "            reward = node._rollout()\n",
    "            node._backpropagate(reward)\n",
    "\n",
    "        return self._best_child(c_param=0.)\n",
    "\n",
    "    def _tree_policy(self):\n",
    "        \"\"\"\n",
    "        Select node from which we run the rollout.\n",
    "        \"\"\"\n",
    "        current_node = self\n",
    "        while not current_node._is_terminal_node():\n",
    "            # if there is an action that has not been tried yet, return the child node corresponding to this action\n",
    "            if not current_node._is_fully_expanded():\n",
    "                return current_node._expand()\n",
    "            # else select the best child node\n",
    "            else:\n",
    "                current_node = current_node._best_child()\n",
    "        return current_node\n",
    "\n",
    "    def _is_terminal_node(self):\n",
    "        \"\"\"\n",
    "        This is used to check if the current node is terminal or not. \n",
    "        Terminal node is reached when the game is over.\n",
    "        \"\"\"\n",
    "        return self.terminal_state\n",
    "    \n",
    "    def _is_fully_expanded(self):\n",
    "        \"\"\"\n",
    "        All the actions are poped out of _untried_actions one by one. \n",
    "        When it becomes empty, that is when the size is zero, it is fully expanded.\n",
    "        \"\"\"\n",
    "        return len(self._untried_actions) == 0\n",
    "\n",
    "    def _expand(self):\n",
    "        \"\"\"\n",
    "        Select an action that has not been tried yet and return the corresponding child node.\n",
    "        \"\"\"\n",
    "        # select an action that has not been tried yet\n",
    "        action = self._untried_actions.pop()\n",
    "        \n",
    "        new_state = deepcopy(self.state)\n",
    "        new_state.step(action)\n",
    "        obs, reward, done, trunc, info = new_state.last()\n",
    "        child_node = MonteCarloTreeSearchNode(state = new_state, n_simulations=self.n_simulations,\n",
    "                                            parent=self, parent_action=action, gamma=self.gamma)\n",
    "        child_node.terminal_state = done\n",
    "        self.children.append(child_node)\n",
    "\n",
    "        return child_node\n",
    "    \n",
    "    def _best_child(self, c_param=0.8):\n",
    "        \"\"\"\n",
    "        Once fully expanded, this function selects the best child out of \n",
    "        the children array. The first term in the formula corresponds to \n",
    "        exploitation and the second term corresponds to exploration.\n",
    "        \"\"\"\n",
    "        choices_weights = [self._uct(child, c_param) for child in self.children] \n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "    \n",
    "    def _uct(self, child, c_param):\n",
    "        if self.number_of_visits == 0 or child.number_of_visits == 0:\n",
    "            return float(\"inf\")\n",
    "        return child.average_reward + \\\n",
    "                c_param*np.sqrt(np.log(self.number_of_visits)/child.number_of_visits)\n",
    "    \n",
    "    def _rollout(self):\n",
    "        \"\"\"\n",
    "        From the current state, entire game is simulated till there is an \n",
    "        outcome for the game. This outcome of the game is returned.\n",
    "        \"\"\"\n",
    "        current_rollout_state = deepcopy(self.state)\n",
    "\n",
    "        _reward = 0\n",
    "        step = 0\n",
    "        done = current_rollout_state.last()[2]\n",
    "        while not done:\n",
    "            possible_moves = self._get_legal_actions()\n",
    "            action = self._rollout_policy(possible_moves)\n",
    "            current_rollout_state.step(action)\n",
    "            obs, reward, done, trunc, info = current_rollout_state.last()\n",
    "            _reward += reward *(self.gamma ** step)\n",
    "            step += 1\n",
    "        return _reward\n",
    "    \n",
    "    def _get_legal_actions(self): \n",
    "        ''' \n",
    "        Returns a list of all of the possible actions from current state.\n",
    "        '''\n",
    "        return list(self.state.unwrapped.get_action_map('taxi_0').values())\n",
    "    \n",
    "    def _rollout_policy(self, possible_moves):\n",
    "        \"\"\"\n",
    "        Randomly selects a move out of possible moves.\n",
    "        \"\"\"\n",
    "        return possible_moves[np.random.randint(len(possible_moves))]\n",
    "\n",
    "    def _backpropagate(self, reward):\n",
    "        \"\"\"\n",
    "        In this step all the statistics for the nodes are updated. \n",
    "        Untill the parent node is reached, the number of visits for \n",
    "        each node is incremented by 1.\n",
    "        \"\"\"\n",
    "        self.number_of_visits += 1.\n",
    "        self.average_reward = self.average_reward + (1/self.number_of_visits)*(reward-self.average_reward)\n",
    "        if self.parent:\n",
    "            self.parent._backpropagate(reward)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6b41c21",
   "metadata": {},
   "source": [
    "Now let's see Monte Carlo Tree Search in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8711d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = new_environment_creator(taxi_pos=(0,0), passenger_pos=(3,3))\n",
    "N_SIMULATIONS = 20\n",
    "GAMMA = 0.78\n",
    "mcts = MonteCarloTreeSearchNode(state=env, n_simulations=N_SIMULATIONS, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b0b4cef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step number: 9\n",
      "Action executed: pickup\n",
      "Q values:\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 10, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcts.run_and_update(max_number_of_steps=1000, render=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da0a5e14",
   "metadata": {},
   "source": [
    "We've seen **Monte Carlo Control** and **Monte Carlo Tree Search** algorithms.\n",
    "\n",
    "Now we are going to look into an extension of Monte Carlo Tree Search called **Rapid Action Value Estimation** (RAVE). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3fe3ed3",
   "metadata": {
    "id": "e3fe3ed3"
   },
   "source": [
    "# Rapid Action Value (RAVE)\n",
    "- It created the first program to achieve dan (master) level in Go. [[Gelly, Silver 2011]](https://www.sciencedirect.com/science/article/pii/S000437021100052X)\n",
    "- **MCTS** requires a **lot of simulations** to sample several pairs of state and action\n",
    "- To **speed up** this estimate we can introduce a **bias**\n",
    "- We will **update** the pair $\\langle s, a \\rangle$ **with** the **reward** obtained **using** $a$ **from** any state in the **subtree** of $s$ (AMAF heuristic)\n",
    "![rave](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/rave.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19d57ed4",
   "metadata": {
    "id": "19d57ed4"
   },
   "source": [
    "- In order to balance these two estimates we will use the following formula\n",
    "$$\n",
    "\\tilde{Q}(s,a) = (1-\\beta(s,a))*Q(s,a) + \\beta(s,a)*RAVE(s,a)\n",
    "$$\n",
    "Where:\n",
    "- $\\tilde{Q}(s,a)$ is the final value attributed to the pair $\\langle s, a \\rangle$\n",
    "- $\\beta(s,a)$ is a parameter which dynamically balances these two estimates\n",
    "- $Q(s,a)$ is the average Q-value of $\\langle s, a \\rangle$\n",
    "- $RAVE(s,a)$ is the RAVE value of $\\langle s, a \\rangle$\n",
    "\n",
    "$$\\beta(s,a) = \\sqrt{\\frac{k}{3N(s,a)+k}}$$\n",
    "Where:\n",
    "- $N(s)$ is the number of times that we visited the current node\n",
    "- $k$ is an hyperparameter that sets the number of simulation at which $Q$ and $RAVE$ are given the same weight\n",
    "\n",
    "![beta](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/beta.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93b0440f",
   "metadata": {},
   "source": [
    "Implementation of RAVE is below. It is similar to Monte Carlo Tree Search, main difference points are highlighted in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "539f0487",
   "metadata": {
    "id": "539f0487"
   },
   "outputs": [],
   "source": [
    "class RAVEMonteCarloTreeSearchNode(MonteCarloTreeSearchNode):\n",
    "    '''Introduce RAVE optimization to MCTS'''\n",
    "    def __init__(self, state, n_simulations=100, parent=None, parent_action=None, k=3, gamma=0.78):\n",
    "        super().__init__(state, n_simulations, parent,parent_action, gamma=gamma)\n",
    "        self.average_reward_rave = 0\n",
    "        self.number_of_visits_rave = 0\n",
    "        self.k = k\n",
    "\n",
    "    def _best_action(self):\n",
    "        \"\"\"\n",
    "        Execute the four stages to find the best action from the current state.\n",
    "        \"\"\"\n",
    "        for _ in range(self.n_simulations):\n",
    "          node = self._tree_policy()\n",
    "          reward, taken_actions = node._rollout()\n",
    "          node._backpropagate(reward, taken_actions)\n",
    "\n",
    "        return self._best_child(c_param=0.)\n",
    "    \n",
    "    def _tree_policy(self):\n",
    "      \"\"\"\n",
    "      Selects node to run rollout.\n",
    "      \"\"\"\n",
    "      current_node = self\n",
    "      while not current_node._is_terminal_node():\n",
    "\n",
    "          if current_node._is_leaf():\n",
    "              # Differently from what we did in Monte Carlo Tree Search, even if we do not select\n",
    "              # a certain child from the current node, we still need to create it so that \n",
    "              # we will be able to update its rave value in the backpropagation step.\n",
    "              current_node._create_all_children()\n",
    "              return current_node._best_child()\n",
    "          \n",
    "          current_node = current_node._best_child()\n",
    "      return current_node\n",
    "\n",
    "    def _is_leaf(self):\n",
    "        \"\"\"\n",
    "        Returns True if the node is a leaf node, False otherwise.\n",
    "        \"\"\"\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def _create_all_children(self):\n",
    "        \"\"\"\n",
    "        Create all possible child nodes for the current node.\n",
    "        \"\"\"\n",
    "        for action in self._untried_actions:\n",
    "            \n",
    "            new_state = deepcopy(self.state)\n",
    "            new_state.step(action)\n",
    "            obs, reward, done, trunc, info = new_state.last()\n",
    "            child_node = RAVEMonteCarloTreeSearchNode(state = new_state, n_simulations=self.n_simulations,\n",
    "                            parent=self, parent_action=action, k=self.k, gamma=self.gamma)\n",
    "            child_node.terminal_state = done\n",
    "\n",
    "            self.children.append(child_node)\n",
    "\n",
    "    def _best_child(self, c_param=0.8):\n",
    "        \"\"\"\n",
    "        Select the best child from the children array according to the MC-RAVE formula.\n",
    "        :param c_param: Exploration parameter.\n",
    "        :return: The child with the highest MC-RAVE value.\n",
    "        \"\"\"\n",
    "        choices_weights = [self._mc_rave(child, c_param) for child in self.children] \n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "\n",
    "    def _mc_rave(self, child, c_param):\n",
    "        \"\"\"\n",
    "        Returns the MC-RAVE value for a child node.\n",
    "        :param child: The child node for which the MC-RAVE value is calculated.\n",
    "        :param c_param: Exploration parameter.\n",
    "        :return: The MC-RAVE value for the child node.\n",
    "        \"\"\"\n",
    "        uct =  float('inf') if (child.number_of_visits == 0 or self.number_of_visits == 0) else child.average_reward + \\\n",
    "        c_param*np.sqrt( \\\n",
    "          np.log(self.number_of_visits)/ child.number_of_visits \\\n",
    "        )\n",
    "        beta = np.sqrt(self.k / (3 * self.number_of_visits + self.k))\n",
    "        monte_carlo_value = float('inf') if uct == float('inf') \\\n",
    "                  else (1-beta)*uct\n",
    "        rave_value = float('inf') if child.number_of_visits_rave == 0 \\\n",
    "                  else beta*child.average_reward_rave\n",
    "        return monte_carlo_value + rave_value\n",
    "\n",
    "    def _rollout(self):\n",
    "        \"\"\"\n",
    "        Conducts a rollout from the current node.\n",
    "        :return: a tuple containing the reward obtained and the actions taken during the rollout.\n",
    "        \"\"\"\n",
    "        current_rollout_state = deepcopy(self.state)\n",
    "\n",
    "        _reward = 0\n",
    "        step = 0\n",
    "        done = current_rollout_state.last()[2]\n",
    "        taken_actions = []\n",
    "        while not done:\n",
    "            possible_moves = self._get_legal_actions()\n",
    "\n",
    "            action = self._rollout_policy(possible_moves)\n",
    "            current_rollout_state.step(action)\n",
    "            obs, reward, done, trunc, info = current_rollout_state.last()\n",
    "            _reward += reward*(self.gamma**step)\n",
    "            # DIFFERENCE FROM MONTE CARLO TREE SEARCH: we need to keep track of the actions taken\n",
    "            taken_actions.append(action)\n",
    "            ##############################\n",
    "            step += 1\n",
    "        return _reward, taken_actions\n",
    "\n",
    "    def _backpropagate(self, reward, taken_actions):\n",
    "        \"\"\"\n",
    "        Backpropagates the reward obtained during the rollout to the parents of the current node.\n",
    "        :param reward: The reward obtained during the rollout.\n",
    "        :param taken_actions: The actions taken during the rollout.\n",
    "        \"\"\"\n",
    "        self.number_of_visits += 1\n",
    "        self.average_reward = self.average_reward + (1/self.number_of_visits)*(reward-self.average_reward)\n",
    "\n",
    "        # DIFFERENCE FROM MONTE CARLO TREE SEARCH: we need to update the rave estimates of the children\n",
    "        for action in set(taken_actions):\n",
    "            for child in self.children:\n",
    "                if child.parent_action == action:\n",
    "                    child._update_rave_estimate(reward)\n",
    "        ##############################\n",
    "        \n",
    "        if self.parent is not None:\n",
    "            self.parent._backpropagate(reward, taken_actions)\n",
    "    \n",
    "    def _update_rave_estimate(self, reward):\n",
    "        \"\"\"\n",
    "        Updates the rave estimate of the current node.\n",
    "        :param reward: The reward obtained during the rollout.\n",
    "        \"\"\"\n",
    "        self.number_of_visits_rave += 1\n",
    "        self.average_reward_rave = self.average_reward_rave + (1/self.number_of_visits_rave)*(reward-self.average_reward_rave)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e316251",
   "metadata": {},
   "source": [
    "Now we show how RAVE is created and executed (we chose 20 simulations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "be33ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = new_environment_creator(taxi_pos=(0,0),passenger_pos=(3,3))\n",
    "rave = RAVEMonteCarloTreeSearchNode(state = env, n_simulations = 20, k=1, gamma=0.78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a95d530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step number: 18\n",
      "Action executed: pickup\n",
      "Q values:\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 19, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rave.run_and_update(max_number_of_steps=1000, render=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "299186ce",
   "metadata": {},
   "source": [
    "Now we will evaluate each of the algorithms for a variety of #simulations.\n",
    "The following function, **evaluate_policy** executes one of the algorithms  for a given of number of simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fac9868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(problem, algorithm, simulations, render:bool=True):\n",
    "    if not algorithm in [\"Monte Carlo Control\", \"Monte Carlo Tree Search\", \"RAVE\", \"NewRAVE\"]:\n",
    "        raise ValueError(\"Algorithm must be one among: Monte Carlo Control, Monte Carlo Tree Search, RAVE and NewRAVE\")\n",
    "      \n",
    "    env = new_environment_creator(taxi_pos=problem[\"taxi_pos\"], passenger_pos=problem[\"passenger_pos\"],domain_map=problem[\"domain_map\"])\n",
    "\n",
    "    if algorithm == \"Monte Carlo Control\":\n",
    "        # Monte Carlo Control\n",
    "        monte_carlo_control = MonteCarloControl(env, gamma=0.99, max_steps_per_episode=1_000)\n",
    "        step = monte_carlo_control.run_and_update(number_of_simulated_episodes=simulations, render=render)\n",
    "        return step\n",
    "\n",
    "    elif (algorithm==\"Monte Carlo Tree Search\"):\n",
    "        # Monte Carlo Tree Search\n",
    "        mcts = MonteCarloTreeSearchNode(state = env, n_simulations=simulations, gamma=0.78)\n",
    "        step = mcts.run_and_update(max_number_of_steps=1000, render=render)\n",
    "        return step\n",
    "    \n",
    "    elif (algorithm ==\"RAVE\"):\n",
    "        # RAVE\n",
    "        rave = RAVEMonteCarloTreeSearchNode(state = env, n_simulations = simulations, k=1, gamma=0.78)\n",
    "        step = rave.run_and_update(max_number_of_steps=1000, render=render)\n",
    "        return step\n",
    "    \n",
    "    elif (algorithm ==\"NewRAVE\"):\n",
    "        # NewRAVE\n",
    "        rave = RAVEMonteCarloTreeSearchNode(state = env, n_simulations = simulations, k=1, gamma=0.78, new_rave=True)\n",
    "        step = rave.run_and_update(max_number_of_steps=1000, render=render)\n",
    "        return step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7b12c91",
   "metadata": {},
   "source": [
    "After specifying the problem, and number of simulation for the chosen algorithm, we're good to go. Here we evaluate the policy for MCTS, for 20 simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "91addc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step number: 11\n",
      "Action executed: pickup\n",
      "Q values:\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 12, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem={\"taxi_pos\":(0,0),\"passenger_pos\":(3,3),\"domain_map\":DEFAULT_MAP}\n",
    "evaluate_policy(problem, \"Monte Carlo Tree Search\", 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98de214e",
   "metadata": {},
   "source": [
    "Finally we plot the performance of the algorithms according to number of simulations.\n",
    "\n",
    "P.S. By default render is set to False to make the notebook run faster. If you want to see the agent in action, set render to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5877ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_steps_per_simulations(problem, algorithms, simulations_per_step, number_of_experiments, render:bool=False):\n",
    "\n",
    "    data_to_plot = [[] for _ in range(len(algorithms))]\n",
    "\n",
    "    \n",
    "    for sim in simulations_per_step:\n",
    "        for i, algorithm in enumerate(algorithms): \n",
    "            print(f\"{algorithm} for {sim} simulations\")\n",
    "            steps=0\n",
    "            for _ in range(number_of_experiments):\n",
    "                steps += evaluate_policy(problem, algorithm ,sim, render=render)\n",
    "            data_to_plot[i].append(steps/number_of_experiments)\n",
    "\n",
    "    print(\"Experiments' results:\")\n",
    "    print(data_to_plot)\n",
    "\n",
    "    _, ax = plt.subplots(1, 1, figsize=(12, 3))\n",
    "    ax.set_title(f\"Performance\")\n",
    "    for i, algorithm in enumerate(algorithms): \n",
    "        ax.plot(simulations_per_step, data_to_plot[i], '-o', label=algorithm)\n",
    "    ax.set_xlabel(\"Simulations per step\")\n",
    "    ax.set_ylabel(\"Steps\")\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    plt.savefig(\"performance.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return data_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1045579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem={\"taxi_pos\":(0,0),\"passenger_pos\":(3,3),\"domain_map\":DEFAULT_MAP}\n",
    "#plot_steps_per_simulations(problem, [\"Monte Carlo Tree Search\"], [10,20,50], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "518ebeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Control for 10 simulations\n",
      "Monte Carlo Tree Search for 10 simulations\n",
      "RAVE for 10 simulations\n",
      "Monte Carlo Control for 20 simulations\n",
      "Monte Carlo Tree Search for 20 simulations\n",
      "RAVE for 20 simulations\n",
      "Monte Carlo Control for 30 simulations\n",
      "Monte Carlo Tree Search for 30 simulations\n",
      "RAVE for 30 simulations\n",
      "Monte Carlo Control for 50 simulations\n",
      "Monte Carlo Tree Search for 50 simulations\n",
      "RAVE for 50 simulations\n",
      "Experiments' results:\n",
      "[[7.6, 7.4, 7.2, 7.2], [15.1, 12.4, 10.5, 8.4], [14.4, 10.4, 10.0, 10.5]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAADgCAYAAAD44ltAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABRl0lEQVR4nO3dd3xUVf7/8deZyaSHUIL03lsIHWTVKAgqiLgq9oYKuEXdr32XFZd1f+t3WXctX1dAxb4ggoKCBUUjKoiC9CKIBgERKQIhpOf8/riTZGYyKYNJJoH38/G4j2RuPXMcyTsnn3uusdYiIiIiIiJlc4W7ASIiIiIitZ1Cs4iIiIhIBRSaRUREREQqoNAsIiIiIlIBhWYRERERkQooNIuIiIiIVEChWUSkFjHGNDHGLDPGZBhjHgl3e0RExBER7gaIiJwMjDHpQBOgAMgE3gF+Z609FuKpJgAHgHpWE+mLiNQaGmkWEak6F1pr44G+QH9gcmUPNA4X0AbYfCKB2RijgRARkWqi0CwiUsWstXtwRpp7GmMGG2OWG2MOG2PWGWNSi/YzxqQZY/5mjPkMOA68CFwP3GOMOWaMGW6MiTLGPGqM+cG7PGqMifIen2qM2W2MudcY8yPwnDHmQWPMa8aYl70lHhuMMZ2NMfcbY34yxuwyxozwacONxpgt3n2/NcZM9NlWdP47vcfuNcbc6LM9xhjziDFmpzHmiDHmU2NMjHdbme9bRKQuUmgWEalixphWwAXAXmAx8BDQELgLmG+Maeyz+7U4JRkJwI3AK8A/rLXx1toPgD8Bg4EUoDcwEP8R7Kbec7fxngfgQuAloAGwBngP59/7FsBUYIbP8T8Bo4F63uv/2xjTN+D8id5jbwKeNMY08G77J9APON3bhnuAQmNMi0q8bxGROkWhWUSk6iwwxhwGPgU+BnYDb1tr37bWFlpr3wdW4QTqIs9bazdZa/OttXlBznk1MNVa+5O1dj/wF5ygXaQQmGKtzbHWZnnXfWKtfc9amw+8BjQGHvaefw7Q1hhTH8Bau9hau8M6PgaWAGf4nD/Pe/08a+3bwDGgi7eUZDxwu7V2j7W2wFq73FqbA1xTifctIlKnKDSLiFSdsdba+tbaNtba3+DcGHiZt0ThsDdQ/wpo5nPMrgrO2RzY6fN6p3ddkf3W2uyAY/b5fJ8FHLDWFvi8BogHMMacb4z53BhzyNu+C4Akn+MPesN3kePeY5OAaGBHkDa3oeL3LSJSp+imERGR6rMLeMlae0s5+1R0w98POCF0k/d1a++6yh5fJm9t9HzgOmChtTbPGLMAMJU4/ACQDXQA1gVsq8z7FhGpUzTSLCJSfV4GLjTGjDTGuI0x0d6b61qGcI7ZwGRjTGNjTBLwgPe8VSESiAL2A/nGmPOBEeUf4rDWFgKzgH8ZY5p7398QbxCvivctIlKrKDSLiFQTa+0u4CLgjzjBdBdwN6H92/sQTj3wemAD8JV3XVW0LwO4DZgL/AxcBbwZwinu8rbpS+AQ8L+Aq4ret4hIrWI0d76IiIiISPn0W7+IiIiISAUUmkVEREREKqDQLCIiIiJSAYVmEREREZEKKDSLiIiIiFSgTjzcJCkpybZt27bGr5uZmUlcXFyNX7euUn+FTn0WGvVXaNRfoVF/hUb9FRr1V2jC2V+rV68+YK1tHLi+ToTmtm3bsmrVqhq/blpaGqmpqTV+3bpK/RU69Vlo1F+hUX+FRv0VGvVXaNRfoQlnfxljdgZbr/IMEREREZEKKDSLiIiIiFRAoTmY9XPh3z05K20s/Lun81pERERETll1oqa5Rq2fC2/dBnlZGIAju5zXAMnjwtkyERGRk1ZeXh67d+8mOzs73E2pFomJiWzZsiXczagzaqK/oqOjadmyJR6Pp1L7KzQHWjoV8rL81+VlOesVmkVERKrF7t27SUhIoG3bthhjwt2cKpeRkUFCQkK4m1FnVHd/WWs5ePAgu3fvpl27dpU6RuUZgY7sLmP9Lti2BPJza7Y9IiIip4Ds7GwaNWp0UgZmqX2MMTRq1Cikv2xopDlQYksnIJdi4L+XQXQidBkFPcZC+7MhIrKmWygiInJSUmCWmhTq500jzYGGPQCeGP91nhgY+x+46jUnMG9dDP8dB9M6whu3wrb3NAItIiJSxxljuOaaa4pf5+fn07hxY0aPHn1C5zt8+DD/+c9/Qj7u2LFjTJw4kQ4dOtCvXz9SU1NZuXJlSOdITU0N6RkXeXl53HfffXTq1Im+ffsyZMgQ3nnnnVCbDsCCBQvYvHlzyMeF2uaappHmQEV1y0unYo/sxiS2dIJ00frOI5yA/G0abF4AWxfBuv9CVCJ0vQB6XKwRaBERkWq2YM0epr33NT8czqJ5/RjuHtmFsX1a/KJzxsXFsXHjRrKysoiJieH999+nRYsTP2dRaP7Nb34T0nE333wz7dq1Y/v27bhcLr777ruQQmhBQUGoTeXPf/4ze/fuZePGjURFRbFv3z4+/vjjkM8DTmgePXo03bt3L7UtPz+fiIi6GT810hxM8jj4w0Y+Tl0Af9hY+gbAiEgnPI/9D9z1jTMC3W00fP22zwj0JPj6XcjPCctbEBEROVktWLOH+1/fwJ7DWVhgz+Es7n99AwvW7PnF577gggtYvHgxALNnz+bKK68s3nbo0CHGjh1LcnIygwcPZv369QA8+OCDjB8/ntTUVNq3b8/jjz8OwH333ceOHTtISUlh8uTJAEybNo0BAwaQnJzMlClTSl1/x44drFy5koceegiXy4lp7dq1Y9SoUQCMHTuWfv360aNHD2bOnFl8XHx8PHfeeSe9e/dmxYoVfuecPXs2vXr1omfPntx7772lrnn8+HGefvppnnjiCaKiogBo0qQJ48aNK/f4+Ph4/vSnP9G7d28GDx7Mvn37WL58OW+++SZ33303KSkp7Nixg9TUVO644w769+/PY489xtKlS+nTpw+9evVi/Pjx5OTUjaxUN6N+bVIUoDuPgPxHA0agZ5eMQHcfCx3Ohoio8LZXRESklvvLW5vY/MPRMrev+f4wuQWFfuuy8gq4Z956Zn/xfdBjujevx5QLe1R47SuuuIKpU6cyevRo1q9fz/jx4/nkk08AmDJlCn369GHBggV8+OGHXHfddaxduxaArVu38tFHH5GRkUGXLl249dZbefjhh9m4cSNr164lIyODJUuWsH37dr744gustYwZM4Zly5Zx5plnFl9/06ZNpKSk4Ha7g7Zv1qxZNGzYkKysLAYMGMAll1xCo0aNyMzMZNCgQTzyyCN++//www/ce++9rF69mgYNGjBixAgWLFjA2LFji/f55ptvaN26NfXq1St1vfKOz8zMZPDgwfztb3/jnnvu4emnn2by5MmMGTOG0aNHc+mllxafJzc3l1WrVpGdnU2nTp1YunQpnTt35rrrruOpp57ijjvuqPC/TbhppLkqBY5AXz2vZAR69uUwrRO8PlEj0CIiIr9AYGCuaH0okpOTSU9PZ/bs2VxwwQV+2z799FOuvfZaAM455xwOHjzI0aNOuB81ahRRUVEkJSVx2mmnsW/fvlLnXrJkCUuWLKFPnz707duXrVu3sn379pDa9/jjjxeP7O7atav4eLfbzSWXXFJq/y+//JLU1FQaN25MREQEV199NcuWLav09co7PjIysrjeu1+/fqSnp5d5nssvvxyAr7/+mnbt2tG5c2cArr/++pDaE04aaa4uEZHQ6VxnyX8UvvsYNi2ArW/B+jkQVQ+6eGugNQItIiJSrKIR4aEPf8iew1ml1reoH8OrE4f84uuPGTOGu+66i7S0NA4ePFipY4rKGsAJsPn5+aX2sdZy//33M3HixDLP06NHD9atW0dBQUGp0ea0tDQ++OADVqxYQWxsLKmpqcVTpkVHR5c5Ol2Rjh078v3333P06NGgo81l8Xg8xTNQlPWei8TFxZ1Q22oTjTTXhKIAPfZJnxHoMbDtXe8IdEfvCPQ7GoEWERGpwN0juxDj8Q+IMR43d4/sUiXnHz9+PFOmTKFXr15+68844wxeeeUVwAmwSUlJ5YbMhIQEMjIyil+PHDmSWbNmcezYMQD27NnDTz/95HdMhw4d6N+/P1OmTMFaC0B6ejqLFy/myJEjNGjQgNjYWLZu3crnn39e4XsZOHAgH3/8MQcOHKCgoIDZs2dz1lln+e0TGxvLTTfdxO23305urjMb2P79+3nttdcqdXxF79tXly5dSE9P55tvvgHgpZdeqvB8tYVGmmua3wj0v+G7ZbDpDacG2m8Eeix0OEcj0CIiIgGKZsmo6tkzirRs2ZLbbrut1PqiG/6Sk5OJjY3lhRdeKPc8jRo1YujQofTs2ZNhw4bx2GOPsWXLFoYMcUbD4+PjefnllznttNP8jnvmmWe488476dixIzExMSQlJTFt2jSSk5OZPn063bp1o0uXLgwePLjC99KsWTMefvhhzj77bKy1jBo1iosuuqjUfg899BCTJ0+me/fuREdHExcXx9SpUyt9vK8rrriCW265hccff5x58+b5bYuOjua5557jsssuIz8/nwEDBjBp0qQK30dtYIp+i6nN+vfvb2ty3r7F3y7msa8eY2/mXprFNeP2vrczqv2o6r1ofq4ToDe/AVsWQfZhb4A+31vCUfsDdFpaGqmpqeFuRp2iPguN+is06q/QqL9CU9X9tWXLFrp161Zl56tt9Bjt0NRUfwX73BljVltr+wfuq5HmAIu/XcyDyx8ku8CpEdqbuZcHlz8IUL3BOSISOg13ltGPwrcflwTo9a+WBOjuY50A7YmuvraIiIiIiB+F5gCPffVYcWAukl2QzWNfPVb9o81F3J7yA3Rkgs80dgrQIiIiItVNoTnAj5k/hrS+2gUG6O8+dmqgfQO0bwmHArSIiIhIldPsGQGaxjUtc9u/Vv2LA1kHarA1Adwe6DgcLnoS7v4Grpnv3DD4zfsw50pnFo75t8DWxZCXXeHpRERERKRyFJoD3N73dqLd/qO1Ue4oejfuzQubX+D8+efzjy//wf7j+8PUQq/iAP1/cNf2gAB9lQK0iIiISBVSeUaAorrlYLNnpB9J5+kNT/PfLf9l7tdzubTzpdzY40aaxDUJb6OLAnTH4TA6YBq7DXO9JRzneUs4hqmEQ0RERCRE1TbSbIyZZYz5yRizMci2O40x1hiTVF3X/yVGtR/FkkuX8ESbJ1hy6ZLiIN02sS1/+9XfeHPsm1zQ7gJe3foq579+Pg99/lD4ap4DuT3QcZjPCPTr0PNi+OYDnxHom52aaI1Ai4iIFDPGcM011xS/zs/Pp3HjxsWPig7V4cOH+c9//hPycceOHWPixIl06NCBfv36kZqaysqVK0M6R2pqKpWdrvfiiy8mJSWFjh07kpiYSEpKCikpKSxfvjzktpflb3/7Gz169CA5OZmUlJSQ308o0tLSTvi/WXmqc6T5eeD/gBd9VxpjWgEjgO+r8drVqnW91kwdOpUJyRN4ZsMzzN8+n/nb53Nxx4u5qddNtIivmsnVf7GiAN1xGIz6l3ce6AVOYN7wWskIdPexzii1RqBFRKSuWD8Xlk6FI7shsSUMewCSx/2iU8bFxbFx40aysrKIiYnh/fffp0WLE/+ZXhSaf/Ob34R03M0330y7du3Yvn07LpeL7777js2bN1f6+IKCgpCu98YbbwBO2PznP//JokWL/Lbn5+cTEXHikXHFihUsWrSIr776iqioKA4cOFD85MFf4pe2K1TVNtJsrV0GHAqy6d/APUDtf6pKBVomtOTB0x9k8cWLuaTTJSz4ZgGjXx/NlOVT2JWxK9zN81cUoMc8AXdtg2vf8I5AL4VXr4ZpHTQCLSIidcP6ufDWbXBkF2Cdr2/d5qz/hS644AIWL14MwOzZs7nyyiuLtx06dIixY8eSnJzM4MGDWb9+PVDypMDU1FTat2/P448/DsB9993Hjh07SElJYfLkyQBMmzaNAQMGkJyczJQpU0pdf8eOHaxcuZKHHnoIl8uJae3atWPUKOev3mPHjqVfv3706NGDmTNnFh8XHx/PnXfeSe/evVmxYoXfOWfPnk2vXr3o2bMn9957b6X64fnnn2fMmDGcc845DBs2jMzMTMaPH8/AgQPp06cPCxcuBJyAfvfddxe/pxkzZpQ61969e0lKSiIqynlIW1JSEs2bNwdg9erVnHXWWfTr14+RI0eyd+/e4usPGDCA3r17c8kll3D8+HEAbrjhBiZNmsSgQYO45557+Oabbxg+fDi9e/emb9++7NixA3BG6y+99FK6du3K1VdfTVU8zK9Ga5qNMRcBe6y164wxNXnpatU8vjmTB0/m5l43M2vjLOZvm8/CbxYyuv1obkm+hTb12oS7if7cHmd6ug7nOCPQ6Z/ApgWw5S3vCHQ8dPbWQHccBp6YcLdYREROJe/cBz9uKHv77i+hIMd/XV4WLPwdrC7j0dZNe8H5D1d46SuuuIKpU6cyevRo1q9fz/jx4/nkk08AmDJlCn369GHBggV8+OGHXHfddaxduxaArVu38tFHH5GRkUGXLl249dZbefjhh9m4cSNr164lIyODJUuWsH37dr744gustYwZM4Zly5Zx5plnFl9/06ZNpKSk4Ha7g7Zv1qxZNGzYkKysLAYMGMAll1xCo0aNyMzMZNCgQTzyyCN++//www/ce++9rF69mgYNGjBixAgWLFjA2LFjK+yLr776ivXr19OwYUP++Mc/cs455zBr1iwOHz7MwIEDGT58OK+88gqJiYl8+eWX5OTkMHToUEaMGEG7du2KzzNixAimTp1K586dGT58OJdffjlnnXUWeXl5/P73v2fhwoU0btyYV199lT/96U/MmjWLCy+8kN///vcATJ48mWeffbb49e7du1m+fDlut5tBgwZx3333cfHFF5OdnU1hYSG7du1izZo1bNq0iebNmzN06FA+++wzfvWrX1X4nstTY6HZGBML/BGnNKMy+08AJgA0adKEtLS06mtcGY4dOxbydU/ndHo068EHRz/g7W/f5s0db9I/rj8jE0fSxBPmGwbL5IJ6v8YMGEP9wxtpvP8zGm99D8/GeeS7oznYaCD7G5/OoYZ9KXSX/SjvE+mvU536LDTqr9Cov0Kj/gpNVfdXYmIiGRkZAETl5eIqyC9zX3dBDsGG3mxBDgVlHFeYl0uO9/zladeuHd9++y3PPfccw4cP5/jx4+Tn55ORkcGyZct46aWXyMjIYMCAARw4cIA9e/aQk5PD8OHDyc3NJSoqiqSkJHbs2EF+fj6FhYVkZGRQUFDAokWLeO+99+jduzfg9OGGDRvo06dP8fWzsrKKrxfMtGnTissndu3axdq1axk4cCBut5sRI0YUH1dQUEBmZibLli1j6NChREdHk5WVxSWXXMIHH3zAsGHDSp3b971mZ2eTmpqKx+MhIyODd999lwULFvCPf/yjuJ1btmzh7bffZuPGjcyd64zyHz16lHXr1pGU5H/bWlpaGsuXL2fZsmWMGzeOv/zlL/Tp04eNGzcWt6WgoIAmTZqQkZHBpk2buOqqqzhy5AiZmZkMGzaMjIwM8vLyGD16NMePHycjI4Pdu3czfPhwv/46fvw4/fr1IzExkczMTHr06MGWLVuK+91XdnZ2pT/HNTnS3AFoBxSNMrcEvjLGDLTWlrqLzlo7E5gJ0L9/f1uVz7evrLS0NE70uhdxEQeyDvD8xueZu20uq35YxXntzmNi8kQ61O9QtQ2tUsOBO6AgD9I/IWLTAppseYsmPy3zGYEe662B9h+B/iX9dapSn4VG/RUa9Vdo1F+hqer+2rJlCwkJCc6LMf8qf+d/9/SWZvgzia2IuPm9Mg+LrEQ7EhISGDt2LJMnTyYtLY2DBw8SERFBQkICLpeL+Pj44nYaY0hISCAqKspvvcfjITrauU/I5XKRkJBARkYGHo+HP/7xj0ycOLHM6/fv35/777+f2NjYUqPNaWlpfPLJJ6xcuZLY2FhSU1Nxu90kJCQQHR1N/fr1i/d1u93ExcURExODx+Mpblt0dDSRkZElfe0jNja2+L0Wnc/3vb7xxht06dLF7xi3282TTz7JyJEjK+zbCy64gAsuuID+/fvzwgsvMHToUHr06FGqnATgt7/9LQsXLqR37948//zzpKWlkZCQgMfjISkpqdR/g8D3ERsb6/eeffvAV3R0tN8vLeWpsXmarbUbrLWnWWvbWmvbAruBvsEC88kiKSaJuwbcxTu/focbe95I2q40Ll54MXd9fBfbft4W7uaVr6iEY8zjziwc1y6AnpfAjg/h1WucWTjmjXdKOvKywt1aERE5lQx7oHTpoCfGWV8Fxo8fz5QpU+jVq5ff+jPOOINXXnkFcAJsUlIS9erVK/M8RWG5yMiRI5k1axbHjh0DYM+ePfz0009+x3To0IH+/fszZcqU4jrc9PR0Fi9ezJEjR2jQoAGxsbFs3bqVzz//vML3MnDgQD7++GMOHDhAQUEBs2fP5qyzzqpcR/gYOXIkTzzxRHGb1qxZU7z+qaeeIi8vD4Bt27aRmZnpd+zXX3/N9u3bi1+vXbuWNm3a0KVLF/bv318cmvPy8ti0aRMAGRkZNGvWjLy8vOI+D5SQkEDLli1ZsGABADk5OcW1z9Wh2kaajTGzgVQgyRizG5hirX22uq5XmzWKacQf+v2BG3rcwEubX+K/W//Le+nvMbz1cCb2nkjXhl3D3cTyuSOgw9nOUlQDvXmBE5g3zveOQI8kyXaCvEGqgRYRkepVNEtGFc+eUaRly5bcdtttpdYX3fCXnJxMbGwsL7xQRv20V6NGjRg6dCg9e/Zk2LBhPPbYY2zZsoUhQ4YAzs17L7/8Mqeddprfcc888wx33nknHTt2JCYmhqSkJKZNm0ZycjLTp0+nW7dudOnShcGDB1f4Xpo1a8bDDz/M2WefjbWWUaNGcdFFF4XQG44///nP3HHHHSQnJ1NYWEi7du1YtGgRN998M+np6fTt2xdrLY0bNy4OsUWOHTvG73//ew4fPkxERAQdO3Zk5syZREZGMm/ePG677TaOHDlCfn4+d9xxBz169GDy5MkMGjSIxo0bM2jQoDLLVV566SUmTpzIAw88gMfj4bXXXgv5vVWWqYq7Catb//79bWXnGqxK1fWnuiM5R3h5y8u8svkVMvIyOLvV2UzsPZEejXpU+bWqVUG+f4A+fhA8cSXT2HU6VwG6AvpzcGjUX6FRf4VG/RWa6ijP6NatW5Wdr7bJyMgIWh4gwdVUfwX73BljVltr+wfuqycChkFiVCK/Tfkt13a/lle2vMJLm1/io0UfcWbLM5mUPIlejXtVfJLawHcE+oJHWPvmk6REfFsyAu2Jg84jnVk4FKBFRESkDlNoDqN6kfW4tfetXNvtWmZvnc0Lm1/gqrevYmiLoUxKnkTKaSnhbmLluSM43KA3pN4OFzwCOz/1TmP3Jmx63SdAj4WO50JkbLhbLCIiIlJpCs21QHxkPLck38JV3a5iztY5vLDpBa5951oGNxvMrb1vpW+TvuFuYmjcEdA+1Vku+KdPgH5LAVpERETqJIXmWiTOE8dNvW7iyq5XMvfruTy36Tmuf/d6BjYdyKTekxjQdEC4mxg6BWgRERE5CSg010Kxnlhu6HkDl3e9nHnb5jFr4yzGvzeefk36Man3JAY1HUSdfKJiqQD9GWx6IyBAj/A+iVABWkRERGoPheZaLCYihmu7X8tlnS9j/vb5zNowi1uW3EJK4xQm9Z7E6c1Pr5vhGbwB+ixnKQrQmxfA5jedIO2JdUagu4+FTiMUoEVERCSsauzhJnLioiOiubrb1bx9ydtMHjSZH4//yKQPJnHN29ewbPcy6sK0geUqCtCj/w13fg3XvQm9r4D0T+G162FaB3jtBqesI7f6Ji0XEZFTm9vtJiUlhZ49e3LhhRdy+PBhv+0pKSlcccUVgPOo5kaNGnH06FG/fcaOHcurr77K888/T+PGjUlJSSElJYWhQ4eyefPmmnorUg0UmuuQKHcUl3e9nLcvfpsHhjzAgawD/Hbpb7li8RV89P1HdT88Q+kAff1bpQP03OsVoEVETnGLv13MiHkjSH4hmRHzRrD428W/+JwxMTGsXbuWjRs30rBhQ5588snibVu2bKGgoIBPPvmEzMxMYmNjGTlyJG+88UbxPkeOHOHTTz/lwgsvBODyyy9n7dq1rF27ls8++4zu3bv/4jZK+Cg010Eet4fLOl/Gol8vYurpUzmac5TbPrqNcYvGsXTnUgptYbibWDVcbmh3ZkCAvtIp5fAL0G9AbmbF5xMRkZPC4m8X8+DyB9mbuReLZW/mXh5c/mCVBOciQ4YMYc+ePcWvZ8+ezbXXXsuIESNYuHAhAFdeeSVz5swp3ueNN95g5MiRxMaqpPBkpJrmOszj8nBxp4sZ3WE0b3/7NjPXz+SOtDvo3KAzE5MnMrzNcFzmJPm9qChAtzsTLpjmvYlwgTMP9OYFTg10pxHOLBydRkBkXJgbLCIiJ+p/v/hfth7aWub29fvXk1uY67cuuyCbBz57gHnb5gU9pmvDrtw78N5KXb+goIClS5dy0003Fa979dVXef/999m6dStPPPEEV111FSNHjuTmm2/m4MGDNGrUiDlz5vC73/3O75hPP/0UgMLCQlauXElMjB70VVedJInq1OZxebio40UsHLuQv5/xd3ILcrnz4zv59cJf885371BQWBDuJlat4hHof3lHoBf5jEDfANM6wtzrNAItInKSCgzMFa2vrKysLFJSUmjatCn79u3j3HPPBWDVqlUkJSXRunVrhg0bxpo1azh06BCRkZGMGTOGefPmceDAAdasWcPIkSOLzxdYnqHAXLdppPkkEuGKYHT70Zzf9nyW7FzCjHUzuGfZPTyV+BQTkidwXtvziHCdZP/JXW5od4azXDANdi4vmYVj80KIiHGmses+1pmNQyPQIiK1XkUjwiPmjWBv5t5S65vFNeO585474esW1TQfP36ckSNH8uSTT3Lbbbcxe/Zstm7dStu2bQE4evQo8+fP55ZbbuHKK6/kr3/9K9ZaLrroIjwezwlfX2o3jTSfhNwuN+e3O5/XL3qdR856hAhXBPd/cj9jF45l4TcLyS/MD3cTq0dRgB71CNy51RmB7nM17FwB826Ef3RwRqA3vq4RaBGROuz2vrcT7Y72Wxftjub2vrdXyfljY2N5/PHHeeSRR8jNzWXu3Lls2LCB9PR00tPTWbhwIbNnzwYgNTWV7du38+STT3LllVdWyfWldlJoPom5jIsRbUcw78J5PJr6KDERMUz+bDIXvnEhb2x/g7zCvHA3sfooQIuInLRGtR/Fg6c/SLO4ZhgMzeKa8eDpDzKq/agqu0afPn1ITk7m73//Oy1atKB58+bF284880w2b97M3r17cblcXHrppRw8eJCzzjrL7xyvvvqq35Rzy5cvr7L2Sc07yf5WL8G4jIthbYZxTutzSNuVxvT103lg+QPMWD+Dm3rdxNgOY/G4T+I/J/mWcJz/D/h+hVPv7FvC0elc50mEKuEQEakTRrUfVaUhGeDYsWN+r9966y0ApkyZ4rfe7Xbz448/Fr9+9NFHefTRR/32ueGGG7jhhhuKX2dkZJCQkFCl7ZWapdB8CjHGcHbrs0ltlconez5h+rrpTF0xlZnrZ3Jzz5u5uNPFRLojw93M6uVyQ9tfOUtxgF7ghOctb/oE6LHQaSRExYe7xSIiIlILKDSfgowxnNnyTM5ocQbLf1jOU+ue4qGVDzFzw0zG9xzPpZ0vJcodFe5mVj+/AP2/JQF6y5sK0CIiIuJHofkUZoxhaIuhnN78dFb+uJKn1j7Fw188zLMbnuXGnjdyaedLiYk4RabHKRWgP3dKOPwC9HCnhEMBWkRE5JSj0CwYYxjcbDCDmw3myx+/ZPq66fzjy3/wzIZnuLHHjYzrMo5Yzyn0dCOXG9oOdZaiAL15gbeE4y2IiHZGoLuPhc7nKUCLiFQRay3GmHA3Q04R1tqQ9ldoFj8Dmg5gQNMBrN63munrpvPI6keYtXEW1/e4niu6XkGc5xS7Sc43QJ/3sE+AflMBWkSkCkVHRxc/WU/BWaqbtZaDBw8SHR1d8c5eCs0SVL8m/Xh6xNOs/Wkt09dP59GvHuW5Tc9xfffrubLrlcRHnoLhMDBA71pZMgtHUYDu6C3hUIAWEQlJy5Yt2b17N/v37w93U6pFdnZ2SAHtVFcT/RUdHU3Lli0rvb9Cs5Qr5bQUpg+fzob9G5i+fjqPr3mc5zY9x7Xdr+XqbldTL7JeuJsYHi43tDndWc77X9j1eUmA3rooIECPhChNMyQiUh6Px0O7du3C3Yxqk5aWRp8+fcLdjDqjNvaXQrNUSq/GvXhy2JNsOriJGetm8J+1/+GlTS9xdferuabbNSRGJYa7ieHjcgUJ0AucGuiyAvT6ubB0Kmcd2Q1rWsKwByB5XLjfiYiIiJRBoVlC0qNRDx4/53G2HtrKjHUzmL5uOi9tfomrul5Fh4IO4W5e+PkF6IeDB+jGXeGnzVCQiwE4sgveus05XsFZRESkVlJolhPStWFX/n32v9n28zZmrp/JMxueIdJEsm31Nq7vcT0NoxuGu4nhVypAe2ugv3wabKH/vnlZ8MGDCs0iIiK1lCvcDZC6rXODzvzzrH/y+pjX6RnTk+c2Psd588/jkVWPcCDrQLibV3u4XNBmCFzwDyhripuje+CpobD4TtgwD47srtk2ioiISJk00ixVomODjtzQ+AYe6PMAz6x/hhc3v8icrXO4tPOljO85nsaxjcPdxNojsaVTkhEoqh7ENYZ1c+DLZ7z7toLWg73LEGjczQngIiIiUqMUmqVKtU9sz/874/8xsfdEnl7/NLO3zmbu13O5tPOl3NjzRprGNQ13E8Nv2ANODXNeVsk6TwyMesQpzyjIh30bnTmhv18B330CG15z9otOhFaDSkJ0877g0RRGIiIi1U2hWapFm3pteOhXDzGx90Se2fAMc7+ey2vbXuPXnX7NTT1voll8s3A3MXyK6paXTsUe2Y1JDJg9wx0BzVOcZfAkp5zj5/SSEP3957B9iXffSGjepyREtxoEsaonFxERqWrVFpqNMbOA0cBP1tqe3nXTgAuBXGAHcKO19nB1tUHCr1VCK/5y+l+YkDyBZzc8y/zt85m/fT5jO47l5l430yK+RbibGB7J4yB5HB+npZGamlr+vsZAw3bOknKlsy7zoHNj4a7PnRC94j/w2WPOtsZdS0J068FQv41zDhERETlh1TnS/Dzwf8CLPuveB+631uYbY/4XuB+4txrbILVEi/gWPDDkASYkT+CZDc/w+vbXWbB9ARd2uJBbet1Cq3qtwt3EuiWuEXS9wFnAKfX4YU3JSPTGN2D18862hGb+IbpJT+fhLCIiIlJp1RaarbXLjDFtA9Yt8Xn5OXBpdV1faqemcU2ZPHgyt/S6hec2Pce8bfN4c8ebjGo/ignJE2hTr024m1g3eWJKprcDKCyE/VtKQvT33icWAkQmQKsBJSG6RT+IjAtf20VEROoAY8ua/qoqTu6E5kVF5RkB294CXrXWvlzGsROACQBNmjTpN2fOnGprZ1mOHTtGfHx8jV+3rjqR/jqSf4SlR5fy6bFPybf59I/rz4jEETT1nBo3DNbkZywqez+JR7aQeGQziUe2EJe5E4Ol0Lg5Ft+eI4ndOZLYjSOJ3ciLrF8jbQqV/p8MjforNOqv0Ki/QqP+Ck04++vss89eba3tH7g+LKHZGPMnoD/wa1uJBvTv39+uWrWqehpZjrTK1JtKsV/SXweyDvDipheZ8/UcsvOzOa/teUxInkDHBh2rtpG1TFg/Y1mHYfeXJaPRu1dBQY6zrVFHn5sLB0OjDrWiLlr/T4ZG/RUa9Vdo1F+hUX+FJpz9ZYwJGpprfPYMY8wNODcIDqtMYJZTQ1JMEv/T/3+4oecNvLjpRWZvnc276e9ybptzmZA8gS4Nu4S7iSefmPrQ6VxnAcjPgb3rSkL01sWwxvuHoNikkhDdZgg0TQa3J2xNFxERqWk1GpqNMecB9wBnWWuP1+S1pW5oGN2QO/rdwQ09buClLS/x3y3/ZcnOJQxrPYxJvSfRtWHXcDfx5BURBa0GOsvQ25266IPbfeqiV8DWRc6+nlinFrqoLrrlAIiuF972i4iIVKPqnHJuNpAKJBljdgNTcGbLiALeN86fej+31k6qrjZI3VU/uj6/7/N7rut+Ha9seYWXN7/M0u+XktoqlUnJk+iR1CPcTTz5uVzQuIuz9LvBWXd0b8k0d9+vgE/+CbYQjMuZlaMoRLceAvVO4bm4RUTkpBNyaDbGNABaWWvXl7eftfbKIKufDfV6cmpLjErkNym/4dru1/LfLf/lxc0vcsXiKzijxRlM6j2J5MbJ4W7iqaVeM+hxsbMA5GR466K9IXrNS/DFDGdb/Tb+ITqpsx4BLiIidValQrMxJg0Y491/NfCTMeYza+3/VGPbRIolRCYwsfdEru52NXO+nsMLm17g6revZmjzoUzqPYmU01LC3cRTU1QCdDjHWQAK8uDH9SUhesdSWO+d+SamgXNTYfEjwFOckhAREZE6oLIjzYnW2qPGmJuBF621U4wx5Y40i1SH+Mh4bu51M1d1vao4PF/7zrUMajaIW3vfSr8m/cLdxFOb2+PUOrfoB0N+6zwC/NC33rpob230tne8+0Z566KLZukY6NycKCIiUgtVNjRHGGOaAeOAP1Vje0QqJdYTy/ie47miyxW8tu01ntv4HDe8ewMDmg5gUvIkBjQdgKkFU6Sd8oxxpqtr1AH6XOOsO7bfpy76c1j+OHz6L8DAad0DHgGuJ0WKiEjtUNnQPBV4D/jMWvulMaY9sL36miVSObGeWK7vcT2Xd7mcedvmMWvjLG5achN9T+vLpN6TGNxssMJzbRPfGLpd6CwAucdhz+qSko71c2GV9/aHei29IdobpG1B+NotIiKntEqFZmvta8BrPq+/BS6prkaJhCo6Ipprul/DZV0u4/Xtr/PshmeZ8P4EejfuzaTekxjafKjCc20VGQvtznAWgMIC2LepJETv/Aw2zgPgV+442HN6SYhu0dd5hLiIiEg1q+yNgO2Bx4DBgAVWAH/whmeRWiPKHcWVXa/kkk6XsOCbBTyz4Rlu/eBWeiX1YlLvSZzR4gyF59rO5YZmyc4yaIJTF334e/j+c376fD7Nj3wPH/7Vu68HmvfxqYseBHGNwtt+ERE5KVW2POO/wJOAd54prgBmA4Oqo1Eiv1SkO5JxXcZxcceLeXPHmzy94Wl+u/S3dGvYjUm9J3F2q7MVnusKY6BBG2jQhm0/N6F5aiocPwS7vii5uXDldKc2GiCpi39ddIO2teIR4CIiUrdVNjTHWmtf8nn9sjHm7upokEhV8rg9XNL5EsZ0HMOiHYt4esPT3P7R7XRp0IVJvSdxTutzcBnNHVznxDaELuc5C0BeNvywpiREb14AX73gbItv6h+im/QEd40+DFVERE4Clf3J8Y4x5j5gDk55xuXA28aYhgDW2kPV1D6RKuFxebi408Vc2OFC3vnuHWaun8kf0v5ApwadmJg8kXPbnKvwXJd5oqHNEGcB5xHg+7f6PALcG6QBIuOdx34XPwK8P0TGha3pIiJSN1Q2NI/zfp0YsP4KnBDdvspaJFKNIlwRXNjhQi5odwHvpr/LjPUzuOvju+iQ2IEJyRMY2XYkbpc73M2UX8rlgibdnWXATc66I7tLAvT3n0Pa3wELxg3Nevs8vXAwxJ8W1uaLiEjtU9nZM9pVd0NEapLb5WZU+1Gc1/Y83v/+fWasm8G9n9zLU+ueYkLyBM5vdz4RLv0J/6SS2BJ6XeosANlHYNeXJaPRq56Fz590tjVs7w3R3qVRB9VFi4ic4io7e0Ys8D9Aa2vtBGNMJ6CLtXZRtbZOpJq5XW7Oa3seI9qMYOn3S5m+bjp//PSPTF83nVuSb2FU+1F4XJ5wN1OqQ3QidBruLAD5ubB3XUmI/vodWPuKsy02yX++6KbJEBEZvraLiEiNq+xQ2nPAauB07+s9OPM2KzTLScFlXJzb5lyGtR7GR7s+Ysa6Gfz5sz8zY90Mbkm+hQvbX4jHrfB8UouIhFYDnGXobc5Udwe2+9RFr4Ct3n/yImKcWuiiIN1yIETXC2/7RUSkWlU2NHew1l5ujLkSwFp73Gi+LjkJuYyLYa2HcU6rc1i2exlPrXuKKcunMGPdDG7qdRNjO44l0q0RxlOCMdC4s7P0u95Zl/GjT130CvjkEbCFYFzQpIdPXfQQqNc8vO0XEZEqVdnQnGuMicG56Q9jTAcgp9paJRJmxhjOanUWZ7Y8k0/3fMr0ddP56+d/5ekNT3NTz5u4uNPFRLmjwt1MqWkJTaHHWGcByMmA3atKQvSaV+CLmc62+q39Q3RSF+cGRRERqZMqG5ofBN4FWhljXgGGAjdWV6NEagtjDGe0PINftfgVK/auYPq66fxt5d94ev3TjO81nks6XUJ0RHS4mynhEpUAHc52FoCCPPhxQ0mI3vERrH/V2RZd378uunkfiNAvXiIidUVlZ89YYoxZjfMYbQPcbq09UK0tE6lFjDGc3vx0hjQbwhc/fsH0ddN5+IuHeWbDM9zY40Yu63IZMREx4W6mhJvbAy36OsuQ3zh10Ye+LQnRu1bCtne9+0Y5+xU/AnwgxDQIb/tFRKRMlZ09Y6m1dhiwOMg6kVOGMYZBzQYxqNkgvvzxS2asm8G0VdN4duOz3NDjBi7vcjmxnthwN1NqC2Oc6eoadYA+VzvrMg844bnoBsPlT8Cn/3a2ndbd/+mFia001Z2ISC1Rbmg2xkQDsUCSMaYBzigzQD2gRTW3TaRWG9B0AAOaDuCrfV8xY/0M/rX6Xzy38Tmu63EdV3a9kjiPnjInQcQlQddRzgKQexx++KokRG+YB6tmOdvqtfAP0ad1Bz18R0QkLCoaaZ4I3AE0x5lyrkgG8H/V1CaROqVvk77MOHcGa39ay4z1M3jsq8d4ftPzXNfdCc8JkQnhbqLUZpGx0PZXzgJQWAA/bS4p6di5AjbOd7ZF1XPKOIrrovs6xwdaPxeWTuWsI7thTUsY9gAkjyu9n4hILbP428U89tVj7M3cS7N5zbi97+2Maj8q3M0CKg7Ny4G5wKXW2ieMMdcDlwDpwH+ruW0idUrKaSk8NfwpNh7YyPR103lizRM8v+l5ru12LVd3v5p6kZrHVyrB5YamvZxl4C1OXfSRXSUh+vvP4cOHvPt6oHmKT130YNixFN66DfKynD8NHtnlvAYFZxGp1RZ/u5gHlz9IdkE2AHsz9/Lg8gcBakVwrig0zwCGewPzmcDfgd8DKcBM4NLqbZ5I3dMzqSf/N+z/2HxwMzPWzeA/6/7Di5tf5OpuV3Nt92tJjEqs1b9JSy1jjDN9Xf3WJaH3+CHY7fMI8JUznNpoAFcEFOb7nyMvC5ZOVWgWkbCz1pJbmEtOQQ65BSVfcwty+eeX/ywOzEWyC7J57KvHasXPyIpCs9tae8j7/eXATGvtfGC+MWZttbZMpI7r3qg7j53zGFsPbWXm+pnMWD+Dlza/xMCmA1mxdwU5Bc5U57XtN2mpA2IbQueRzgKQlw171zoh+oMHgx9zZBfMvgoatIH6baBBW+/3rSFS9fcip4r8wvzikOobXHMKS4dYv+2BIbew/O1lnSe3MDfkNv+Y+WM19EToKgzNxpgIa20+MAyYEMKxIgJ0bdiVf6X+i+0/b2fm+pm8m/5uqX2yC7KZ9uU0OjXoRJwnjtiIWOI8cXr6oFSOJ7pkDugvn3UCcqCIaDi0A3Z8CPlZ/tviGnuDdGCgbgOJLZ2p9ETkF7PWkleYVxwy8wryygycvzTEBtsvryCPfJtfcUMrEOWOItIdSZQ7yu/7SFckke5I6kXWK1kXsJ/vvoHbp34+lUPZh0pdr2lc01/c5qpQUfCdDXxsjDkAZAGfABhjOgJHqrltIieVTg06Me2sabyX/h7Webimn4PZB7nkzUv81kW4IooDdPFXT6z/955Y4iKc7wO3Fx1XtC4mIgajKcxObsMeKK5pLuaJgQsfd8ozrIXM/fDzTji8E35Od5bDO52nG25aALag5FjjdmbxaFAUqtv6hOs2EN9E0+JJnVFQWEBuYW7ZwbO8YFqYW6mw67u/73my87LJezHvF7+HCBOBx+0pO7i6I0mITCgOsOXt5xdkXcG3R7mjiq8X5Y7C4/JU28+R7IJsv5pmgGh3NLf3vb1arheqckOztfZvxpilQDNgibW26Ce9C6e2WURC1DSuKXsz95Za3zC6IX8a9CeO5x8nMy+T43nHy/z+YPZBZ12es66yf+5yGZcTor1Buihkx0XEEeOJcYJ2GQG8rNDuMno0dK1SVLe8dCr2yG5MYsDsGcZA/GnO0mpA6eML8uHoHm+g3lkSqH/eCdvfh2P7/PePiHZKPBq0DRit9n6NqV+Nb1bqEmst+YX5JcGzsHTwPJEQW5lR2KL98wPr/U9A4Aip3+ipO5L4yHgauhuWCqk/7vmRjm07lhti/QJrkBAb6Y4kwnXy/qG/qESx+J6fuNp1z0+FPW+t/TzIum3V0xyRk9/tfW8P+pv0PQPuYUTbESd0zrzCvOIQXRSuM/MyOZ5fEqzLCuCZeZn8ePxHv/2yAv98X46YiBj/EO7zfeBId+C6UmHcE4vHpVKAXyx5HCSP4+O0NFJTU0M71h1RMqrcLsj2vCw4/H1AoPZ+/X4l5AT8ETI6MUig9r6u39opLZEaUWgLyxwVLSvEBoZQ3/U7D+zk7Y/frlQpQdHXYH9lC4XLuPxCZbBR0VhPbMWjqz6jsGWNrpYKsd5R1hMdKEhLSyO1T+ovev+nglHtRzGq/Sinv0L996uanby/rojUUtXxm7TH5SExKpHEqMQqaWNBYQFZ+VlBg7ZvGC8K2pn5mSWv8zP5Oftndmfs9juusj8sI12RZY50Hz14lOUrl5c56h0stEe6IlWSUpU8MdC4i7MEk/Vz6RHqwzvhpy2w7T3w3gBbLKFZ8BHqBm2cspCT5GEu1lrybX6lR1F9Q2xlb7QqXl8YfL+8wl9eGuBxlZQF2DxLvUP1/AJsXEQcDaMalj16+gtCrMftIcJE6P9nCZtqC83GmFnAaOAna21P77qGwKtAW5y5nsdZa3+urjaI1Fa1+TdpALfLTXxkPPGR8VVyPmst2QXZ5Qbw4pFv3wDu3X4s9xj7MvdxKPsQW7/dyvG845W+mcVt3JUa6Y7xxBAXEVdmAC/eT3Xh5Ytp4CzNU0pvKyx0yjsCA/XPO2HncuehLL6/XLk8zo2IpQJ1W2eJbVTpeupg01wFhs4tWVso/L6wzJB6QrMF+ATYQlv4i7rWYCqsOU30JFa6NrXM0dUg5QdFX31HWWvrv18i1aU6R5qfx3lq4Is+6+4DllprHzbG3Od9fW81tkFEagFjDDERMcRExEDMiZ+n6Id00R3opcpOvKHbN5wXj4YHBPSfc3722y8ncAS0rPeC816KAnjR9743ZBaPdPu8Li+0u6t4NLU2zgNePM1VZAw5Se3JbdCSnNb9/UdFczPJyfiB3Iy95B7bR07mT+QcP0Bu1o/k7NpK7nfZ5BhDjjHkGkOuO4KcyFhyPdHkREQ5r11ucoEcLDmFucU3bFV6mqufyt4U4YooM1BGuaOIiYihflT9CmcIqOwobGCIjXBplFUknKotNFtrlxlj2gasvghI9X7/ApCGQrOIhMgYUxw4GtCgSs6ZV5jnlKQEjHoHBvKi7Vn5WX5hfP/x/aTnpReH+FDqwqPd0RXPilJWHXiE/w2dH+/+mIc+f6jUE7XyC/M5p/U5Fd9oFeRP+yc6W4Dv9gLfGTlCFQHERxLljifSeIgyLqKAyMJCIgsLiMrPIjL7MPUK8om0lihrna/uKKIiE/BENyAqtgFRsUlExiURFdeEyPimRAXUvm5ct5HBAwaXGVyr+pcbEalbarqmuYm1tmjagB+BJjV8fRGRoDwuD55IT5U97rzQFhYH62BlJ76vg5WrHM4+zA/5P/iNhp/on/ezC7KZ/Nlk+OzE34/buMsfFY2Iop67XqWmuapohoDA/So1zZW1cPygt+Qj3X9KvQM74cga/yclGhckNPcr+Ug4fJxux7o56+Kagkszw4hICVMyi1w1nNwZaV7kU9N82Fpb32f7z9baoMNExpgJeB+m0qRJk35z5syptnaW5dixY8THV01N56lA/RU69VloTuX+staSZ/PIsTlkF2aTY51R4WybTU5hTvHr+T/PL/McFze42Jnj1XjwGOemqgj8X/t+Ld7HROA2dXyU1RYQlXOI6Ox9xGTtIzp7n9/3Ubn+D1QoNB6yoxuTHd2ErJimZEef5v2+CdnRTciPiD/l56c+lf9/PBHqr9CEs7/OPvvs1dba/oHra3qkeZ8xppm1dq8xphnlVI9Za2cCMwH69+9vw3GzgW5yCI36K3Tqs9Covyq2fN7yoPOAN4trxtQxU8PQojoiL5uV789jUKemcDgd18/pxP68k9jDO+HQCsg+7L9/VL3SU+gVva7fGiJjw/EuapT+fwyN+is0tbG/ajo0vwlcDzzs/bqwhq8vInJSK2se8NryRK1ayxNNVmxL6JQafHv2kYCnKHq/P7AdvvkA8rP99487zf9x5H5T6bV05sMWkTqlOqecm41z01+SMWY3MAUnLM81xtwE7ATGVdf1RURORbX9iVp1VnQiNEt2lkDWwrGfAgK19+uulbDx9dKPJi81lV5b70Nl2kJc41O+9EOkNqrO2TOuLGPTsOq6poiI1P55wE86xkBCE2dpNbD09oI859HkviPURV+3vQeZAZWKnlinxCNwhLqoDCS6am5WFZHQ6O9DIiIi1cntKXkgSzC5mSWPJg/20JfcDP/9YxqUEajbQv1WEBFVve9H5BSl0CwiIhJOkXFwWjdnCWSt99Hk6aUD9Y8b4et3oMD3wS3GeTS539MTfcJ1QrOT5tHkIjVNoVlERKS2MgZiGzpLi76ltxcWQsbe0oH653RI/wTWv0qpR5PXbxU8UNdv61xH9dQiQSk0i4iI1FUuFyS2cJY2p5fenp8DR3b7j1QXfb95IWT5z09NZHzpGmrfYB0ZVwNvSqR2UmgWERE5WUVEQaMOzhJMTkbpEerDO+HQd/BtGuQd998/Nql4hLrdUQMJ6SWBOrGVU78tcpJSaBYRETlVRSVA057OEshayDzgM5Veekm43rOaVod3wffzSvY3LmcO6mBzU9dvA/FN9GhyqdMUmkVERKQ0YyC+sbO0LPVEYT75cCln9e0UfCq9bz6AYz/6HxARXf5UejH1a+JdiZwwhWYREREJmXW5vSG4NbQLskNeFhze5TNCnV4SrHd/4Txl0Vd0YkCgblsSqOu3Bk90tb8nkfIoNIuIiEjV88RA487OEkzWz8Hnpt7/NWxbAgU5/vvHNw0+Qt2gDdRroan0pNopNIuIiEjNi2ngLM1TSm8rLIRj+0oH6sM74fsVsHEe2MKS/V0RzqPJSwVq79e4JE2lJ7+YQrOIiIjULi4X1GvmLK0Hl95ekAdHdgUZqU6HrW/D8QP++3vinBKPsqbSi0qoiXcldZxCs4iIiNQtbg80bO8sweQccx5NHhiof94J6Z9C7jH//WMalhGo2zpT6UVEVvMbkrpAoVlEREROLlHx0KS7swSyFo4fgsPppQP13nWwZREU5vkcYJya6bKm0ktopqn0ThEKzSIiInLqMAbiGjlLi36ltxcWOI8mDwzUh3c6D3zJ2Ivfo8ndkcGn0isaqY5poHrqk4RCs4iIiEgRl9u5qTCxJTC09Pb8HJ+p9NL966p/+MqZFcRXVD2o34YeBbGQvcQ/UNdvDZGx1f+epEooNIuIiIhUVkQUJHV0lmCyjwSdSi92z2ZYNQvys/z3jzst+Ah1gzbOExbdimq1hf5LiIiIiFSV6ERoluwsPr5MSyP1rLMgc79PyUd6SbDe/SVsegNsQclBxg2JLXwCdduSQF2/DcSfptKPGqTQLCIiIlITjHGCbvxp0Gpg6e0F+XB0d/CHvmxbApk/+e8fEVMylZ7vCHVRyI5OrJG3dapQaBYRERGpDdwRJY8PDyb3eNlT6X3/OeQc9d8/un6QqfTaOl8TW+nR5CFSaBYRERGpCyJj4bSuzhLIWucmxGCBet8m+PodKMj1PyahWfCHvdRvA/Wa69HkARSaRUREROo6YyC2obM071N6e2EhHPsx+FR66Z/C+lfxm0rP5YH6rYIE6rbO19hGp1w9tUKziIiIyMnO5XJGj+s1hzZDSm/Pz3UeTV40Uv1zesn3W96C4wf994+ML2Nuau/XqPgaeVs1SaFZRERE5FQXEQmNOjhLMDkZTj217wh1Ubj+9mPIy/TfPzYpeKAuejS52xP8OuvnwtKpnHVkN6xpCcMegORxVfhGT5xCs4iIiIiULyoBmvRwlkDWOiPRP6f7j1Af3gk/rIEtb0Jhfsn+xuU8mjxwbupDO+CzxyA/GwPOyPdbtznH1ILgrNAsIiIiIifOGIhLcpaW/UtvLyyAo3uCT6W340Pvo8nLkJcFS6cqNIuIiIjISc7lduaTrt8aOKP09rws59HkTw7E72bEIkd2V3cLK8UV7gaIiIiIyCnMEwONO0Niy+Dby1pfwxSaRURERCT8hj3gBGhfnhhnfS2g0CwiIiIi4Zc8Di58HBJbYTHOLBsXPl4r6pkhTDXNxpg/ADfjFK5sAG601maHoy0iIiIiUkskj4PkcXyclkZqamq4W+OnxkeajTEtgNuA/tbanoAbuKKm2yEiIiIiUlnhKs+IAGKMMRFALPBDmNohIiIiIlIhY22QqT2q+6LG3A78DcgCllhrrw6yzwRgAkCTJk36zZkzp2YbCRw7doz4+JPvMZDVRf0VOvVZaNRfoVF/hUb9FRr1V2jUX6EJZ3+dffbZq621pSacrvHQbIxpAMwHLgcOA68B86y1L5d1TP/+/e2qVatqpoE+0mphPU1tpv4KnfosNOqv0Ki/QqP+Co36KzTqr9CEs7+MMUFDczjKM4YD31lr91tr84DXgdPD0A4RERERkUoJR2j+HhhsjIk1xhhgGLAlDO0QEREREamUGg/N1tqVwDzgK5zp5lzAzJpuh4iIiIhIZYVlnmZr7RRgSjiuXRkL1uxh2ntfs+dwFi0+/5C7R3ZhbJ8W4W6WiIiIiIRJWEJzbbZgzR7uf30DWXkFAOw5nMX9r28AUHAWEREROUUpNAeY9t7XxYG5SFZeAXfPW8dLn+/E4zZERriJdBs8bhcet4vICO9Xtyn+vmh9pNtVfIzHZ3uk24UnwtkWFfSYovOa4v1dLhOmXhERERE5tSk0B/jhcFbQ9XkFlhiPm9z8Qo5m5ZFXUEhufiF5BYXkFVhyir931ucXVv1UfhGuoqDuE9x9QrYnwkWU24UnwvgF85LgXhLYS4d74/cLgMft8gnzJdcpPoc30PuuC8ec3yIiIiI1QaE5QPP6MewJEpxb1I/h5ZsHVfo8hYWWvMKiYG2Lw3SuN1jn5VtyCwrIzbd+ATy3wP+YvILCUoE8r8D67Ffoc25LXn4hOXmFZGTn+50zL9+/DbkFhVR1xjWAZ+k7xSG93JHzUutdREYYv18ASoK7f6D3D/3G71x+5y46X1Hgr0Wj9aqbFxERqVsUmgPcPbKLX00zQIzHzd0ju4R0HpfLEOVyExXhruomVpn8guABvCSoB4T9gNCf411XdMy2Hd/RvGXrMn4J8F7Le45jOflB1+f5XafqR67dLlNqhNxv1L3cEptgo/Al4Tyw9Kasc32yfT+PfrCdnPxCwKmbv+/19eQXFHJJv5Y4MzGKiIhIbaLQHKBotK94FLB+zEk7ChjhdhHhhhiqJtinufaQmtq1Ss4FYK0tDvVF4TzHJ2gHH4V3RtxLj8L7jLb77ltOiU1mTr73XAVljPxbCqqoDCc7r5C75q3nrnnrS0bqA0bji0bPg4+2u0v9MlBW6U2ZI/V+28su13HXktF6ERGRmqTQHMTYPi0Y26eFHnkZZsYYp2QjwgVR4W5NcAWFpYN4YOmNf2mO5ZYXy34k/G3DOgWE+UJy8/1/ccj1C/UlvwyUVcZT1VyGcgJ4GTfJBpTelBxniHS7S43WBwb+otff/FxAo91H/Ep9ikf9ff4qcKqP1qv8R0Tqqtr875dCs8gv4HYZ3C430Z7Kj9a3KKdu/n/O7VyVzSserS8O0wE188FKb8oK6b7HlFd647vP8ayCUqP+gaE+5NH6lZ9WuEvRqLunVLgva9S9nFr7gDKb8m649f0FInBWHN/rVedovabNFJG6qrb/+6XQLFLDqqpuvjL8RutrKd/R+rz8sgN4bkEhq75aS7cevUqXyviM5Pv+IlAS7oMH/uO5+RzJKn2jbuANt1XNZSg9Uu8zGu93c2txIC8rpJeMxkdFuHjiw2+CTpv5l7c2UagZbsq1ZU8eh77aHe5m1Bnqr9Covyr210Wbg/77Ne29rxWaRU5Fp1LdfGWEMlqfuyuC1O5NaqBVJay15BcGBmtbqi7edzQ+pFr7gJAeeMPt8aw8v18A8vILS9XaV2aKy5+P5/E/c9fVQI/VcRvURyFRf4VG/XVCypoOuKYpNIuEgerm6w5jTPFobmxkuFsTXNFofV5BISP+vYy9R7JL7dOkXhRzJw4JQ+vqjpUrVzJoUOWnFj3Vqb9Co/6q2LgZK9h3NKfU+ub1Y8LQmtIUmkVE6jjf0fp7z+satPzn/vO70aZRXBhbWft9F+tSH4VA/RUa9VfF7j+/W42VL54IhWYRkZOIyn9EpK6q7f9+KTSLiJxkVP4jInVVbf73q/beUi8iIiIiUksoNIuIiIiIVEChWURERESkAsbWgcnujTH7gZ1huHQScCAM162r1F+hU5+FRv0VGvVXaNRfoVF/hUb9FZpw9lcba23jwJV1IjSHizFmlbW2f7jbUVeov0KnPguN+is06q/QqL9Co/4KjforNLWxv1SeISIiIiJSAYVmEREREZEKKDSXb2a4G1DHqL9Cpz4LjforNOqv0Ki/QqP+Co36KzS1rr9U0ywiIiIiUgGNNIuIiIiIVECh2csYM8sY85MxZqPPuobGmPeNMdu9XxuEs421SRn99aAxZo8xZq13uSCcbaxNjDGtjDEfGWM2G2M2GWNu967XZyyIcvpLn7EgjDHRxpgvjDHrvP31F+/6dsaYlcaYb4wxrxpjIsPd1tqgnP563hjznc/nKyXMTa1VjDFuY8waY8wi72t9vsoRpL/0+SqHMSbdGLPB2zervOtq1c9IheYSzwPnBay7D1hqre0ELPW+FsfzlO4vgH9ba1O8y9s13KbaLB+401rbHRgM/NYY0x19xspSVn+BPmPB5ADnWGt7AynAecaYwcD/4vRXR+Bn4KbwNbFWKau/AO72+XytDVcDa6nbgS0+r/X5Kl9gf4E+XxU529s3RVPN1aqfkQrNXtbaZcChgNUXAS94v38BGFuTbarNyugvKYO1dq+19ivv9xk4/5C2QJ+xoMrpLwnCOo55X3q8iwXOAeZ51+vz5VVOf0kZjDEtgVHAM97XBn2+yhTYX3LCatXPSIXm8jWx1u71fv8j0CScjakjfmeMWe8t31CpQRDGmLZAH2Al+oxVKKC/QJ+xoLx/Cl4L/AS8D+wADltr87277Ea/eBQL7C9rbdHn62/ez9e/jTFR4WthrfMocA9Q6H3dCH2+yvMo/v1VRJ+vsllgiTFmtTFmgnddrfoZqdBcSdaZZkQjEeV7CuiA8+fOvcAjYW1NLWSMiQfmA3dYa4/6btNnrLQg/aXPWBmstQXW2hSgJTAQ6BreFtVugf1ljOkJ3I/TbwOAhsC94Wth7WGMGQ38ZK1dHe621AXl9Jc+X+X7lbW2L3A+Tknemb4ba8PPSIXm8u0zxjQD8H79KcztqdWstfu8P4gKgadxfnCLlzHGgxMAX7HWvu5drc9YGYL1lz5jFbPWHgY+AoYA9Y0xEd5NLYE94WpXbeXTX+d5y4KstTYHeA59vooMBcYYY9KBOThlGY+hz1dZSvWXMeZlfb7KZ63d4/36E/AGTv/Uqp+RCs3lexO43vv99cDCMLal1iv6YHtdDGwsa99Tjbf+71lgi7X2Xz6b9BkLoqz+0mcsOGNMY2NMfe/3McC5OHXgHwGXenfT58urjP7a6vPD2eDUTurzBVhr77fWtrTWtgWuAD601l6NPl9BldFf1+jzVTZjTJwxJqHoe2AETv/Uqp+RERXvcmowxswGUoEkY8xuYArwMDDXGHMTsBMYF74W1i5l9FeqdwodC6QDE8PVvlpoKHAtsMFbRwnwR/QZK0tZ/XWlPmNBNQNeMMa4cQZD5lprFxljNgNzjDEPAWtwfhGRsvvrQ2NMY8AAa4FJYWxjXXAv+nyF4hV9vsrUBHjD+X2CCOC/1tp3jTFfUot+RuqJgCIiIiIiFVB5hoiIiIhIBRSaRUREREQqoNAsIiIiIlIBhWYRERERkQooNIuIiIiIVEChWUTEhzHmT8aYTd5H3a41xgzyrn/GGNO9iq6RboxJqmCfPwa8Xl4V1w4XY8zYquo/EZFw0JRzIiJexpghwL+AVGttjjfYRlprf6ji66QD/a21B8rZ55i1Nr4qr1sdvA9qMN6nNJa33/PAImvtvBppmIhIFdNIs4hIiWbAAe9jbrHWHigKzMaYNGNMf+/3x4wx07wj0h8YYwZ6t39rjBnj3ecGY8z/FZ3YGLPIGJMaeEFjzAJjzGrvuSZ41z0MxHhHul8puqb3q/Fee6MxZoMx5nLv+lRvG+YZY7YaY17xBlqMMQ8bYzZ7R8//GaQNDxpjXjLGrDDGbDfG3OKz7W5jzJfeY//iXdfWGPO1MeZFnKd2tQo4n9/1jDGnA2OAad731MG7vOt9758YY7p6j33eGDPdGLPKGLPNGDM69P+MIiJVT08EFBEpsQR4wBizDfgAeNVa+3GQ/eJwHo17tzHmDeAhnEcxdwdewHn0a2WNt9Ye8j7O+UtjzHxr7X3GmN9Za1OC7P9rIAXoDSR5j1nm3dYH6AH8AHwGDDXGbMF55HhXa6013sdHB5EMDPa+tzXGmMVAT6ATMBDnKWZvGmPOBL73rr/eWvu570mMMY0Cr2etPWyMeROfkWZjzFJgkrV2u7cE5j/AOd7TtPVeswPwkTGmo7U2uxJ9KSJSbTTSLCLiZa09BvQDJgD7gVeNMTcE2TUXeNf7/QbgY2ttnvf7tiFe9jZjzDrgc5wR204V7P8rYLa1tsBauw/4GBjg3faFtXa3t1RirbctR4Bs4FljzK+B42Wcd6G1NstbMvIRTmgd4V3WAF8BXX3atzMwMHtVeD1jTDxwOvCa9zHpM3BG+YvMtdYWWmu3A996rysiElYaaRYR8WGtLQDSgDRjzAbgeuD5gN3ybMkNIYVAUTlHoTGm6N/VfPwHJqIDr+Ut1xgODLHWHjfGpAXbLwQ5Pt8XABHW2nxjzEBgGHAp8DtKRnR9Bd7gYnFGl/9urZ0R0O62QGawBlTyei7gcBkj6WW1RUQkrDTSLCLiZYzpYozxHelNAXae4OnSgRRjjMsY0wpn5DZQIvCzNzB3xSmPKJJnjPEEOeYT4HJjjNsY0xg4E/iirEZ4R3UTrbVvA3/AKesI5iJjTLS3vCIV+BJ4DxjvPQfGmBbGmNPKfsvlXi8DSACw1h4FvjPGXOY9xhhjfNt1mbffOgDtga/Lu6aISE3QSLOISIl44Alv3W8+8A1OqcaJ+Az4DtgMbMEpbwj0LjDJW3f8NU6JRpGZwHpjzFfW2qt91r8BDAHW4YzA3mOt/bHoRrogEoCFxphonJHj/yljv/U4ZRlJwF+9N0D+YIzpBqzw3lN4DLgGZxS7LGVdbw7wtDHmNpwR6KuBp4wxkwGPd/s6777f4/wiUA+n7ln1zCISdppyTkTkFGeMeRA4Zq0tNbNGGNryPJqaTkRqIZVniIiIiIhUQCPNIiIiIiIV0EiziIiIiEgFFJpFRERERCqg0CwiIiIiUgGFZhERERGRCig0i4iIiIhUQKFZRERERKQC/x8qkZh6gssOfwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 864x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "problem={\"taxi_pos\":(0,0),\"passenger_pos\":(3,3),\"domain_map\":DEFAULT_MAP}\n",
    "results = plot_steps_per_simulations(problem, [\"Monte Carlo Control\", \"Monte Carlo Tree Search\", \"RAVE\"],[10,20,30,50],10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06723efa",
   "metadata": {},
   "source": [
    "![performance](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/performance.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ec0f9d3",
   "metadata": {},
   "source": [
    "# TODO 1\n",
    "\n",
    "In this exercise you will try to see how the algorithms behave in a harder problem.\n",
    "In this case we will bring the passenger **farther** away from the taxi, so that the agent will need to take **more actions** to achieve the goal.\n",
    "\n",
    "Please, follow the instructions below:\n",
    "\n",
    "1. Define a problem with the passenger location = (6,7), the taxi location = (0,0) and the map = DEFAULT_MAP\n",
    "2. Run plot_steps_per_simulations with this new problem for Monte Carlo Control, Monte Carlo Tree Search and RAVE with 20 simulations and 3 experiments\n",
    "3. Compare the performance of the algorithms in this new problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48097737",
   "metadata": {},
   "source": [
    "# TODO 2\n",
    "\n",
    "In this exercise you will try to see how the algorithms behave in a bigger map. In this case, despite the taxi being as close to the passenger as before, the **map will be bigger** and thus the **Monte Carlo Simulations** are likely to be longer, more expensive and **noisier**.\n",
    "\n",
    "Please, follow the instructions below:\n",
    "\n",
    "1. Define a problem with the taxi location = (2,0), the passenger location = (3,3) and the map = BIG_MAP\n",
    "2. Run plot_steps_per_simulations with this new problem for Monte Carlo Control, Monte Carlo Tree Search and RAVE with 20 simulations and 3 experiments\n",
    "3. Compare the performance of the algorithms in this new problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ed4588b",
   "metadata": {},
   "source": [
    "# TODO 3 (Optional)\n",
    "\n",
    "In this exercise you will modify the way RAVE balances the two estimates. In particular you will need to modify the function with which we estimate the value of a node, so that the reliance on the RAVE value decays slower compared to what happens in RAVE with the same value of k.\n",
    "\n",
    "Please, follow the instructions below:\n",
    "\n",
    "1. Implement the TODO in the function _mc_rave of the class NewRAVENode\n",
    "2. Define a problem with the taxi location = (0,0), the passenger location = (3,3) and the map = DEFAULT_MAP\n",
    "3. Run plot_steps_per_simulations with this problem for NewRAVE with 20 simulations and 3 experiments\n",
    "4. Compare the performance of NewRAVE with the performance of Monte Carlo Control and Monte Carlo Tree Search and RAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "81bcc47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewRAVENode(RAVEMonteCarloTreeSearchNode):\n",
    "    def __init__(self, state, n_simulations=100, parent=None, parent_action=None, k=3, gamma=0.78):\n",
    "        super().__init__(state, n_simulations, parent,parent_action, k=k, gamma=gamma)\n",
    "    \n",
    "    def _mc_rave(self, child, c_param=0.8):\n",
    "        \"\"\"\n",
    "        Returns the MC-RAVE value for a child node.\n",
    "        :param child: The child node for which the MC-RAVE value is calculated.\n",
    "        :param c_param: Exploration parameter.\n",
    "        :return: The MC-RAVE value for the child node.\n",
    "        \"\"\"\n",
    "        uct =  float('inf') if (child.number_of_visits == 0 or self.number_of_visits == 0) else child.average_reward + \\\n",
    "        c_param*np.sqrt( \\\n",
    "          np.log(self.number_of_visits)/ child.number_of_visits \\\n",
    "        )\n",
    "        \n",
    "        ##############################\n",
    "        \"\"\"\n",
    "        TODO: modify beta so that the reliance on the RAVE value decays slower compared\n",
    "            to what happens in RAVE with the same value of k.\n",
    "            Hint: Can we replace the squared root with a different function?\n",
    "        \"\"\"\n",
    "        beta = None\n",
    "        ##############################\n",
    "        monte_carlo_value = float('inf') if uct == float('inf') \\\n",
    "                  else (1-beta)*uct\n",
    "        rave_value = float('inf') if child.number_of_visits_rave == 0 \\\n",
    "                  else beta*child.average_reward_rave\n",
    "        return monte_carlo_value + rave_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fffc8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(problem, algorithm, simulations, render:bool=False):\n",
    "    if not algorithm in [\"Monte Carlo Control\", \"Monte Carlo Tree Search\", \"RAVE\", \"NewRAVE\"]:\n",
    "        raise ValueError(\"Algorithm must be one among: Monte Carlo Control, Monte Carlo Tree Search, RAVE and NewRAVE\")\n",
    "      \n",
    "    env = new_environment_creator(taxi_pos=problem[\"taxi_pos\"], passenger_pos=problem[\"passenger_pos\"],domain_map=problem[\"domain_map\"])\n",
    "\n",
    "    if algorithm == \"Monte Carlo Control\":\n",
    "        # Monte Carlo Control\n",
    "        monte_carlo_control = MonteCarloControl(env, gamma=0.99, max_steps_per_episode=1_000)\n",
    "        step = monte_carlo_control.run_and_update(number_of_simulated_episodes=simulations, render=render)\n",
    "        return step\n",
    "\n",
    "    elif (algorithm==\"Monte Carlo Tree Search\"):\n",
    "        # Monte Carlo Tree Search\n",
    "        mcts = MonteCarloTreeSearchNode(state = env, n_simulations=simulations, gamma=0.78)\n",
    "        step = mcts.run_and_update(max_number_of_steps=1000, render=render)\n",
    "        return step\n",
    "    \n",
    "    elif (algorithm ==\"RAVE\"):\n",
    "        # RAVE\n",
    "        rave = RAVEMonteCarloTreeSearchNode(state = env, n_simulations = simulations, k=1, gamma=0.78)\n",
    "        step = rave.run_and_update(max_number_of_steps=1000, render=render)\n",
    "        return step\n",
    "    \n",
    "    elif (algorithm ==\"NewRAVE\"):\n",
    "        # NewRAVE\n",
    "        rave = NewRAVENode(state = env, n_simulations = simulations, k=1, gamma=0.78, new_rave=True)\n",
    "        step = rave.run_and_update(max_number_of_steps=1000, render=render)\n",
    "        return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a0c65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eecc88c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'git+https://github.com/sarah-keren/multi-taxi'\"\n"
     ]
    }
   ],
   "source": [
    "!pip install -q 'git+https://github.com/sarah-keren/multi-taxi'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a017c5",
   "metadata": {},
   "source": [
    "# Our Problem\n",
    "The problem that we will focus on today is to make a taxi move towards a single passenger and to pick it up, without dropping it at its destination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f954d24b",
   "metadata": {},
   "source": [
    "![Taxi_domain_show](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/Taxi_domain_show.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b615e",
   "metadata": {},
   "source": [
    "The only guidance that we will provide to our agent is a **reward function** that prizes the agent for picking up the passenger and that penalizes the agent for taking too many steps to reach the goal.\n",
    "\n",
    "In particular, the reward function is defined as follows:\n",
    "- +100 points for a successful drop-off\n",
    "- -5 point for every bad pick-up\n",
    "- -2 points for hitting an obstacle (e.g. a wall)\n",
    "- -1 for every step taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7716d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_taxi import Event\n",
    "\n",
    "customized_reward = {\n",
    "    Event.PICKUP: 100,\n",
    "    Event.BAD_PICKUP: -5,\n",
    "    Event.HIT_OBSTACLE: -2,\n",
    "    Event.STEP: -1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb119675",
   "metadata": {},
   "source": [
    "# Our Tools\n",
    "In our case we are **not** given access to a model, i.e. we don't know the transition function $ T: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S} $ and the reward function $ R: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S} $, where $\\mathcal{S}$ is the set of states and $\\mathcal{A}$ is the set of actions. \n",
    "\n",
    "We can only interact with the environment by taking actions and observing the resulting state and reward.\n",
    "This approach is thus based on Monte Carlo simulations and it is called **model-free**.\n",
    "\n",
    "## Monte Carlo Simulation\n",
    "To understand how Monte Carlo simulations work, let's see it in action.\n",
    "First, we will create a helper function which will allow us to create an environment by specifying the position of the taxi and the passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9818b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_taxi import multi_taxi_v0\n",
    "from multi_taxi.world.maps import DEFAULT_MAP, BIG_MAP\n",
    "\n",
    "def new_environment_creator(taxi_pos, passenger_pos, \n",
    "                            num_taxis=1,num_passengers=1, \n",
    "                            pickup_only=True, can_see_other_taxi_info=False, \n",
    "                            domain_map=DEFAULT_MAP,reward_table=customized_reward):\n",
    "    \"\"\"\n",
    "    A helper function to setup a new environment with predefined taxi and passenger locations\n",
    "    :param taxi_pos: The location of the taxi\n",
    "    :param passenger_pos: The location of the passenger\n",
    "    :param num_taxis: The number of taxis in the environment (default: 1)\n",
    "    :param num_passengers: The number of passengers in the environment (default: 1)\n",
    "    :param pickup_only: Whether the passenger can be dropped off or not (default: True)\n",
    "    :param can_see_other_taxi_info: Whether the taxi can see the other taxis' locations (default: False)\n",
    "    :param domain_map: The map of the environment (default: DEFAULT_MAP)\n",
    "    :param reward_table: The reward table of the environment (default: customized_reward)\n",
    "    :return: The new environment\n",
    "    \"\"\"\n",
    "    new_env = multi_taxi_v0.env(num_taxis=num_taxis, num_passengers=num_passengers, pickup_only=pickup_only, domain_map=domain_map,\n",
    "                                can_see_other_taxi_info=can_see_other_taxi_info,reward_table=reward_table, \n",
    "                                render_mode='human', allow_arrived_passengers_on_reset=False)\n",
    "    new_env.reset()\n",
    "\n",
    "    state = new_env.state()\n",
    "    state.taxis[0].location = taxi_pos\n",
    "    state.passengers[0].location = passenger_pos\n",
    "    new_env.unwrapped.set_state(state)\n",
    "\n",
    "    return new_env\n",
    "\n",
    "\n",
    "# Create the environment from which we will run the Monte Carlo Simulations\n",
    "taxi_position = (0,0)\n",
    "passenger_position = (3,3)\n",
    "env = new_environment_creator(taxi_position, passenger_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e787dcf4",
   "metadata": {},
   "source": [
    "Next, we will create a function that will allow us to simulate a single episode, i.e. a single run of the environment, by taking random actions until the episode ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fdfb4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from copy import deepcopy\n",
    "\n",
    "def run_monte_carlo_simulation(environment, gamma, max_number_of_steps: int =100_000, verbose: bool = False, render: bool = False):\n",
    "    \"\"\"\n",
    "    Runs a Monte Carlo simulation on a given environment\n",
    "    :param environment: The environment on which to run the simulation\n",
    "    :param gamma: The discount factor\n",
    "    :param max_number_of_steps: The maximum number of steps to run the simulation for (default: 100_000)\n",
    "    :param verbose: Whether to print the results of the simulation (default: False)\n",
    "    :param render: Whether to render the environment (default: False)\n",
    "    :return: The accumulated reward\n",
    "    \"\"\"\n",
    "    # Copy the environment so we don't change the original\n",
    "    env = deepcopy(environment)\n",
    "\n",
    "    number_of_actions = len(env.unwrapped.get_action_map('taxi_0').values())\n",
    "    accumulated_reward = 0\n",
    "    # Run the simulation for at most max_number_of_steps steps\n",
    "    for step in range(max_number_of_steps):\n",
    "        # Choose a random action\n",
    "        action = np.random.choice(number_of_actions)\n",
    "        # Take the action\n",
    "        env.step(action)\n",
    "        # Get the last state\n",
    "        obs, reward, done, trunc, info = env.last()\n",
    "        # Accumulate the reward\n",
    "        accumulated_reward += reward * (gamma ** step)\n",
    "        if render:\n",
    "            clear_output(wait=True)\n",
    "            if verbose:\n",
    "                print(f\"Step {step}: Accumulated reward: {accumulated_reward}\")\n",
    "            env.render()\n",
    "        # If the simulation is done, stop\n",
    "        if done:\n",
    "            break\n",
    "    del env\n",
    "\n",
    "    return accumulated_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb3f56",
   "metadata": {},
   "source": [
    "Now we are ready to simulate a single episode.\n",
    "\n",
    "Try to run the next cell multiple times and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d184b8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 330: Accumulated reward: -733.0\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 331, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-733.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAMMA = 1.0\n",
    "run_monte_carlo_simulation(env, gamma=GAMMA, render=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3d1af",
   "metadata": {},
   "source": [
    "It is possible to notice that some simulations are really long and some are short. This shows the **variance** to which Monte Carlo simulations are exposed. This variance will be propagated to our estimates and it is the reason why we will need to run many Monte Carlo simulations to obtain a good estimate.\n",
    "\n",
    "Another important thing that we need to pay attention to is the impact of the **discount factor** $\\gamma$. Try to see what happens as you lower the value of $\\gamma$.\n",
    "\n",
    "Run the next cell multiple times and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe49c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 528: Accumulated reward: -7.384256467457955\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 529, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-7.384256467457955"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAMMA = 0.70\n",
    "run_monte_carlo_simulation(env, gamma=GAMMA, render=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402daae",
   "metadata": {},
   "source": [
    "As you can see, with a **lower value** of $\\gamma$ the actions taken after a **high number of steps** have a very **limited impact** on the accumulated reward. This is because the reward is discounted by a factor of $\\gamma$ at each step. For this reason, the agent will care less about the actions taken after a high number of steps, limiting the horizon of its planning. For example, the agent will not care too much if it reached the passenger in 1_000 steps or in 1_100 steps. This will lead to a more **short-sighted** behaviour. \n",
    "\n",
    "More importantly, it will lower the variance of our estimates, requiring less Monte Carlo simulations to converge.\n",
    "\n",
    "Now we will see how to use Monte Carlo simulations to solve our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba05337f",
   "metadata": {},
   "source": [
    "# Monte Carlo Control\n",
    "\n",
    "The most immediate way to exploit Monte Carlo simulations to solve our problem is to use them to estimate the **q-value function** $Q(s,a)$, i.e. the expected reward that we will get by executing action $a$ in the state $s$.\n",
    "\n",
    "## Algorithm Structure\n",
    "Monte Carlo Control follows the paradigm of Generalised Policy Iteration (GPI), which is composed of two main steps:\n",
    "1. **Policy Evaluation** (Monte Carlo Estimation for Action Values):\n",
    "   - Use Monte Carlo simulations to estimate $Q(s,a)$\n",
    "2. **Policy Improvement** (Greedy Policy Improvement):\n",
    "   - Improve the policy $\\pi$ by acting greedily with respect to $Q(s,a)$\n",
    "3. Repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2b713c",
   "metadata": {
    "id": "4b2b713c"
   },
   "source": [
    "![monte_carlo_control_schema](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/Monte_Carlo_Control_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d407c",
   "metadata": {},
   "source": [
    "On top of this basic algorithm, we will add a **decaying $\\epsilon$-greedy policy** to allow the agent to explore the environment.\n",
    "\n",
    "Here is the complete algorithm:\n",
    "\n",
    "P.S. Ignore the line \"Unless the pair $S_t, A_t$ appears in $S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}$\". This is just a technicality that can be ignored (this is a detail of the first-visit version of Monte Carlo Control, the every-visit version, which is the one we are going to use, does not check if this condition is verified)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a23c14",
   "metadata": {},
   "source": [
    "# Monte Carlo Control algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef176e",
   "metadata": {},
   "source": [
    "![monte_carlo_control_algorithm](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/Monte_carlo_control_algorithm_book.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d8ee6f",
   "metadata": {},
   "source": [
    "Now that we have seen the pseudocode, let's see how to implement it.\n",
    "\n",
    "We have chosen to make the agent run a certain number of episodes with which the policy is updated and then the agent will take a step in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1929d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "class MonteCarloControl:\n",
    "    def __init__(self, state, gamma:float = 0.9, train_episode_number:int=1_000,\n",
    "                 max_steps_per_episode:int = 1_000) -> None:\n",
    "        '''\n",
    "        :args state: environment with the initial state\n",
    "        :args gamma: discount factor\n",
    "        :args train_episode_number: number of episodes used to train the agent (it is only used in the function train)\n",
    "        :args max_steps_per_episode: maximum number of steps \n",
    "        '''\n",
    "        self.state = state\n",
    "        self.Q = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.N = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.visited = set()\n",
    "        self.episode_number = 0\n",
    "        self.gamma = gamma\n",
    "        self.train_episode_number = train_episode_number\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "\n",
    "    def _policy(self, state, epsilon = None):\n",
    "        \"\"\"\n",
    "        Compute the action to take in a given state using the epsilon-greedy policy\n",
    "        :param state: The state in which to take the action\n",
    "        :param epsilon: The epsilon value to use for the epsilon-greedy policy (default: 1/sqrt(episode_number + 1))\n",
    "        :return: The action to take\n",
    "        \"\"\"\n",
    "        # epsilon-greedy policy\n",
    "        if epsilon is None:\n",
    "            epsilon = 1/np.sqrt(self.episode_number + 1)\n",
    "        else:\n",
    "            epsilon = epsilon\n",
    "        number_of_actions = len(self.state.unwrapped.get_action_map('taxi_0').values())\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(number_of_actions)\n",
    "        else:\n",
    "            # convert state to use as key in Q\n",
    "            state = state.unwrapped.state().taxis[0].location\n",
    "            \n",
    "            if len(self.Q[state]) == 0:\n",
    "                action = np.random.choice(number_of_actions)\n",
    "            else:\n",
    "                action = max(self.Q[state], key=self.Q[state].get)\n",
    "        return action\n",
    "    \n",
    "    def _update(self, episode):\n",
    "        \"\"\"\n",
    "        Update the Q values using the episode\n",
    "        :param episode: The episode to use to update the Q values\n",
    "        \"\"\"\n",
    "        G = 0\n",
    "        for i, (state, action, reward) in enumerate(reversed(episode)):\n",
    "            G = (self.gamma**i) * G + reward\n",
    "            # convert state to use as key in Q\n",
    "            state = state.unwrapped.state().taxis[0].location\n",
    "            self.visited.add(state)\n",
    "            self.N[state][action] += 1\n",
    "            self.Q[state][action] += (G - self.Q[state][action]) / self.N[state][action]\n",
    "    \n",
    "    \n",
    "    def _train(self, starting_state):\n",
    "        \"\"\"\n",
    "        Generate an episode from the starting state and update the Q values\n",
    "        :args starting_state: environment with the initial state\n",
    "        \"\"\"\n",
    "        self.episode_number += 1\n",
    "        current_episode_state = deepcopy(starting_state)\n",
    "        episode = []\n",
    "        for _ in range(self.max_steps_per_episode):\n",
    "\n",
    "            action = self._policy(current_episode_state)\n",
    "            old_state = deepcopy(current_episode_state)\n",
    "            current_episode_state.step(action)\n",
    "            obs, reward, done, trunc, info = current_episode_state.last()\n",
    "            episode.append((old_state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "        self._update(episode)\n",
    "    \n",
    "    def _step(self, current_state, render:bool, step_number:int):\n",
    "        \"\"\"\n",
    "        Make a step in the environment using the current policy\n",
    "        :args current_state: environment with the current state\n",
    "        :args render: if True render the environment after the step (default False)\n",
    "        :args verbose: if True print the action executed and the Q values (default False)\n",
    "        \"\"\"\n",
    "        action = self._policy(current_state, epsilon=0)\n",
    "        old_state = current_state.unwrapped.state().taxis[0].location\n",
    "        current_state.step(action)\n",
    "        obs, reward, done, trunc, info = current_state.last()\n",
    "        if render:\n",
    "            time.sleep(1)\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Step number: {step_number}\")\n",
    "            print(f\"Action executed: {current_state.unwrapped.get_action_meanings('taxi_0')[action]}\")\n",
    "            print(\"Names: | Average reward: | Number of visits:\")\n",
    "            for act, action_name in current_state.unwrapped.get_action_meanings('taxi_0').items():\n",
    "                if act == action:\n",
    "                    print(f\"{action_name}| {round(self.Q[old_state][act], 3)} | {self.N[old_state][act]} <--\")\n",
    "                else:\n",
    "                    print(f\"{action_name}| {round(self.Q[old_state][act], 3)} | {self.N[old_state][act]}\")\n",
    "            current_state.unwrapped.render()\n",
    "        return current_state, done\n",
    "\n",
    "    def run_and_update(self, number_of_simulated_episodes:int=100, max_number_of_steps:int=150, render:bool=False)-> int: \n",
    "        \"\"\"\n",
    "        Continuosly train the policy for number_of_simulated_episodes and then make a step until the simulation is done\n",
    "        or until max_number_of_steps is reached\n",
    "        :args number_of_simulated_episodes: number of episodes used to train the agent during each step (default 100)\n",
    "        :args max_number_of_steps: maximum number of steps allowed (default 150)\n",
    "        :return: the number of steps executed to reach the end of the simulation\n",
    "        \"\"\"\n",
    "        current_state = deepcopy(self.state)\n",
    "        steps = 0\n",
    "        for i in range(1,max_number_of_steps+1):\n",
    "            steps = i\n",
    "            # TRAINING STEP\n",
    "            self.episode_number = 0\n",
    "            for _ in range(number_of_simulated_episodes):\n",
    "                self._train(current_state)\n",
    "            \n",
    "            # ACTUAL STEP\n",
    "            current_state, done = self._step(current_state, render=render, step_number=i)\n",
    "            if done:\n",
    "                break\n",
    "        return steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ad491d",
   "metadata": {},
   "source": [
    "Let's see the agent in action.\n",
    "Try to play with GAMMA and run multiple times the next cell to see how the agent behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa1c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step number: 9\n",
      "Action executed: pickup\n",
      "Names: | Average reward: | Number of visits:\n",
      "south| 41.228 | 17\n",
      "north| 72.405 | 16\n",
      "east| 51.569 | 7\n",
      "west| 53.605 | 11\n",
      "pickup| 99.0 | 88 <--\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 9, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = new_environment_creator(taxi_pos=(0,0), passenger_pos=(3,3))\n",
    "GAMMA = 0.99\n",
    "monte_carlo_control = MonteCarloControl(env, gamma=GAMMA, max_steps_per_episode=1_000)\n",
    "monte_carlo_control.run_and_update(number_of_simulated_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a76d6e",
   "metadata": {},
   "source": [
    "### Considerations\n",
    "As you can see Monte Carlo Control behaves really well! Indeed it often **learns** **the optimal policy** or a policy that is really close to the optimal one (the optimal policy takes 7 steps).\n",
    "\n",
    "If so, are we done? Can we always use Monte Carlo Control to solve any problem?\n",
    "\n",
    "Unfortunately, the answer is no. Monte Carlo Control is not always applicable. In particular, it needs to store all the visited states and actions in memory. This is not always possible. For example, if we are playing a game like chess or Go where the state space and the action space are too big to fit in memory, we cannot store the Q-table that contains of all the states and actions.\n",
    "\n",
    "For this reason, we will need to use a different algorithm, which is more scalable and that collects trajectories in a wiser way: **Monte Carlo Tree Search**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cac94c",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search (MCTS)\n",
    "\n",
    "Monte Carlo Tree Search follows a different approach compared to Monte Carlo Control. Instead of storing all the visited states and actions, it will store only the **most promising** ones. This will allow us to scale to bigger problems.\n",
    "\n",
    "In order to do so, Monte Carlo Tree Search will build a **search tree** that will be used to store the most promising states and actions.\n",
    "\n",
    "A state and action are considered promising if they present good statistical values. Indeed, in order to manage the exploration-exploitation trade-off, Monte Carlo Tree Search will use the **Upper Confidence Bound** (UCB), which in our use case is called UCT, to select the most promising state and action.\n",
    "\n",
    "Let's see how it works in more detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b174d",
   "metadata": {
    "id": "4d2b174d"
   },
   "source": [
    "# The 4 steps of MCTS\n",
    "\n",
    "MCTS consists of four main steps: selection, expansion, simulation, and backpropagation.\n",
    "\n",
    "1. **Selection.** Select a leaf node using **tree policy**.\n",
    "2. **Expansion.** **Add children** to the selected leaf using unexplored actions\n",
    "3. **Rollout.** From the selected child **simulate** an **episode** using the **rollout policy**\n",
    "4. **Backpropagation**. **Update** the average **value** of the nodes starting **from** the selected **child** up **to** the **root** using the results of the rollout episode\n",
    "    - ATTENTION: No values are saved for the states and actions visited by the rollout policy beyond the tree! \n",
    "\n",
    "![mcts_4_steps](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/mcts_4_steps.png)\n",
    "\n",
    "MCTS **repeats** this cycle until **no time** is left (starting at the root node each time). **Finally**, MCTS **chooses** the **action** to make from the root node according to some statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be701a",
   "metadata": {
    "id": "05be701a"
   },
   "source": [
    "# Rollout policy and Tree policy\n",
    "\n",
    "* **Rollout policy**: a simple and cheap policy that takes care of generating actions in the **simulated trajectories**\n",
    "\n",
    "* **Tree policy**: a policy that decises how to grow the tree and that **selects** a **child node** that is most **promising** according to a selection policy, which balances exploration and exploitation of the search space. \n",
    "    * examples : $\\epsilon$-greedy, UCT\n",
    "    \n",
    "    * $ \\text{UCT} = \\underbrace{Q}_ {\\textbf{Exploitation}} + \\underbrace{C \\sqrt{\\frac{\\log(N)}{n}}}_{\\textbf{Exploration}} $\n",
    "   \n",
    "   Where: \n",
    "   - $Q$ is the average Q-value of the considered state-action pair\n",
    "   - $C$ is a constant that balances exploration and exploitation\n",
    "   - $N$ is the total number of times that the current node has been visited\n",
    "   - $n$ is the number of times that the considered child node has been visited\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13865a9",
   "metadata": {},
   "source": [
    "Now that we got the gist of how MCTS works we'll see its implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fc385bf",
   "metadata": {
    "id": "4fc385bf"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import time\n",
    "class MonteCarloTreeSearchNode():\n",
    "    def __init__(self, state, n_simulations=100, parent=None, parent_action=None, gamma = 0.9):\n",
    "        # environment holding the current state\n",
    "        self.state = state\n",
    "        # None for the root node, otherwise it is equal to the node it is derived from.\n",
    "        self.parent = parent\n",
    "        # None for the root node, otherwise it is equal to the action which its parent carried out.\n",
    "        self.parent_action = parent_action \n",
    "        # Contains all possible actions from the current node.\n",
    "        self.children = []\n",
    "        # Number of times current node is visited.\n",
    "        self.number_of_visits = 0 \n",
    "        self.average_reward = 0\n",
    "        # Set of all of the possible actions\n",
    "        self._untried_actions = self._get_legal_actions()\n",
    "        self.terminal_state = False\n",
    "        self.gamma = gamma\n",
    "        # Number of loops with the four stages\n",
    "        self.n_simulations = n_simulations\n",
    "        # Function used to evaluate the nodes (it will be used in rendering)\n",
    "        self._evaluate_function = self._uct\n",
    "    \n",
    "    def run_and_update(self, max_number_of_steps:int = 10_000, render:bool=False)-> int:\n",
    "        \"\"\"\n",
    "        Run Monte Carlo Tree Search for at most max_number_of_steps steps.\n",
    "        :args max_number_of_steps: maximum number of steps allowed (default 10_000)\n",
    "        :args render: if True render the environment after the step (default False)\n",
    "        \"\"\"\n",
    "        if render:\n",
    "            self.state.unwrapped.render()\n",
    "        steps = 0\n",
    "        selected_node = self._best_action()\n",
    "        for i in range(1,max_number_of_steps+1):\n",
    "            steps = i\n",
    "            if not render:\n",
    "                selected_node = selected_node._best_action()\n",
    "            else:\n",
    "                selected_node, before_move_node = selected_node._best_action(render=render)\n",
    "                time.sleep(1)\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Step number: {i+1}\")\n",
    "                print(f\"Action executed: {self.state.unwrapped.get_action_meanings('taxi_0')[selected_node.parent_action]}\")\n",
    "                print(\"Names: | Values: | Average reward: | Number of visits:\")\n",
    "                for child in before_move_node.children:\n",
    "                    action_name = self.state.unwrapped.get_action_meanings('taxi_0')[child.parent_action]\n",
    "                    if child.parent_action == selected_node.parent_action:\n",
    "                        print(f\"{action_name}| {round(before_move_node._evaluate_function(child, c_param=0.8), 3)} | {round(child.average_reward, 3)} | {child.number_of_visits} <--\")\n",
    "                    else:\n",
    "                        print(f\"{action_name}| {round(before_move_node._evaluate_function(child, c_param=0.8), 3)} | {round(child.average_reward, 3)} | {child.number_of_visits}\")\n",
    "                del before_move_node\n",
    "                selected_node.state.unwrapped.render()\n",
    "            \n",
    "            if selected_node.terminal_state:\n",
    "                break\n",
    "        \n",
    "        return steps+1\n",
    "    \n",
    "    def _best_action(self, render:bool=False):\n",
    "        \"\"\"\n",
    "        Execute the four stages to find the best action from the current state.\n",
    "        \"\"\"\n",
    "        for _ in range(self.n_simulations):\n",
    "            node = self._tree_policy()\n",
    "            reward = node._rollout()\n",
    "            node._backpropagate(reward)\n",
    "        if not render:\n",
    "            return self._best_child(c_param=0.)\n",
    "            #return self.children[np.argmax([child.number_of_visits for child in self.children])]\n",
    "        else:\n",
    "            return self._best_child(c_param=0.), self\n",
    "            #return self.children[np.argmax([child.number_of_visits for child in self.children])], self\n",
    "\n",
    "    def _tree_policy(self):\n",
    "        \"\"\"\n",
    "        Select node from which we run the rollout.\n",
    "        \"\"\"\n",
    "        current_node = self\n",
    "        while not current_node._is_terminal_node():\n",
    "            # if there is an action that has not been tried yet, return the child node corresponding to this action\n",
    "            if not current_node._is_fully_expanded():\n",
    "                return current_node._expand()\n",
    "            # else select the best child node\n",
    "            else:\n",
    "                current_node = current_node._best_child()\n",
    "        return current_node\n",
    "\n",
    "    def _is_terminal_node(self):\n",
    "        \"\"\"\n",
    "        This is used to check if the current node is terminal or not. \n",
    "        Terminal node is reached when the game is over.\n",
    "        \"\"\"\n",
    "        return self.terminal_state\n",
    "    \n",
    "    def _is_fully_expanded(self):\n",
    "        \"\"\"\n",
    "        All the actions are poped out of _untried_actions one by one. \n",
    "        When it becomes empty, that is when the size is zero, it is fully expanded.\n",
    "        \"\"\"\n",
    "        return len(self._untried_actions) == 0\n",
    "\n",
    "    def _expand(self):\n",
    "        \"\"\"\n",
    "        Select an action that has not been tried yet and return the corresponding child node.\n",
    "        \"\"\"\n",
    "        # select an action that has not been tried yet\n",
    "        action = self._untried_actions.pop()\n",
    "        \n",
    "        new_state = deepcopy(self.state)\n",
    "        new_state.step(action)\n",
    "        obs, reward, done, trunc, info = new_state.last()\n",
    "        child_node = MonteCarloTreeSearchNode(state = new_state, n_simulations=self.n_simulations,\n",
    "                                            parent=self, parent_action=action, gamma=self.gamma)\n",
    "        child_node.terminal_state = done\n",
    "        child_node.average_reward = reward\n",
    "        child_node.number_of_visits = 1\n",
    "        self.children.append(child_node)\n",
    "\n",
    "        return child_node\n",
    "    \n",
    "    def _best_child(self, c_param=0.8):\n",
    "        \"\"\"\n",
    "        Once fully expanded, this function selects the best child out of \n",
    "        the children array. The first term in the formula corresponds to \n",
    "        exploitation and the second term corresponds to exploration.\n",
    "        \"\"\"\n",
    "        choices_weights = [self._uct(child, c_param) for child in self.children] \n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "    \n",
    "    def _uct(self, child, c_param):\n",
    "        if self.number_of_visits == 0 or child.number_of_visits == 0:\n",
    "            return float(\"inf\")\n",
    "        return child.average_reward + \\\n",
    "                c_param*np.sqrt(np.log(self.number_of_visits)/child.number_of_visits)\n",
    "    \n",
    "    def _rollout(self):\n",
    "        \"\"\"\n",
    "        From the current state, entire game is simulated till there is an \n",
    "        outcome for the game. This outcome of the game is returned.\n",
    "        \"\"\"\n",
    "        current_rollout_state = deepcopy(self.state)\n",
    "\n",
    "        _reward = 0\n",
    "        step = 0\n",
    "        done = current_rollout_state.last()[2]\n",
    "        while not done:\n",
    "            possible_moves = self._get_legal_actions()\n",
    "            action = self._rollout_policy(possible_moves)\n",
    "            current_rollout_state.step(action)\n",
    "            obs, reward, done, trunc, info = current_rollout_state.last()\n",
    "            _reward += reward *(self.gamma ** step)\n",
    "            step += 1\n",
    "        return _reward\n",
    "    \n",
    "    def _get_legal_actions(self): \n",
    "        ''' \n",
    "        Returns a list of all of the possible actions from current state.\n",
    "        '''\n",
    "        return list(self.state.unwrapped.get_action_map('taxi_0').values())\n",
    "    \n",
    "    def _rollout_policy(self, possible_moves):\n",
    "        \"\"\"\n",
    "        Randomly selects a move out of possible moves.\n",
    "        \"\"\"\n",
    "        return possible_moves[np.random.randint(len(possible_moves))]\n",
    "\n",
    "    def _backpropagate(self, reward):\n",
    "        \"\"\"\n",
    "        In this step all the statistics for the nodes are updated. \n",
    "        Untill the parent node is reached, the number of visits for \n",
    "        each node is incremented by 1.\n",
    "        \"\"\"\n",
    "        self.number_of_visits += 1.\n",
    "        self.average_reward = self.average_reward + (1/self.number_of_visits)*(reward-self.average_reward)\n",
    "        if self.parent:\n",
    "            self.parent._backpropagate(reward)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b41c21",
   "metadata": {},
   "source": [
    "Now let's see how Monte Carlo Tree Search behaves in our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8711d99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step number: 11\n",
      "Action executed: pickup\n",
      "Names: | Values: | Average reward: | Number of visits:\n",
      "pickup| 1.495 | 1.303 | 76.0 <--\n",
      "west| -2.417 | -3.603 | 2.0\n",
      "east| -4.873 | -6.059 | 2.0\n",
      "north| -3.739 | -4.925 | 2.0\n",
      "south| -2.191 | -3.376 | 2.0\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 11, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = new_environment_creator(taxi_pos=(0,0), passenger_pos=(3,3))\n",
    "N_SIMULATIONS = 20\n",
    "GAMMA = 0.78\n",
    "mcts = MonteCarloTreeSearchNode(state=env, n_simulations=N_SIMULATIONS, gamma=GAMMA)\n",
    "mcts.run_and_update(max_number_of_steps=1000, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc340cd",
   "metadata": {},
   "source": [
    "As you can see, despite the fact that we are using much less data, Monte Carlo Tree Search is still able to learn a good policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a5e14",
   "metadata": {},
   "source": [
    "We've seen **Monte Carlo Control** and **Monte Carlo Tree Search** algorithms.\n",
    "\n",
    "In order to make a more rigorous comparison between the two, we define a helper function *evaluate_policy* that runs a specific algorithm (either MCC or MCTS) for a specific number of simulations and *plot_steps_per_simulations* which will take care of creating a plot out of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78402ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_policy(problem, algorithm, simulations, render:bool=True):\n",
    "    if not algorithm in [\"Monte Carlo Control\", \"Monte Carlo Tree Search\"]:\n",
    "        raise ValueError(\"Algorithm must be one among: Monte Carlo Control, Monte Carlo Tree Search\")\n",
    "      \n",
    "    env = new_environment_creator(taxi_pos=problem[\"taxi_pos\"], passenger_pos=problem[\"passenger_pos\"],domain_map=problem[\"domain_map\"])\n",
    "\n",
    "    if algorithm == \"Monte Carlo Control\":\n",
    "        # Monte Carlo Control\n",
    "        monte_carlo_control = MonteCarloControl(env, gamma=0.99, max_steps_per_episode=1_000)\n",
    "        step = monte_carlo_control.run_and_update(number_of_simulated_episodes=simulations, render=render)\n",
    "        return step\n",
    "\n",
    "    elif (algorithm==\"Monte Carlo Tree Search\"):\n",
    "        # Monte Carlo Tree Search\n",
    "        mcts = MonteCarloTreeSearchNode(state = env, n_simulations=simulations, gamma=0.78)\n",
    "        step = mcts.run_and_update(max_number_of_steps=1000, render=render)\n",
    "        return step\n",
    "    \n",
    "def plot_steps_per_simulations(filename, problem, algorithms, simulations_per_step, number_of_experiments, render:bool=False):\n",
    "\n",
    "    data_to_plot = [[] for _ in range(len(algorithms))]\n",
    "\n",
    "    \n",
    "    for sim in simulations_per_step:\n",
    "        for i, algorithm in enumerate(algorithms): \n",
    "            print(f\"{algorithm} for {sim} simulations\")\n",
    "            steps=0\n",
    "            for _ in range(number_of_experiments):\n",
    "                steps += evaluate_policy(problem, algorithm ,sim, render=render)\n",
    "            data_to_plot[i].append(steps/number_of_experiments)\n",
    "\n",
    "    print(\"Experiments' results:\")\n",
    "    print(data_to_plot)\n",
    "\n",
    "    _, ax = plt.subplots(1, 1, figsize=(12, 3))\n",
    "    ax.set_title(f\"Performance\")\n",
    "    for i, algorithm in enumerate(algorithms): \n",
    "        ax.plot(simulations_per_step, data_to_plot[i], '-o', label=algorithm)\n",
    "    ax.set_xlabel(\"Simulations per step\")\n",
    "    ax.set_ylabel(\"Steps\")\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    return data_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d6f9021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Control for 20 simulations\n",
      "Monte Carlo Tree Search for 20 simulations\n",
      "Monte Carlo Control for 50 simulations\n",
      "Monte Carlo Tree Search for 50 simulations\n",
      "Monte Carlo Control for 100 simulations\n",
      "Monte Carlo Tree Search for 100 simulations\n",
      "Experiments' results:\n",
      "[[8.2, 7.8, 7.0], [11.2, 10.4, 8.2]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[8.2, 7.8, 7.0], [11.2, 10.4, 8.2]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment to run the experiments (it takes around 6 minutes to run)\n",
    "\n",
    "# problem={\"taxi_pos\":(0,0),\"passenger_pos\":(3,3),\"domain_map\":DEFAULT_MAP}\n",
    "# plot_steps_per_simulations(\"mcc_vs_mcts.png\", problem, [\"Monte Carlo Control\",\"Monte Carlo Tree Search\"], [20,50,100], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765227db",
   "metadata": {},
   "source": [
    "![mcc_vs_mcts](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/mcc_vs_mcts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749aa08",
   "metadata": {},
   "source": [
    "As we can see from the graph, Monte Carlo Control (MCC) outperforms Monte Carlo Tree Search (MCTS).\n",
    "\n",
    "We relate it to the fact that **MCC** **uses a lot more data** than MCTS.\n",
    "\n",
    "Indeed, while **MCTS** maintains the values **only** for nodes (state-action pairs) **in the tree**, **MCC** maintains a value **for each state-action pair it ever encountered** and thus it has access to more data.\n",
    "\n",
    "This allows **MCC** to have a **better estimate** of the value of each state-action pair and therefore to have a better policy. However, this comes at the cost of **a lot more memory usage**.\n",
    "\n",
    "On the other hand, **MCTS** is more **scalable** and can be used in problems where the state-action space is too big to fit in memory. This is the case of games like chess or Go. In fact, MCTS (with the extension that we're going to present in the next section, i.e. RAVE, and with neural networks) is the algorithm that allowed DeepMind to create AlphaGo, the first AI that was able to beat the world champion in Go. \n",
    "\n",
    "However, **MCTS** does **not always find the optimal policy**, as we have seen in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7771ea",
   "metadata": {},
   "source": [
    "Without further ado, let's take a look at an extension of Monte Carlo Tree Search called **Rapid Action Value Estimation** (RAVE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fe3ed3",
   "metadata": {
    "id": "e3fe3ed3"
   },
   "source": [
    "# Rapid Action Value Estimation (RAVE)\n",
    "- RAVE is a family of heuristics which created the first program to achieve dan (master) level in Go. [[Gelly, Silver 2011]](https://www.sciencedirect.com/science/article/pii/S000437021100052X)\n",
    "- **MCTS** requires a **lot of simulations** to sample several pairs of state and action\n",
    "- To **speed up** this estimate we can introduce a **bias**\n",
    "- We will **update** the pair $\\langle s, a \\rangle$ **with** the **reward** obtained **using** $a$ **from** any state in the **subtree** of $s$ (AMAF heuristic)\n",
    "![rave](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/rave.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79943ab4",
   "metadata": {},
   "source": [
    "RAVE shares the value of actions across subtrees of the search tree. \n",
    "\n",
    "It is in this way that **RAVE** forms a very **fast** and **rough**\n",
    "estimate of the action value, whereas the value estimated by **Monte-Carlo Tree Search** is **slower** **but** more **accurate**.\n",
    "\n",
    "It is possible to **combine** these two value estimates in a principled fashion\n",
    "\n",
    "There are multiple ways to weight these two values. In particular, we will use **UCT-RAVE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b0440f",
   "metadata": {},
   "source": [
    "Now we are going to implement UCT-RAVE.\n",
    "In order to keep track of the action values taken in the subtree we will need to override several functions of the MonteCarloTreeSearchNode class.\n",
    "\n",
    "We will highlight the biggest conceptual differences in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "539f0487",
   "metadata": {
    "id": "539f0487"
   },
   "outputs": [],
   "source": [
    "class RAVEMonteCarloTreeSearchNode(MonteCarloTreeSearchNode):\n",
    "    '''Introduce RAVE optimization to MCTS'''\n",
    "    def __init__(self, state, n_simulations=100, parent=None, parent_action=None, k=3, gamma=0.78):\n",
    "        super().__init__(state, n_simulations, parent,parent_action, gamma=gamma)\n",
    "        self.average_reward_rave = 0\n",
    "        self.number_of_visits_rave = 0\n",
    "        self.k = k\n",
    "        # We need to override the evaluate function to use the rave value in rendering\n",
    "        self._evaluate_function = self._uct_rave\n",
    "\n",
    "    def _best_action(self, render:bool=False):\n",
    "        \"\"\"\n",
    "        Execute the four stages to find the best action from the current state.\n",
    "        \"\"\"\n",
    "        for _ in range(self.n_simulations):\n",
    "          node = self._tree_policy()\n",
    "          reward, taken_actions = node._rollout()\n",
    "          node._backpropagate(reward, taken_actions)\n",
    "        if not render:\n",
    "            return self._best_child(c_param=0.)\n",
    "        else:\n",
    "            return self._best_child(c_param=0.), self\n",
    "    \n",
    "    def _tree_policy(self):\n",
    "      \"\"\"\n",
    "      Selects node to run rollout.\n",
    "      \"\"\"\n",
    "      current_node = self\n",
    "      while not current_node._is_terminal_node():\n",
    "\n",
    "          if current_node._is_leaf():\n",
    "              # Differently from what we did in Monte Carlo Tree Search, even if we do not select\n",
    "              # a certain child from the current node, we still need to create it so that \n",
    "              # we will be able to update its rave value in the backpropagation step.\n",
    "              current_node._create_all_children()\n",
    "              return current_node._best_child()\n",
    "          \n",
    "          current_node = current_node._best_child()\n",
    "      return current_node\n",
    "\n",
    "    def _is_leaf(self):\n",
    "        \"\"\"\n",
    "        Returns True if the node is a leaf node, False otherwise.\n",
    "        \"\"\"\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def _create_all_children(self):\n",
    "        \"\"\"\n",
    "        Create all possible child nodes for the current node.\n",
    "        \"\"\"\n",
    "        for action in self._untried_actions:\n",
    "            \n",
    "            new_state = deepcopy(self.state)\n",
    "            new_state.step(action)\n",
    "            obs, reward, done, trunc, info = new_state.last()\n",
    "            child_node = RAVEMonteCarloTreeSearchNode(state = new_state, n_simulations=self.n_simulations,\n",
    "                            parent=self, parent_action=action, k=self.k, gamma=self.gamma)\n",
    "            child_node.terminal_state = done\n",
    "            child_node.average_reward = reward\n",
    "            child_node.number_of_visits = 1\n",
    "            child_node.average_reward_rave = reward\n",
    "            child_node.number_of_visits_rave = 1\n",
    "            \n",
    "\n",
    "            self.children.append(child_node)\n",
    "\n",
    "    def _best_child(self, c_param=0.8):\n",
    "        \"\"\"\n",
    "        Select the best child from the children array according to the MC-RAVE formula.\n",
    "        :param c_param: Exploration parameter.\n",
    "        :return: The child with the highest MC-RAVE value.\n",
    "        \"\"\"\n",
    "        choices_weights = [self._uct_rave(child, c_param) for child in self.children] \n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "\n",
    "    def _uct_rave(self, child, c_param):\n",
    "        \"\"\"\n",
    "        Returns the UCT-RAVE value for a child node.\n",
    "        :param child: The child node for which the MC-RAVE value is calculated.\n",
    "        :param c_param: Exploration parameter.\n",
    "        :return: The UCT-RAVE value for the child node.\n",
    "        \"\"\"\n",
    "        \n",
    "        mc = float('inf') if child.number_of_visits == 0 else child.average_reward\n",
    "        rave = float('inf') if child.number_of_visits_rave == 0 else child.average_reward_rave\n",
    "        beta = np.sqrt(self.k / (3 * self.number_of_visits + self.k))\n",
    "        # mc_rave value which combines the mc and the rave values:\n",
    "        q_star = (1-beta)*mc + beta*rave\n",
    "        uct_rave =  float('inf') if (child.number_of_visits == 0 or self.number_of_visits == 0) else q_star + \\\n",
    "        c_param*np.sqrt( \\\n",
    "            np.log(self.number_of_visits)/ child.number_of_visits \\\n",
    "            )\n",
    "        return uct_rave\n",
    "\n",
    "    def _rollout(self):\n",
    "        \"\"\"\n",
    "        Conducts a rollout from the current node.\n",
    "        :return: a tuple containing the reward obtained and the actions taken during the rollout.\n",
    "        \"\"\"\n",
    "        current_rollout_state = deepcopy(self.state)\n",
    "\n",
    "        _reward = 0\n",
    "        step = 0\n",
    "        done = current_rollout_state.last()[2]\n",
    "        taken_actions = []\n",
    "        while not done:\n",
    "            possible_moves = self._get_legal_actions()\n",
    "\n",
    "            action = self._rollout_policy(possible_moves)\n",
    "            current_rollout_state.step(action)\n",
    "            obs, reward, done, trunc, info = current_rollout_state.last()\n",
    "            _reward += reward*(self.gamma**step)\n",
    "            # DIFFERENCE FROM MONTE CARLO TREE SEARCH: we need to keep track of the actions taken\n",
    "            taken_actions.append(action)\n",
    "            ##############################\n",
    "            step += 1\n",
    "        return _reward, taken_actions\n",
    "\n",
    "    def _backpropagate(self, reward, taken_actions):\n",
    "        \"\"\"\n",
    "        Backpropagates the reward obtained during the rollout to the parents of the current node.\n",
    "        :param reward: The reward obtained during the rollout.\n",
    "        :param taken_actions: The actions taken during the rollout.\n",
    "        \"\"\"\n",
    "        self.number_of_visits += 1\n",
    "        self.average_reward = self.average_reward + (1/self.number_of_visits)*(reward-self.average_reward)\n",
    "\n",
    "        # DIFFERENCE FROM MONTE CARLO TREE SEARCH: we need to update the rave estimates of the children\n",
    "        for action in taken_actions:\n",
    "            for child in self.children:\n",
    "                if child.parent_action == action:\n",
    "                    child._update_rave_estimate(reward)\n",
    "        ###############################################\n",
    "        \n",
    "        if self.parent is not None:\n",
    "            self.parent._backpropagate(reward, taken_actions)\n",
    "    \n",
    "    def _update_rave_estimate(self, reward):\n",
    "        \"\"\"\n",
    "        Updates the rave estimate of the current node.\n",
    "        :param reward: The reward obtained during the rollout.\n",
    "        \"\"\"\n",
    "        self.number_of_visits_rave += 1\n",
    "        self.average_reward_rave = self.average_reward_rave + (1/self.number_of_visits_rave)*(reward-self.average_reward_rave)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e316251",
   "metadata": {},
   "source": [
    "Now let's see RAVE in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be33ddba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step number: 14\n",
      "Action executed: pickup\n",
      "Names: | Values: | Average reward: | Number of visits:\n",
      "south| 0.657 | -1 | 1\n",
      "north| 0.657 | -1 | 1\n",
      "east| 0.657 | -1 | 1\n",
      "west| 0.657 | -1 | 1\n",
      "pickup| 8.133 | 1.356 | 73 <--\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 14, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = new_environment_creator(taxi_pos=(0,0),passenger_pos=(3,3))\n",
    "rave = RAVEMonteCarloTreeSearchNode(state = env, n_simulations = 20, k=1, gamma=0.78)\n",
    "rave.run_and_update(max_number_of_steps=1000, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25668105",
   "metadata": {},
   "source": [
    "As we can see, despite having access to the **same** amount of **data** **as MCTS**, RAVE is able to learn a **better policy** than the one learned by MCTS.\n",
    "\n",
    "This is due to the fact that RAVE is able to use the data in a **more efficient** way, by means of its **heuristic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299186ce",
   "metadata": {},
   "source": [
    "To make a more rigorous comparison, we will evaluate each of the algorithms for a variety of simulations. We redefine evaluate_policy to executes one of the algorithms: \"Monte Carlo Control\", \"Monte Carlo Tree Search\", \"RAVE\" for any given of number of simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac9868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(problem, algorithm, simulations, render:bool=True):\n",
    "    if not algorithm in [\"Monte Carlo Control\", \"Monte Carlo Tree Search\", \"RAVE\"]:\n",
    "        raise ValueError(\"Algorithm must be one among: Monte Carlo Control, Monte Carlo Tree Search, RAVE and NewRAVE\")\n",
    "      \n",
    "    env = new_environment_creator(taxi_pos=problem[\"taxi_pos\"], passenger_pos=problem[\"passenger_pos\"],domain_map=problem[\"domain_map\"])\n",
    "\n",
    "    if algorithm == \"Monte Carlo Control\":\n",
    "        # Monte Carlo Control\n",
    "        monte_carlo_control = MonteCarloControl(env, gamma=0.99, max_steps_per_episode=1_000)\n",
    "        step = monte_carlo_control.run_and_update(number_of_simulated_episodes=simulations, render=render)\n",
    "        return step\n",
    "\n",
    "    elif (algorithm==\"Monte Carlo Tree Search\"):\n",
    "        # Monte Carlo Tree Search\n",
    "        mcts = MonteCarloTreeSearchNode(state = env, n_simulations=simulations, gamma=0.78)\n",
    "        step = mcts.run_and_update(max_number_of_steps=1000, render=render)\n",
    "        return step\n",
    "    \n",
    "    elif (algorithm ==\"RAVE\"):\n",
    "        # RAVE\n",
    "        rave = RAVEMonteCarloTreeSearchNode(state = env, n_simulations = simulations, k=1, gamma=0.78)\n",
    "        step = rave.run_and_update(max_number_of_steps=1000, render=render)\n",
    "        return step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287cd82",
   "metadata": {},
   "source": [
    "Finally we plot the performance of the algorithms according to number of simulations.\n",
    "\n",
    "P.S. By default render is set to False to make the notebook run faster. If you want to see the agent in action, set render to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "518ebeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Control for 10 simulations\n",
      "Monte Carlo Tree Search for 10 simulations\n",
      "RAVE for 10 simulations\n",
      "Monte Carlo Control for 20 simulations\n",
      "Monte Carlo Tree Search for 20 simulations\n",
      "RAVE for 20 simulations\n",
      "Monte Carlo Control for 30 simulations\n",
      "Monte Carlo Tree Search for 30 simulations\n",
      "RAVE for 30 simulations\n",
      "Monte Carlo Control for 50 simulations\n",
      "Monte Carlo Tree Search for 50 simulations\n",
      "RAVE for 50 simulations\n",
      "Experiments' results:\n",
      "[[7.7, 7.0, 7.4, 7.6], [20.9, 13.1, 9.8, 10.0], [10.6, 9.5, 10.8, 9.4]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uncomment to execute the code (it takes around 16 minutes)\n",
    "\n",
    "problem={\"taxi_pos\":(0,0),\"passenger_pos\":(3,3),\"domain_map\":DEFAULT_MAP}\n",
    "results = plot_steps_per_simulations(\"new_comparison_mcc_vs_mcts_vs_rave.png\", problem, [\"Monte Carlo Control\", \"Monte Carlo Tree Search\", \"RAVE\"],[10,20,30,50],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06723efa",
   "metadata": {},
   "source": [
    "![comparison](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/new_comparison_mcc_vs_mcts_vs_rave.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c49ec5",
   "metadata": {},
   "source": [
    "Our previous hypothesis seems to be confirmed by the last plot.\n",
    "\n",
    "In summary:\n",
    "- **MCC** (Monte Carlo Control) is the **best** algorithm among these ones in terms of **performance** but it **requires** a **lot of data** to learn a good policy\n",
    "- **MCTS** (Monte Carlo Tree Search) is **scalable** and can be used in problems where the state-action space is too big to fit in memory, but it **does not always find the optimal policy**\n",
    "- **RAVE** (Rapid Action Value Estimation) is an **extension** of **MCTS** that **speeds up** the **learning** of the **action values** by **introducing a bias**. This leads to a **more efficient** **use of data** and to a **better policy** than the one learned by vanilla MCTS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0f9d3",
   "metadata": {},
   "source": [
    "# TODO 1\n",
    "\n",
    "In this exercise you will try to see how the algorithms behave in a harder problem.\n",
    "In this case we will bring the passenger **farther** away from the taxi, so that the agent will need to take **more actions** to achieve the goal.\n",
    "\n",
    "Please, follow the instructions below:\n",
    "\n",
    "1. Define a problem with the passenger location = (5,4), the taxi location = (0,0) and the map = DEFAULT_MAP\n",
    "2. Run plot_steps_per_simulations with this new problem for Monte Carlo Control, Monte Carlo Tree Search and RAVE with [10,20] simulations and 3 experiments\n",
    "3. Compare the performance of the algorithms in this new problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48097737",
   "metadata": {},
   "source": [
    "# TODO 2\n",
    "\n",
    "In this exercise you will try to see how the algorithms behave in a bigger map. In this case, despite the taxi being as close to the passenger as before, the **map will be bigger** and thus the **Monte Carlo Simulations** are likely to be longer, more expensive and **noisier**.\n",
    "\n",
    "Please, follow the instructions below:\n",
    "\n",
    "1. Define a problem with the taxi location = (2,0), the passenger location = (3,3) and the map = BIG_MAP\n",
    "2. Run plot_steps_per_simulations with this new problem for Monte Carlo Control, Monte Carlo Tree Search and RAVE with [10,30] simulations and 3 experiments\n",
    "3. Compare the performance of the algorithms in this new problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed4588b",
   "metadata": {},
   "source": [
    "# TODO 3 (Optional)\n",
    "\n",
    "In this exercise you will modify the way RAVE balances the two estimates. In particular you will need to modify the function with which we estimate the value of a node, so that the reliance on the RAVE value decays slower compared to what happens in RAVE with the same value of k.\n",
    "\n",
    "Please, follow the instructions below:\n",
    "\n",
    "1. Implement the TODO in the function _mc_rave of the class NewRAVENode\n",
    "2. Define a problem with the taxi location = (0,0), the passenger location = (3,3) and the map = DEFAULT_MAP\n",
    "3. Run plot_steps_per_simulations with this problem for NewRAVE with 20 simulations and 3 experiments\n",
    "4. Compare the performance of NewRAVE with the performance of Monte Carlo Control and Monte Carlo Tree Search and RAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "81bcc47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewRAVENode(RAVEMonteCarloTreeSearchNode):\n",
    "    def __init__(self, state, n_simulations=100, parent=None, parent_action=None, k=3, gamma=0.78):\n",
    "        super().__init__(state, n_simulations, parent,parent_action, k=k, gamma=gamma)\n",
    "    \n",
    "    def _uct_rave(self, child, c_param=0.8):\n",
    "        \"\"\"\n",
    "        Returns the UCT-RAVE value for a child node.\n",
    "        :param child: The child node for which the MC-RAVE value is calculated.\n",
    "        :param c_param: Exploration parameter.\n",
    "        :return: The UCT-RAVE value for the child node.\n",
    "        \"\"\"\n",
    "        mc = float('inf') if child.number_of_visits == 0 else child.average_reward\n",
    "        rave = float('inf') if child.number_of_visits_rave == 0 else child.average_reward_rave\n",
    "        \n",
    "        ##############################\n",
    "        \"\"\"\n",
    "        TODO: modify beta so that the reliance on the RAVE value decays slower compared\n",
    "            to what happens in RAVE with the same value of k.\n",
    "            Hint: Can we replace the squared root with a different function?\n",
    "        \"\"\"\n",
    "        beta = None\n",
    "        ##############################\n",
    "        q_star = (1-beta)*mc + beta*rave\n",
    "        uct_rave =  float('inf') if (child.number_of_visits == 0 or self.number_of_visits == 0) else q_star + \\\n",
    "        c_param*np.sqrt( \\\n",
    "            np.log(self.number_of_visits)/ child.number_of_visits \\\n",
    "            )\n",
    "        return uct_rave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72882e0d",
   "metadata": {},
   "source": [
    "Now we will extend evaluate_policy so that it will be able to evaluate also the NewRAVE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fffc8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(problem, algorithm, simulations, render:bool=False):\n",
    "    if not algorithm in [\"Monte Carlo Control\", \"Monte Carlo Tree Search\", \"RAVE\", \"NewRAVE\"]:\n",
    "        raise ValueError(\"Algorithm must be one among: Monte Carlo Control, Monte Carlo Tree Search, RAVE and NewRAVE\")\n",
    "      \n",
    "    env = new_environment_creator(taxi_pos=problem[\"taxi_pos\"], passenger_pos=problem[\"passenger_pos\"],domain_map=problem[\"domain_map\"])\n",
    "\n",
    "    if algorithm == \"Monte Carlo Control\":\n",
    "        # Monte Carlo Control\n",
    "        monte_carlo_control = MonteCarloControl(env, gamma=0.99, max_steps_per_episode=1_000)\n",
    "        step = monte_carlo_control.run_and_update(number_of_simulated_episodes=simulations, render=render)\n",
    "        return step\n",
    "\n",
    "    elif (algorithm==\"Monte Carlo Tree Search\"):\n",
    "        # Monte Carlo Tree Search\n",
    "        mcts = MonteCarloTreeSearchNode(state = env, n_simulations=simulations, gamma=0.78)\n",
    "        step = mcts.run_and_update(max_number_of_steps=1000, render=render)\n",
    "        return step\n",
    "    \n",
    "    elif (algorithm ==\"RAVE\"):\n",
    "        # RAVE\n",
    "        rave = RAVEMonteCarloTreeSearchNode(state = env, n_simulations = simulations, k=1, gamma=0.78)\n",
    "        step = rave.run_and_update(max_number_of_steps=1000, render=render)\n",
    "        return step\n",
    "    \n",
    "    elif (algorithm ==\"NewRAVE\"):\n",
    "        # NewRAVE\n",
    "        rave = NewRAVENode(state = env, n_simulations = simulations, k=1, gamma=0.78)\n",
    "        step = rave.run_and_update(max_number_of_steps=1000, render=render)\n",
    "        return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a0c65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eecc88c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'git+https://github.com/sarah-keren/multi-taxi'\"\n"
     ]
    }
   ],
   "source": [
    "!pip install -q 'git+https://github.com/sarah-keren/multi-taxi'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88a017c5",
   "metadata": {},
   "source": [
    "# Our Problem\n",
    "The problem that we will focus on today is to make a taxi pick up a passenger, without dropping it at its destination."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f954d24b",
   "metadata": {},
   "source": [
    "![Taxi_domain_show](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/Taxi_domain_show.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "503b615e",
   "metadata": {},
   "source": [
    "The only guidance that we will provide to our agent is a **reward function** that prizes the agent for picking up the passenger and that penalizes the agent for taking too many steps to reach the goal.\n",
    "\n",
    "In particular, the reward function is defined as follows:\n",
    "- +100 points for a successful drop-off\n",
    "- -1 point for every bad pick-up\n",
    "- -2 points for hitting an obstacle (e.g. a wall)\n",
    "- -1 for every step taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7716d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_taxi import Event\n",
    "\n",
    "customized_reward = {\n",
    "    Event.PICKUP: 100,\n",
    "    Event.BAD_PICKUP: -5,\n",
    "    Event.HIT_OBSTACLE: -2,\n",
    "    Event.STEP: -1\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb119675",
   "metadata": {},
   "source": [
    "# Our Tools\n",
    "\n",
    "## Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d2598e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multi_taxi.world.maps import DEFAULT_MAP, BIG_MAP\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from multi_taxi import multi_taxi_v0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d9818b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_environment_creator(taxi_pos, passenger_pos, \n",
    "                            num_taxis=1,num_passengers=1, \n",
    "                            pickup_only=True, can_see_other_taxi_info=False, \n",
    "                            domain_map=DEFAULT_MAP,reward_table=customized_reward):\n",
    "    \"\"\"\n",
    "    A helper function to setup a new environment with predefined taxi and passenger locations\n",
    "    \"\"\"\n",
    "    new_env = multi_taxi_v0.env(num_taxis=num_taxis, num_passengers=num_passengers, pickup_only=pickup_only, domain_map=domain_map,\n",
    "                                can_see_other_taxi_info=can_see_other_taxi_info,reward_table=reward_table, \n",
    "                                render_mode='human', allow_arrived_passengers_on_reset=False)\n",
    "    new_env.reset()\n",
    "\n",
    "    state = new_env.state()\n",
    "    state.taxis[0].location = taxi_pos\n",
    "    state.passengers[0].location = passenger_pos\n",
    "    new_env.unwrapped.set_state(state)\n",
    "\n",
    "    return new_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "083da092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1255: accumulated reward -21.40420320579739\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 1256, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_position = (0,0)\n",
    "passenger_position = (3,3)\n",
    "env = new_environment_creator(taxi_position, passenger_position)\n",
    "\n",
    "done = False\n",
    "step = 0\n",
    "gamma = 0.9\n",
    "accumulated_reward = 0\n",
    "while not done:\n",
    "    action = np.random.randint(5)\n",
    "    env.step(action)\n",
    "    obs, reward, done, trunc, info = env.last()\n",
    "    clear_output(wait=True)\n",
    "    accumulated_reward += reward*(gamma**step)\n",
    "    print(f\"step {step}: accumulated reward {accumulated_reward}\")\n",
    "    env.render()\n",
    "    step += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b2b713c",
   "metadata": {
    "id": "4b2b713c"
   },
   "source": [
    "![monte_carlo_control_schema](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/Monte_Carlo_Control_complete.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60b0f7b4",
   "metadata": {
    "id": "60b0f7b4"
   },
   "source": [
    "# Monte Carlo Control Algorithm\n",
    "![monte_carlo_control_algorithm](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/monte_carlo_control_algorithm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1929d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from tqdm import trange\n",
    "\n",
    "class MonteCarloControl:\n",
    "    def __init__(self, state, gamma:float = 0.9, train_episode_number:int=1_000,\n",
    "                 max_steps_per_episode:int = 1_000) -> None:\n",
    "        '''\n",
    "        :args state: environment with the initial state\n",
    "        :args gamma: discount factor\n",
    "        :args train_episode_number: number of episodes used to train the agent (it is only used in the function train)\n",
    "        :args max_steps_per_episode: maximum number of steps \n",
    "        '''\n",
    "        self.state = state\n",
    "        self.Q = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.N = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.visited = set()\n",
    "        self.episode_number = 0\n",
    "        self.gamma = gamma\n",
    "        self.train_episode_number = train_episode_number\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "    \n",
    "    def save(self, filename):\n",
    "        state = self.state.unwrapped.state()\n",
    "        taxi_position = state.taxis[0].location\n",
    "        passenger_position = state.passengers[0].location\n",
    "        json_data = {\n",
    "            'Q': self.Q,\n",
    "            'N': self.N,\n",
    "            'episode_number': self.episode_number,\n",
    "            'gamma': self.gamma,\n",
    "            'max_episode_number': self.max_episode_number,\n",
    "            'max_steps_per_episode': self.max_steps_per_episode,\n",
    "            'taxi_position': taxi_position,\n",
    "            'passenger_position': passenger_position\n",
    "        }\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(json_data, f)\n",
    "\n",
    "\n",
    "    def load(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "        self.Q = json_data['Q']\n",
    "        self.N = json_data['N']\n",
    "        self.episode_number = json_data['episode_number']\n",
    "        self.gamma = json_data['gamma']\n",
    "        self.max_episode_number = json_data['max_episode_number']\n",
    "        self.max_steps_per_episode = json_data['max_steps_per_episode']\n",
    "        taxi_position = json_data['taxi_position']\n",
    "        passenger_position = json_data['passenger_position']\n",
    "        state = self.env.unwrapped.state()\n",
    "        state.taxis[0].location = taxi_position\n",
    "        state.passengers[0].location = passenger_position\n",
    "        self.state.unwrapped.set_state(state)\n",
    "\n",
    "    def policy(self, state, epsilon = None, warn=False):\n",
    "        if epsilon is None:\n",
    "            epsilon = 1/np.sqrt(self.episode_number + 1)\n",
    "        else:\n",
    "            epsilon = epsilon\n",
    "        number_of_actions = len(self.state.unwrapped.get_action_map('taxi_0').values())\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(number_of_actions)\n",
    "        else:\n",
    "            # convert state to use as key in Q\n",
    "            state = state.unwrapped.state().taxis[0].location\n",
    "            if state in self.visited:\n",
    "                action = max(self.Q[state], key=self.Q[state].get)\n",
    "            else:\n",
    "                if warn:\n",
    "                    print('The action was not in Q! Chose randomly')\n",
    "                action = np.random.choice(number_of_actions)\n",
    "        return action\n",
    "    \n",
    "    def update(self, episode):\n",
    "        G = 0\n",
    "        for i, (state, action, reward) in enumerate(reversed(episode)):\n",
    "            G = (self.gamma**i) * G + reward\n",
    "            # convert state to use as key in Q\n",
    "            state = state.unwrapped.state().taxis[0].location\n",
    "            self.visited.add(state)\n",
    "            self.N[state][action] += 1\n",
    "            self.Q[state][action] += (G - self.Q[state][action]) / self.N[state][action]\n",
    "    \n",
    "    def train(self):\n",
    "        # Train the policy without taking steps in the actual experience\n",
    "        for _ in trange(self.train_episode_number):\n",
    "            self.episode_number += 1\n",
    "            current_episode_state = deepcopy(self.state)\n",
    "            episode = []\n",
    "            for _ in range(self.max_steps_per_episode):\n",
    "\n",
    "                action = self.policy(current_episode_state)\n",
    "                old_state = current_episode_state\n",
    "                current_episode_state.step(action)\n",
    "                obs, reward, done, trunc, info = current_episode_state.last()\n",
    "                episode.append((old_state, action, reward))\n",
    "                if done:\n",
    "                    break\n",
    "            self.update(episode)\n",
    "    \n",
    "    def run(self, starting_state=None, max_number_of_steps=100):\n",
    "        # Run the policy without improving it\n",
    "        if starting_state is not None:\n",
    "            current_state = deepcopy(starting_state)\n",
    "        else:\n",
    "            current_state = deepcopy(self.state)\n",
    "        for i in range(1,max_number_of_steps+1):\n",
    "            action = self.policy(current_state, epsilon=0)\n",
    "            current_state.step(action)\n",
    "            obs, reward, done, trunc, info = current_state.last()\n",
    "            clear_output(wait=True)\n",
    "            print(i)\n",
    "            current_state.unwrapped.render()\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    def step_and_update(self, number_of_simulated_episodes=100, max_number_of_steps=150):\n",
    "        # Train the policy for max_number_of_simulated_episodes and then take a step\n",
    "        current_state = deepcopy(self.state)\n",
    "        steps = 0\n",
    "        for i in range(1,max_number_of_steps+1):\n",
    "            steps = i\n",
    "            # TRAINING STEP\n",
    "            self.episode_number = 0\n",
    "            for _ in range(number_of_simulated_episodes):\n",
    "                # train on the current state\n",
    "                self.episode_number += 1\n",
    "                current_training_state = deepcopy(current_state)\n",
    "                episode = []\n",
    "                for _ in range(self.max_steps_per_episode):\n",
    "\n",
    "                    action = self.policy(current_training_state)\n",
    "                    old_state = deepcopy(current_training_state)\n",
    "                    current_training_state.step(action)\n",
    "                    obs, reward, done, trunc, info = current_training_state.last()\n",
    "                    episode.append((old_state, action, reward))\n",
    "                    if done:\n",
    "                        break\n",
    "                self.update(episode)\n",
    "            # ACTUAL STEP\n",
    "            action = self.policy(current_state)\n",
    "            old_state = current_state.unwrapped.state().taxis[0].location\n",
    "            current_state.step(action)\n",
    "            obs, reward, done, trunc, info = current_state.last()\n",
    "            time.sleep(1)\n",
    "            clear_output(wait=True)\n",
    "            print(i)\n",
    "#             print(f\"action executed:{action}\")\n",
    "#             print(f\"q values: {self.Q[old_state].items()}\")\n",
    "            current_state.unwrapped.render()\n",
    "            if done:\n",
    "                break\n",
    "        return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8aa1c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = new_environment_creator(taxi_pos=(0,0), passenger_pos=(3,3))\n",
    "monte_carlo_control = MonteCarloControl(env, gamma=0.99, max_steps_per_episode=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "734eecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[33mP\u001b[0m: : | : : : : : |\n",
      "| :\u001b[43m \u001b[0m: : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (4, 1), Engine: ON, Collided: False, Step: 100, ALIVE\n",
      "Passenger0-YELLOW: Location: (3, 3), Destination: (-1, -1)\n",
      "Env done: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# monte_carlo_control.train()\n",
    "# monte_carlo_control.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2fc5266d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 7, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte_carlo_control.step_and_update(number_of_simulated_episodes=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10cac94c",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d2b174d",
   "metadata": {
    "id": "4d2b174d"
   },
   "source": [
    "# The 4 steps of MCTS\n",
    "\n",
    "MCTS consists of four main steps: selection, expansion, simulation, and backpropagation.\n",
    "\n",
    "1. **Selection.** Select a leaf node using **tree policy**.\n",
    "2. **Expansion.** **Add children** to the selected leaf using unexplored actions\n",
    "3. **Rollout.** From the selected child **simulate** an **episode** using the **rollout policy**\n",
    "4. **Backpropagation**. **Update** the average **value** of the nodes starting **from** the selected **child** up **to** the **root** using the results of the rollout episode\n",
    "    - ATTENTION: No values are saved for the states and actions visited by the rollout policy beyond the tree! \n",
    "\n",
    "![mcts_4_steps](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/mcts_4_steps.png)\n",
    "\n",
    "MCTS **repeats** this cycle until **no time** is left (starting at the root node each time). **Finally**, MCTS **chooses** the **action** to make from the root node."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05be701a",
   "metadata": {
    "id": "05be701a"
   },
   "source": [
    "# Rollout policy and Tree policy\n",
    "\n",
    "* **Rollout policy**: a simple policy generating actions in the **simulated trajectories**\n",
    "\n",
    "* **Tree policy**: policy for traversing the tree and **selecting** a **child node** that is most promising according to a selection policy, which balances exploration and exploitation of the search space. \n",
    "    * examples : $\\epsilon$-greedy, UCT\n",
    "    \n",
    "    * $UCT = \\underbrace{Q}_{exploitation} + \\underbrace{C \\sqrt{\\frac{\\log(N)}{n}}}_{exploration}$\n",
    "   \n",
    "   Where: \n",
    "   - $Q$ is the average Q-value of the considered state-action pair\n",
    "   - $C$ is a constant that balances exploration and exploitation\n",
    "   - $N$ is the total number of times that the current node has been visited\n",
    "   - $n$ is the number of times that the considered child node has been visited\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fc385bf",
   "metadata": {
    "id": "4fc385bf"
   },
   "outputs": [],
   "source": [
    "class MonteCarloTreeSearchNode():\n",
    "    def __init__(self, state, n_simulations=100, parent=None, parent_action=None, gamma = 0.9, new_env_creator=new_environment_creator):\n",
    "        # environment holding the current state\n",
    "        self.state = state\n",
    "        # None for the root node, otherwise it is equal to the node it is derived from.\n",
    "        self.parent = parent\n",
    "        # None for the root node, otherwise it is equal to the action which its parent carried out.\n",
    "        self.parent_action = parent_action \n",
    "        # Contains all possible actions from the current node.\n",
    "        self.children = []\n",
    "        # Number of times current node is visited.\n",
    "        self.number_of_visits = 0 \n",
    "        self.average_reward = 0\n",
    "        # Set of all of the possible actions\n",
    "        self._untried_actions = self.get_legal_actions()\n",
    "        self.terminal_state = False\n",
    "        self.new_env_creator = new_env_creator\n",
    "        self.gamma = gamma\n",
    "        # Number of loops with the four stages\n",
    "        self.n_simulations = n_simulations\n",
    "    \n",
    "    def best_action(self):\n",
    "        \"\"\"\n",
    "        Execute the four stages to find the best action from the current state.\n",
    "        \"\"\"\n",
    "        for _ in range(self.n_simulations):\n",
    "            node = self._tree_policy()\n",
    "            reward = node.rollout()\n",
    "            node.backpropagate(reward)\n",
    "\n",
    "        return self.best_child(c_param=0.)\n",
    "\n",
    "    def _tree_policy(self):\n",
    "        \"\"\"\n",
    "        Select node from which we run the rollout.\n",
    "        \"\"\"\n",
    "        current_node = self\n",
    "        while not current_node.is_terminal_node():\n",
    "            # if there is an action that has not been tried yet, return the child node corresponding to this action\n",
    "            if not current_node.is_fully_expanded():\n",
    "                return current_node.expand()\n",
    "            # else select the best child node\n",
    "            else:\n",
    "                current_node = current_node.best_child()\n",
    "        return current_node\n",
    "\n",
    "    def is_terminal_node(self):\n",
    "        \"\"\"\n",
    "        This is used to check if the current node is terminal or not. \n",
    "        Terminal node is reached when the game is over.\n",
    "        \"\"\"\n",
    "        return self.terminal_state\n",
    "    \n",
    "    def is_fully_expanded(self):\n",
    "        \"\"\"\n",
    "        All the actions are poped out of _untried_actions one by one. \n",
    "        When it becomes empty, that is when the size is zero, it is fully expanded.\n",
    "        \"\"\"\n",
    "        return len(self._untried_actions) == 0\n",
    "\n",
    "    def expand(self):\n",
    "        \"\"\"\n",
    "        Select an action that has not been tried yet and return the corresponding child node.\n",
    "        \"\"\"\n",
    "        # select an action that has not been tried yet\n",
    "        action = self._untried_actions.pop()\n",
    "        \n",
    "        state = self.state.unwrapped.state()\n",
    "        taxi_pos = state.taxis[0].location\n",
    "        passenger_pos = state.passengers[0].location\n",
    "        new_env = self.new_env_creator(taxi_pos,passenger_pos)\n",
    "        new_env.step(action)\n",
    "        obs, reward, done, trunc, info = new_env.last()\n",
    "        child_node = MonteCarloTreeSearchNode(state = new_env, n_simulations=self.n_simulations,\n",
    "                                            parent=self, parent_action=action, new_env_creator= self.new_env_creator,   gamma=self.gamma)\n",
    "        child_node.terminal_state = done\n",
    "        \n",
    "        self.children.append(child_node)\n",
    "        return child_node\n",
    "    \n",
    "    def best_child(self, c_param=0.8):\n",
    "        \"\"\"\n",
    "        Once fully expanded, this function selects the best child out of \n",
    "        the children array. The first term in the formula corresponds to \n",
    "        exploitation and the second term corresponds to exploration.\n",
    "        \"\"\"\n",
    "        choices_weights = [self.uct(child, c_param) for child in self.children] \n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "    \n",
    "    def uct(self, child, c_param):\n",
    "        if self.number_of_visits == 0 or child.number_of_visits == 0:\n",
    "            return float(\"inf\")\n",
    "        return child.average_reward + \\\n",
    "                c_param*np.sqrt(np.log(self.number_of_visits)/child.number_of_visits)\n",
    "    \n",
    "    def rollout(self):\n",
    "        \"\"\"\n",
    "        From the current state, entire game is simulated till there is an \n",
    "        outcome for the game. This outcome of the game is returned.\n",
    "        \"\"\"\n",
    "        \n",
    "        state = self.state.env.unwrapped.state()\n",
    "        taxi_pos = state.taxis[0].location\n",
    "        passenger_pos = state.passengers[0].location\n",
    "        \n",
    "        current_rollout_state = self.new_env_creator(taxi_pos,passenger_pos)\n",
    "\n",
    "        _reward = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            possible_moves = self.get_legal_actions()\n",
    "\n",
    "            action = self.rollout_policy(possible_moves)\n",
    "            current_rollout_state.step(action)\n",
    "            obs, reward, done, trunc, info = current_rollout_state.last()\n",
    "            _reward += reward *(self.gamma ** step)\n",
    "            step += 1\n",
    "        return _reward\n",
    "    \n",
    "    def get_legal_actions(self): \n",
    "        ''' \n",
    "        Returns a list of all of the possible actions from current state.\n",
    "        '''\n",
    "        return list(self.state.unwrapped.get_action_map('taxi_0').values())\n",
    "    \n",
    "    def rollout_policy(self, possible_moves):\n",
    "        \"\"\"\n",
    "        Randomly selects a move out of possible moves.\n",
    "        \"\"\"\n",
    "        return possible_moves[np.random.randint(len(possible_moves))]\n",
    "\n",
    "    def backpropagate(self, reward):\n",
    "        \"\"\"\n",
    "        In this step all the statistics for the nodes are updated. \n",
    "        Untill the parent node is reached, the number of visits for \n",
    "        each node is incremented by 1.\n",
    "        \"\"\"\n",
    "        self.number_of_visits += 1.\n",
    "        self.average_reward = self.average_reward + (1/self.number_of_visits)*(reward-self.average_reward)\n",
    "        if self.parent:\n",
    "            self.parent.backpropagate(reward)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcb1efc4",
   "metadata": {
    "id": "bcb1efc4",
    "outputId": "edeee943-4cb1-4979-db44-16a26ecd5269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 1, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = new_environment_creator(taxi_pos=(0,0), passenger_pos=(3,3))\n",
    "root = MonteCarloTreeSearchNode(state = env, n_simulations = 20, gamma=0.78, new_env_creator = new_environment_creator)\n",
    "root.state.unwrapped.render()\n",
    "selected_node = root.best_action()\n",
    "step = 1\n",
    "while not selected_node.terminal_state:\n",
    "    selected_node = selected_node.best_action()\n",
    "    clear_output(wait=True)\n",
    "    print(step)\n",
    "    selected_node.state.env.render()\n",
    "    step += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3fe3ed3",
   "metadata": {
    "id": "e3fe3ed3"
   },
   "source": [
    "# Rapid Action Value (RAVE)\n",
    "- It created the first program to achieve dan (master) level in Go. [[Gelly, Silver 2011]](https://www.sciencedirect.com/science/article/pii/S000437021100052X)\n",
    "- **MCTS** requires a **lot of simulations** to sample several pairs of state and action\n",
    "- To **speed up** this estimate we can introduce a **bias**\n",
    "- We will **update** the pair $\\langle s, a \\rangle$ **with** the **reward** obtained **using** $a$ **from** any state in the **subtree** of $s$ (AMAF heuristic)\n",
    "![rave](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/rave.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19d57ed4",
   "metadata": {
    "id": "19d57ed4"
   },
   "source": [
    "- In order to balance these two estimates we will use the following formula\n",
    "$$\n",
    "\\tilde{Q}(s,a) = (1-\\beta(s,a))*Q(s,a) + \\beta(s,a)*RAVE(s,a)\n",
    "$$\n",
    "Where:\n",
    "- $\\tilde{Q}(s,a)$ is the final value attributed to the pair $\\langle s, a \\rangle$\n",
    "- $\\beta(s,a)$ is a parameter which dynamically balances these two estimates\n",
    "- $Q(s,a)$ is the average Q-value of $\\langle s, a \\rangle$\n",
    "- $RAVE(s,a)$ is the RAVE value of $\\langle s, a \\rangle$\n",
    "\n",
    "$$\\beta(s,a) = \\sqrt{\\frac{k}{3N(s,a)+k}}$$\n",
    "Where:\n",
    "- $N(s)$ is the number of times that we visited the current node\n",
    "- $k$ is an hyperparameter that sets the number of simulation at which $Q$ and $RAVE$ are given the same weight\n",
    "\n",
    "![beta](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/beta.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "539f0487",
   "metadata": {
    "id": "539f0487"
   },
   "outputs": [],
   "source": [
    "class RAVEMonteCarloTreeSearchNode(MonteCarloTreeSearchNode):\n",
    "    '''Introduce RAVE optimization to MCTS'''\n",
    "    def __init__(self, state, n_simulations=100, parent=None, parent_action=None, k=3, gamma=0.78, new_env_creator=new_environment_creator):\n",
    "        super().__init__(state, n_simulations, parent,parent_action, gamma=gamma, new_env_creator=new_env_creator)\n",
    "        self.average_reward_rave = 0\n",
    "        self.number_of_visits_rave = 0\n",
    "        self.k = k\n",
    "    \n",
    "    def best_action(self):\n",
    "        \"\"\"\n",
    "        Execute the four stages to find the best action from the current state.\n",
    "        \"\"\"\n",
    "        for _ in range(self.n_simulations):\n",
    "          node = self._tree_policy()\n",
    "          reward, taken_actions = node.rollout()\n",
    "          node.backpropagate(reward, taken_actions)\n",
    "\n",
    "        return self.best_child(c_param=0.)\n",
    "    \n",
    "    def _tree_policy(self):\n",
    "      \"\"\"\n",
    "      Selects node to run rollout.\n",
    "      \"\"\"\n",
    "      current_node = self\n",
    "      while not current_node.is_terminal_node():\n",
    "\n",
    "          if current_node._is_leaf():\n",
    "              # Differently from what we did in Monte Carlo Tree Search, even if we do not select\n",
    "              # a certain child from the current node, we still need to create it so that \n",
    "              # we will be able to update its rave value in the backpropagation step.\n",
    "              current_node._create_all_children()\n",
    "              return current_node.best_child()\n",
    "          \n",
    "          current_node = current_node.best_child()\n",
    "      return current_node\n",
    "\n",
    "    def _is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def _create_all_children(self):\n",
    "\n",
    "        for action in self._untried_actions:\n",
    "\n",
    "            state = self.state.unwrapped.state()\n",
    "            taxi_pos = state.taxis[0].location\n",
    "            passenger_pos = state.passengers[0].location\n",
    "            new_env = self.new_env_creator(taxi_pos,passenger_pos)\n",
    "            new_env.step(action)\n",
    "            obs, reward, done, trunc, info = new_env.last()\n",
    "            child_node = RAVEMonteCarloTreeSearchNode(state = new_env, n_simulations=self.n_simulations,\n",
    "                            parent=self, parent_action=action, new_env_creator=self.new_env_creator,\n",
    "                            k=self.k, gamma=self.gamma)\n",
    "            child_node.terminal_state = done\n",
    "\n",
    "            self.children.append(child_node)\n",
    "\n",
    "    def best_child(self, c_param=0.8):\n",
    "        choices_weights = [self.mc_rave(child, c_param) for child in self.children] \n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "\n",
    "    def mc_rave(self, child, c_param):\n",
    "        uct =  float('inf') if (child.number_of_visits == 0 or self.number_of_visits == 0) else child.average_reward + \\\n",
    "        c_param*np.sqrt( \\\n",
    "          np.log(self.number_of_visits)/ child.number_of_visits \\\n",
    "        )\n",
    "        beta = np.sqrt(self.k / (3 * self.number_of_visits + self.k))\n",
    "        monte_carlo_value = float('inf') if uct == float('inf') \\\n",
    "                  else (1-beta)*uct\n",
    "        rave_value = float('inf') if child.number_of_visits_rave == 0 \\\n",
    "                  else beta*child.average_reward_rave\n",
    "        return monte_carlo_value + rave_value\n",
    "\n",
    "    def rollout(self):\n",
    "        state = self.state.env.unwrapped.state()\n",
    "        taxi_pos = state.taxis[0].location\n",
    "        passenger_pos = state.passengers[0].location\n",
    "\n",
    "        current_rollout_state = self.new_env_creator(taxi_pos,passenger_pos)\n",
    "\n",
    "        _reward = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "        taken_actions = []\n",
    "        while not done:\n",
    "            possible_moves = self.get_legal_actions()\n",
    "\n",
    "            action = self.rollout_policy(possible_moves)\n",
    "            current_rollout_state.step(action)\n",
    "            obs, reward, done, trunc, info = current_rollout_state.last()\n",
    "            _reward += reward*(self.gamma**step)\n",
    "            # DIFFERENCE FROM MONTE CARLO TREE SEARCH: we need to keep track of the actions taken\n",
    "            taken_actions.append(action)\n",
    "            ##############################\n",
    "            step += 1\n",
    "        return _reward, taken_actions\n",
    "\n",
    "    def backpropagate(self, reward, taken_actions):\n",
    "        self.number_of_visits += 1\n",
    "        self.average_reward = self.average_reward + (1/self.number_of_visits)*(reward-self.average_reward)\n",
    "\n",
    "        # DIFFERENCE FROM MONTE CARLO TREE SEARCH: we need to update the rave estimates of the children\n",
    "        for action in set(taken_actions):\n",
    "            for child in self.children:\n",
    "                if child.parent_action == action:\n",
    "                    child.update_rave_estimate(reward)\n",
    "        ##############################\n",
    "        \n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(reward, taken_actions)\n",
    "    \n",
    "    def update_rave_estimate(self, reward):\n",
    "        self.number_of_visits_rave += 1\n",
    "        self.average_reward_rave = self.average_reward_rave + (1/self.number_of_visits_rave)*(reward-self.average_reward_rave)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "497f052b",
   "metadata": {
    "id": "497f052b",
    "outputId": "24cf45f3-6b44-4991-989c-beadbb6a2f1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 1, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = new_environment_creator(taxi_pos=(0,0),passenger_pos=(3,3))\n",
    "root = RAVEMonteCarloTreeSearchNode(state = env, n_simulations = 20, k=1, gamma=0.78)\n",
    "root.state.unwrapped.render()\n",
    "selected_node = root.best_action()\n",
    "step = 1\n",
    "while not selected_node.terminal_state:\n",
    "    selected_node = selected_node.best_action()\n",
    "    clear_output(wait=True)\n",
    "    print(step)\n",
    "    selected_node.state.env.render()\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac9868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(algorithm, simulations):\n",
    "    if not algorithm in [\"Monte Carlo Control\", \"Monte Carlo Tree Search\", \"RAVE\"]:\n",
    "        raise ValueError(\"Algorithm must be one among: Monte Carlo Control, Monte Carlo Tree Search and RAVE\")\n",
    "    \n",
    "    env = new_environment_creator(taxi_pos=(0,0), passenger_pos=(3,3))\n",
    "\n",
    "    if algorithm == \"Monte Carlo Control\":\n",
    "        # Monte Carlo Control\n",
    "        monte_carlo_control = MonteCarloControl(env, gamma=0.99, max_steps_per_episode=1_000)\n",
    "        step = monte_carlo_control.step_and_update(number_of_simulated_episodes=10)\n",
    "        return step\n",
    "\n",
    "    elif (algorithm==\"Monte Carlo Tree Search\"):\n",
    "        # Monte Carlo Tree Search\n",
    "        root = MonteCarloTreeSearchNode(state = env, n_simulations=simulations, gamma=0.78, new_env_creator=new_environment_creator)\n",
    "        root.state.unwrapped.render()\n",
    "        selected_node = root.best_action()\n",
    "        step = 1\n",
    "        while not selected_node.terminal_state:\n",
    "            selected_node = selected_node.best_action()\n",
    "            clear_output(wait=True)\n",
    "            print(step)\n",
    "            selected_node.state.env.render()\n",
    "            step += 1\n",
    "        return step\n",
    "    \n",
    "    elif (algorithm ==\"RAVE\"):\n",
    "        # RAVE\n",
    "        root = RAVEMonteCarloTreeSearchNode(state = env, n_simulations = simulations, k=1, gamma=0.78, new_env_creator=new_environment_creator)\n",
    "        root.state.unwrapped.render()\n",
    "        selected_node = root.best_action()\n",
    "        step = 1\n",
    "        while not selected_node.terminal_state:\n",
    "            selected_node = selected_node.best_action()\n",
    "            clear_output(wait=True)\n",
    "            print(step)\n",
    "            selected_node.state.env.render()\n",
    "            step += 1\n",
    "        return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5877ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_steps_per_simulations(algorithms, simulations_per_step, number_of_experiments):\n",
    "\n",
    "    data_to_plot = [[] for _ in range(len(algorithms))]\n",
    "\n",
    "    \n",
    "    for sim in simulations_per_step:\n",
    "        for i, algorithm in enumerate(algorithms): \n",
    "            print(f\"{algorithm} for {sim} simulations\")\n",
    "            steps=0\n",
    "            for _ in range(number_of_experiments):\n",
    "                steps += evaluate_policy(algorithm ,sim)\n",
    "            data_to_plot[i].append(steps/number_of_experiments)\n",
    "\n",
    "    print(\"Experiments' results:\")\n",
    "    print(data_to_plot)\n",
    "\n",
    "    _, ax = plt.subplots(1, 1, figsize=(12, 3))\n",
    "    ax.set_title(f\"Performance\")\n",
    "    for i, algorithm in enumerate(algorithms): \n",
    "        ax.plot(simulations_per_step, data_to_plot[i], '-o', label=algorithm)\n",
    "    ax.set_xlabel(\"Simulations per step\")\n",
    "    ax.set_ylabel(\"Steps\")\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    plt.savefig(\"performance.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return data_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53f88283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 1, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n",
      "Experiments' results:\n",
      "[[10.8, 10.9, 10.4, 12.9], [20.3, 12.4, 11.5, 11.4], [23.3, 12.0, 12.3, 11.8]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# results = plot_steps_per_simulations([\"Monte Carlo Control\", \"Monte Carlo Tree Search\", \"RAVE\"],[10,20,30,50],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae21e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "# import pickle\n",
    "\n",
    "# with open('results.pickle', 'wb') as handle:\n",
    "#     pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06723efa",
   "metadata": {},
   "source": [
    "![performance](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/performance.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ec0f9d3",
   "metadata": {},
   "source": [
    "# TODO 1\n",
    "1. Define a new function which calls new_environment_creator with passenger_location = (6,7) with the DEFAULT_MAP\n",
    "2. Create a new environment with this new function\n",
    "3. Initialize MCTS node by setting as environment this environment, by setting new_env_creator to be the function created in step 1. and by setting n_simulations to be 20\n",
    "4. Run MCTS\n",
    "5. Repeat step 4-5 with n_simulations to be 100\n",
    "6.Repeat the previous stepsforRAVE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48097737",
   "metadata": {},
   "source": [
    "# TODO 2\n",
    "1. Define a new function which calls new_environment_creator with taxi position = (2,0) and with domain_map=BIG_MAP\n",
    "2. Create a new environment with this new function\n",
    "3. Initialize MCTS node by setting as environment this environment, by setting new_env_creator to be the function created in step 1. and by setting n_simulations to be 20\n",
    "4. Run MCTS\n",
    "5. Repeat step 4-5 with n_simulations to be 100\n",
    "6.Repeat the previous stepsforRAVE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ed4588b",
   "metadata": {},
   "source": [
    "# TODO 3 (Optional)\n",
    "1. Define a new function which calls new_environment_creator with taxi position = (2,0) with passenger_location = (6,7) and with domain_map=BIG_MAP\n",
    "2. Create a new environment with this new function\n",
    "3. Initialize MCTS node by setting as environment this environment, by setting new_env_creator to be the function created in step 1. and by setting n_simulations to be 20\n",
    "4. Run MCTS\n",
    "5. Repeat step 4-5 with n_simulations to be 100\n",
    "6. Repeat the previous stepsforRAVE"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

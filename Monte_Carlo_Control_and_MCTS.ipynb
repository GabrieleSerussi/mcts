{
 "cells": [
  {
   "attachments": {
    "Taxi_domain_show.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "88a017c5",
   "metadata": {},
   "source": [
    "# Our Problem\n",
    "![Taxi_domain_show.png](attachment:Taxi_domain_show.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb119675",
   "metadata": {},
   "source": [
    "# Our Tools\n",
    "\n",
    "## Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eecc88c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'git+https://github.com/sarah-keren/multi-taxi'\"\n"
     ]
    }
   ],
   "source": [
    "!pip install -q 'git+https://github.com/sarah-keren/multi-taxi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d2598e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multi_taxi.world.maps import DEFAULT_MAP, BIG_MAP\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from multi_taxi import multi_taxi_v0\n",
    "from multi_taxi import Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d9818b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_reward = {\n",
    "    Event.PICKUP: 100,\n",
    "    Event.BAD_PICKUP: -1,\n",
    "    Event.HIT_OBSTACLE: -1,\n",
    "    Event.STEP: -1\n",
    "}\n",
    "# This variable will be used later in Monte Carlo Tree Search \n",
    "N_SIMULATIONS = 100\n",
    "def new_environment_creator(taxi_pos, passenger_pos, \n",
    "                            num_taxis=1,num_passengers=1, \n",
    "                            pickup_only=True, can_see_other_taxi_info=False, \n",
    "                            domain_map=DEFAULT_MAP,reward_table=customized_reward):\n",
    "    \"\"\"\n",
    "    A helper function to setup a new environment with predefined taxi and passenger locations\n",
    "    \"\"\"\n",
    "    new_env = multi_taxi_v0.env(num_taxis=num_taxis, num_passengers=num_passengers, pickup_only=pickup_only, domain_map=domain_map,\n",
    "                                can_see_other_taxi_info=can_see_other_taxi_info,reward_table=reward_table, \n",
    "                                render_mode='human', allow_arrived_passengers_on_reset=False)\n",
    "    new_env.reset()\n",
    "\n",
    "    state = new_env.state()\n",
    "    state.taxis[0].location = taxi_pos\n",
    "    state.passengers[0].location = passenger_pos\n",
    "    new_env.unwrapped.set_state(state)\n",
    "\n",
    "    return new_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "083da092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 475, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_position = (0,0)\n",
    "passenger_position = (3,3)\n",
    "env = new_environment_creator(taxi_position, passenger_position)\n",
    "\n",
    "done = False\n",
    "step = 0\n",
    "while not done:\n",
    "    action = np.random.randint(5)\n",
    "    env.step(action)\n",
    "    obs, reward, done, trunc, info = env.last()\n",
    "    clear_output(wait=True)\n",
    "    print(step)\n",
    "    env.render()\n",
    "    step += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b2b713c",
   "metadata": {
    "id": "4b2b713c"
   },
   "source": [
    "![monte_carlo_control_schema](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/Monte_Carlo_Control_complete.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60b0f7b4",
   "metadata": {
    "id": "60b0f7b4"
   },
   "source": [
    "# Monte Carlo Control Algorithm\n",
    "![monte_carlo_control_algorithm](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/monte_carlo_control_algorithm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1929d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "class MonteCarloControl:\n",
    "    def __init__(self, state, gamma:float = 0.9, max_episode_number:int=10_000,\n",
    "                 max_episode_length:int = 1_000) -> None:\n",
    "        self.state = state\n",
    "        self.Q = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.N = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.visited = set()\n",
    "        self.episode_number = 0\n",
    "        self.gamma = gamma\n",
    "        self.max_episode_number = max_episode_number\n",
    "        self.max_episode_length = max_episode_length\n",
    "    \n",
    "    def save(self, filename):\n",
    "        state = self.state.unwrapped.state()\n",
    "        taxi_position = state.taxis[0].location\n",
    "        passenger_position = state.passengers[0].location\n",
    "        json_data = {\n",
    "            'Q': self.Q,\n",
    "            'N': self.N,\n",
    "            'episode_number': self.episode_number,\n",
    "            'gamma': self.gamma,\n",
    "            'max_episode_number': self.max_episode_number,\n",
    "            'max_episode_length': self.max_episode_length,\n",
    "            'taxi_position': taxi_position,\n",
    "            'passenger_position': passenger_position\n",
    "        }\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(json_data, f)\n",
    "\n",
    "\n",
    "    def load(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "        self.Q = json_data['Q']\n",
    "        self.N = json_data['N']\n",
    "        self.episode_number = json_data['episode_number']\n",
    "        self.gamma = json_data['gamma']\n",
    "        self.max_episode_number = json_data['max_episode_number']\n",
    "        self.max_episode_length = json_data['max_episode_length']\n",
    "        taxi_position = json_data['taxi_position']\n",
    "        passenger_position = json_data['passenger_position']\n",
    "        state = self.env.unwrapped.state()\n",
    "        state.taxis[0].location = taxi_position\n",
    "        state.passengers[0].location = passenger_position\n",
    "        self.state.unwrapped.set_state(state)\n",
    "\n",
    "    def policy(self, state, epsilon = None, warn=False):\n",
    "        if epsilon is None:\n",
    "            epsilon = 1/np.sqrt(self.episode_number + 1)\n",
    "        else:\n",
    "            epsilon = epsilon\n",
    "        number_of_actions = len(self.state.unwrapped.get_action_map('taxi_0').values())\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(number_of_actions)\n",
    "        else:\n",
    "            # convert state to use as key in Q\n",
    "            state = state.unwrapped.state().taxis[0].location\n",
    "            if state in self.visited:\n",
    "                action = max(self.Q[state], key=self.Q[state].get)\n",
    "            else:\n",
    "                if warn:\n",
    "                    print('The action was not in Q! Chose randomly')\n",
    "                action = np.random.choice(number_of_actions)\n",
    "        return action\n",
    "    \n",
    "    def train(self):\n",
    "        for _ in range(self.max_episode_number):\n",
    "            self.episode_number += 1\n",
    "            current_episode_state = deepcopy(self.state)\n",
    "            episode = []\n",
    "            for _ in range(self.max_episode_length):\n",
    "\n",
    "                action = self.policy(current_episode_state)\n",
    "                old_state = current_episode_state\n",
    "                current_episode_state.step(action)\n",
    "                obs, reward, done, trunc, info = current_episode_state.last()\n",
    "                episode.append((old_state, action, reward))\n",
    "                if done:\n",
    "                    break\n",
    "            self.update(episode)\n",
    "    \n",
    "    def update(self, episode):\n",
    "        G = 0\n",
    "        for i, (state, action, reward) in enumerate(reversed(episode)):\n",
    "            # convert state to use as key in Q\n",
    "            state = state.unwrapped.state().taxis[0].location\n",
    "            G = (self.gamma**i) * G + reward\n",
    "            self.visited.add(state)\n",
    "            self.N[state][action] = self.N[state][action] + 1\n",
    "            self.Q[state][action] = self.Q[state][action]+(G - self.Q[state][action]) / self.N[state][action]\n",
    "    \n",
    "    def run(self, starting_state=None, max_number_of_steps=100):\n",
    "        if starting_state is not None:\n",
    "            current_state = deepcopy(starting_state)\n",
    "        else:\n",
    "            current_state = deepcopy(self.state)\n",
    "        for i in range(1,max_number_of_steps+1):\n",
    "            action = self.policy(current_state, epsilon=0)\n",
    "            current_state.step(action)\n",
    "            obs, reward, done, trunc, info = current_state.last()\n",
    "            clear_output(wait=True)\n",
    "            print(i)\n",
    "            current_state.unwrapped.render()\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    def step_and_update(self, starting_state=None, max_number_of_steps=150, max_number_of_simulated_episodes=100):\n",
    "        if starting_state is not None:\n",
    "            current_state = deepcopy(starting_state)\n",
    "        else:\n",
    "            current_state = deepcopy(self.state)\n",
    "        steps = 0\n",
    "        for i in range(1,max_number_of_steps+1):\n",
    "            steps = i\n",
    "            # TRAINING STEP\n",
    "            self.episode_number = 0\n",
    "            for _ in range(max_number_of_simulated_episodes):\n",
    "                # train on the current state\n",
    "                self.episode_number += 1\n",
    "                current_training_state = deepcopy(current_state)\n",
    "                episode = []\n",
    "                for _ in range(self.max_episode_length):\n",
    "\n",
    "                    action = self.policy(current_training_state)\n",
    "                    old_state = deepcopy(current_training_state)\n",
    "                    current_training_state.step(action)\n",
    "                    obs, reward, done, trunc, info = current_training_state.last()\n",
    "                    episode.append((old_state, action, reward))\n",
    "                    if done:\n",
    "                        break\n",
    "                self.update(episode)\n",
    "            # ACTUAL STEP\n",
    "            action = self.policy(current_state, warn=True)\n",
    "            old_state = current_state.unwrapped.state().taxis[0].location\n",
    "            current_state.step(action)\n",
    "            obs, reward, done, trunc, info = current_state.last()\n",
    "            time.sleep(1)\n",
    "            clear_output(wait=True)\n",
    "            print(i)\n",
    "#             print(f\"action executed:{action}\")\n",
    "#             print(f\"q values: {self.Q[old_state].items()}\")\n",
    "            current_state.unwrapped.render()\n",
    "            if done:\n",
    "                break\n",
    "        return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aa1c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = new_environment_creator((0,0), (3,3))\n",
    "monte_carlo_control = MonteCarloControl(env, gamma=0.99, max_episode_number=100, max_episode_length=1_000)\n",
    "#monte_carlo_control.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fc5266d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 7, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte_carlo_control.step_and_update(max_number_of_simulated_episodes=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10cac94c",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d2b174d",
   "metadata": {
    "id": "4d2b174d"
   },
   "source": [
    "# The 4 steps of MCTS\n",
    "\n",
    "MCTS consists of four main steps: selection, expansion, simulation, and backpropagation.\n",
    "\n",
    "1. **Selection.** Select a leaf node using **tree policy**.\n",
    "2. **Expansion.** **Add children** to the selected leaf using unexplored actions\n",
    "3. **Rollout.** From the selected child **simulate** an **episode** using the **rollout policy**\n",
    "4. **Backpropagation**. **Update** the average **value** of the nodes starting **from** the selected **child** up **to** the **root** using the results of the rollout episode\n",
    "    - ATTENTION: No values are saved for the states and actions visited by the rollout policy beyond the tree! \n",
    "\n",
    "![mcts_4_steps](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/mcts_4_steps.png)\n",
    "\n",
    "MCTS **repeats** this cycle until **no time** is left (starting at the root node each time). **Finally**, MCTS **chooses** the **action** to make from the root node."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05be701a",
   "metadata": {
    "id": "05be701a"
   },
   "source": [
    "# Rollout policy and Tree policy\n",
    "\n",
    "* **Rollout policy**: a simple policy generating actions in the **simulated trajectories**\n",
    "\n",
    "* **Tree policy**: policy for traversing the tree and **selecting** a **child node** that is most promising according to a selection policy, which balances exploration and exploitation of the search space. \n",
    "    * examples : $\\epsilon$-greedy, UCB \n",
    "    \n",
    "    * $UCB = \\underbrace{Q}_{exploitation} + \\underbrace{C \\sqrt{\\frac{\\log(N)}{n}}}_{exploration}$\n",
    "   \n",
    "   Where: \n",
    "   - $Q$ is the average Q-value of the considered state-action pair\n",
    "   - $C$ is a constant that balances exploration and exploitation\n",
    "   - $N$ is the total number of times that the current node has been visited\n",
    "   - $n$ is the number of times that the considered child node has been visited\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff3a0e9f",
   "metadata": {
    "id": "ff3a0e9f"
   },
   "outputs": [],
   "source": [
    "customized_reward = {\n",
    "    Event.PICKUP: 100,\n",
    "    Event.BAD_PICKUP: -1,\n",
    "    Event.HIT_OBSTACLE: -1,\n",
    "    Event.STEP: -1\n",
    "}\n",
    "N_SIMULATIONS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1501d8c8",
   "metadata": {
    "id": "1501d8c8"
   },
   "outputs": [],
   "source": [
    "def new_environment_creator(taxi_pos, passenger_pos, \n",
    "                            num_taxis=1,num_passengers=1, \n",
    "                            pickup_only=True, can_see_other_taxi_info=False, \n",
    "                            domain_map=DEFAULT_MAP,reward_table=customized_reward):\n",
    "    \"\"\"\n",
    "    A helper function to setup a new environment with predefined taxi and passenger locations\n",
    "    \"\"\"\n",
    "    new_env = multi_taxi_v0.env(num_taxis=num_taxis, num_passengers=num_passengers, pickup_only=pickup_only, domain_map=domain_map,\n",
    "                                can_see_other_taxi_info=can_see_other_taxi_info,reward_table=reward_table, \n",
    "                                render_mode='human', allow_arrived_passengers_on_reset=False)\n",
    "    new_env.reset()\n",
    "\n",
    "    state = new_env.state()\n",
    "    state.taxis[0].location = taxi_pos\n",
    "    state.passengers[0].location = passenger_pos\n",
    "    new_env.unwrapped.set_state(state)\n",
    "\n",
    "    return new_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fc385bf",
   "metadata": {
    "id": "4fc385bf"
   },
   "outputs": [],
   "source": [
    "class MonteCarloTreeSearchNode():\n",
    "    def __init__(self, state, n_simulations=N_SIMULATIONS, parent=None, parent_action=None, new_env_creator=new_environment_creator, gamma = 0.9):\n",
    "        self.state = state  #represents the state\n",
    "        self.parent = parent #None for the root node, otherwise it is equal to the node it is derived from.\n",
    "        self.parent_action = parent_action #None for the root node, otherwise it is equal to the action which it’s parent carried out.\n",
    "        self.children = [] #Contains all possible actions from the current node.\n",
    "        self.number_of_visits = 0 #Number of times current node is visited.\n",
    "        self.average_reward = 0\n",
    "        self._untried_actions = set(self.get_legal_actions()) #Represents the list of all possible actions. An action is a move which has to be carried out.\n",
    "        self.terminal_state = False\n",
    "        self.new_env_creator = new_env_creator\n",
    "        self.gamma = gamma\n",
    "        self.n_simulations = n_simulations\n",
    "    \n",
    "    def expand(self):\n",
    "        \"\"\"\n",
    "        From the present state, next state is generated depending on the action \n",
    "        which is carried out. In this step all the possible child nodes \n",
    "        corresponding to generated states are appended to the children array \n",
    "        and the child_node is returned. The states which are possible from the\n",
    "        present state are all generated and the child_node corresponding to this\n",
    "        generated state is returned.\n",
    "        \"\"\"\n",
    "        # selection\n",
    "        action = self._untried_actions.pop()\n",
    "        \n",
    "        state = self.state.unwrapped.state()\n",
    "        taxi_pos = state.taxis[0].location\n",
    "        passenger_pos = state.passengers[0].location\n",
    "        new_env = self.new_env_creator(taxi_pos,passenger_pos)\n",
    "        new_env.step(action)\n",
    "        obs, reward, done, trunc, info = new_env.last()\n",
    "        child_node = MonteCarloTreeSearchNode(new_env, new_env_creator= self.new_env_creator, parent=self, parent_action=action, gamma=self.gamma)\n",
    "        child_node.terminal_state = done\n",
    "        \n",
    "        self.children.append(child_node)\n",
    "        return child_node\n",
    "    \n",
    "    def is_terminal_node(self):\n",
    "        \"\"\"\n",
    "        This is used to check if the current node is terminal or not. \n",
    "        Terminal node is reached when the game is over.\n",
    "        \"\"\"\n",
    "        return self.terminal_state\n",
    "    \n",
    "    def rollout(self):\n",
    "        \"\"\"\n",
    "        From the current state, entire game is simulated till there is an \n",
    "        outcome for the game. This outcome of the game is returned.\n",
    "        \"\"\"\n",
    "        \n",
    "        state = self.state.env.unwrapped.state()\n",
    "        taxi_pos = state.taxis[0].location\n",
    "        passenger_pos = state.passengers[0].location\n",
    "        \n",
    "        current_rollout_state = self.new_env_creator(taxi_pos,passenger_pos)\n",
    "\n",
    "        _reward = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            possible_moves = self.get_legal_actions()\n",
    "\n",
    "            action = self.rollout_policy(possible_moves)\n",
    "            current_rollout_state.step(action)\n",
    "            obs, reward, done, trunc, info = current_rollout_state.last()\n",
    "            _reward += reward *(self.gamma ** step)\n",
    "            step += 1\n",
    "        return _reward\n",
    "    \n",
    "    def backpropagate(self, reward):\n",
    "        \"\"\"\n",
    "        In this step all the statistics for the nodes are updated. \n",
    "        Untill the parent node is reached, the number of visits for \n",
    "        each node is incremented by 1.\n",
    "        \"\"\"\n",
    "        self.number_of_visits += 1.\n",
    "        self.average_reward = self.average_reward + (1/self.number_of_visits)*(reward-self.average_reward)\n",
    "        if self.parent:\n",
    "            self.parent.backpropagate(reward)\n",
    "            \n",
    "    def is_fully_expanded(self):\n",
    "        \"\"\"\n",
    "        All the actions are poped out of _untried_actions one by one. \n",
    "        When it becomes empty, that is when the size is zero, it is fully expanded.\n",
    "        \"\"\"\n",
    "        return len(self._untried_actions) == 0\n",
    "    \n",
    "    def ucb1(self, child, c_param=2):\n",
    "        if self.number_of_visits == 0 or child.number_of_visits == 0:\n",
    "            return float(\"inf\")\n",
    "        return child.average_reward + \\\n",
    "                c_param*np.sqrt(2*np.log(self.number_of_visits)/child.number_of_visits)\n",
    "    \n",
    "    \n",
    "    def best_child(self, c_param=0.8):\n",
    "        \"\"\"\n",
    "        Once fully expanded, this function selects the best child out of \n",
    "        the children array. The first term in the formula corresponds to \n",
    "        exploitation and the second term corresponds to exploration.\n",
    "        \"\"\"\n",
    "        choices_weights = [self.ucb1(child, c_param) for child in self.children] \n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "    \n",
    "    def rollout_policy(self, possible_moves):\n",
    "        \"\"\"\n",
    "        Randomly selects a move out of possible moves. \n",
    "        This is an example of random playout.\n",
    "        \"\"\"\n",
    "        return possible_moves[np.random.randint(len(possible_moves))]\n",
    "    \n",
    "    def _tree_policy(self):\n",
    "        \"\"\"\n",
    "        Selects node to run rollout.\n",
    "        \"\"\"\n",
    "        current_node = self\n",
    "        while not current_node.is_terminal_node():\n",
    "\n",
    "            if not current_node.is_fully_expanded():\n",
    "                return current_node.expand()\n",
    "            else:\n",
    "                current_node = current_node.best_child()\n",
    "        return current_node\n",
    "    \n",
    "    def best_action(self):\n",
    "        \"\"\"\n",
    "        This is the best action function which returns the \n",
    "        node corresponding to best possible move. The step of expansion, \n",
    "        simulation and backpropagation are carried out by the code above.\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.n_simulations):\n",
    "            v = self._tree_policy()\n",
    "            reward = v.rollout()\n",
    "            v.backpropagate(reward)\n",
    "\n",
    "        return self.best_child(c_param=0.)\n",
    "    \n",
    "    def get_legal_actions(self): \n",
    "        '''\n",
    "        Modify according to your game or\n",
    "        needs. Constructs a list of all\n",
    "        possible actions from current state.\n",
    "        Returns a list.\n",
    "        '''\n",
    "        return list(self.state.unwrapped.get_action_map('taxi_0').values())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcb1efc4",
   "metadata": {
    "id": "bcb1efc4",
    "outputId": "edeee943-4cb1-4979-db44-16a26ecd5269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 1, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#N_SIMULATIONS = 20\n",
    "env = new_environment_creator((0,0), (3,3))\n",
    "root = MonteCarloTreeSearchNode(new_env_creator = new_environment_creator, state = env, n_simulations = 20, gamma=0.78)\n",
    "root.state.unwrapped.render()\n",
    "selected_node = root.best_action()\n",
    "step = 1\n",
    "while not selected_node.terminal_state:\n",
    "    selected_node = selected_node.best_action()\n",
    "    clear_output(wait=True)\n",
    "    print(step)\n",
    "    selected_node.state.env.render()\n",
    "    step += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3fe3ed3",
   "metadata": {
    "id": "e3fe3ed3"
   },
   "source": [
    "# Rapid Action Value (RAVE)\n",
    "- It created the first program to achieve dan (master) level in Go. [[Gelly, Silver 2011]](https://www.sciencedirect.com/science/article/pii/S000437021100052X)\n",
    "- **MCTS** requires a **lot of simulations** to sample several pairs of state and action\n",
    "- To **speed up** this estimate we can introduce a **bias**\n",
    "- We will **update** the pair $\\langle s, a \\rangle$ **with** the **reward** obtained **using** $a$ **from** any state in the **subtree** of $s$ (AMAF heuristic)\n",
    "![rave](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/rave.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19d57ed4",
   "metadata": {
    "id": "19d57ed4"
   },
   "source": [
    "- In order to balance these two estimates we will use the following formula\n",
    "$$\n",
    "\\tilde{Q}(s,a) = (1-\\beta(s,a))*Q(s,a) + \\beta(s,a)*RAVE(s,a)\n",
    "$$\n",
    "Where:\n",
    "- $\\tilde{Q}(s,a)$ is the final value attributed to the pair $\\langle s, a \\rangle$\n",
    "- $\\beta(s,a)$ is a parameter which dynamically balances these two estimates\n",
    "- $Q(s,a)$ is the average Q-value of $\\langle s, a \\rangle$\n",
    "- $RAVE(s,a)$ is the RAVE value of $\\langle s, a \\rangle$\n",
    "\n",
    "$$\\beta(s,a) = \\sqrt{\\frac{k}{3N(s,a)+k}}$$\n",
    "Where:\n",
    "- $N(s)$ is the number of times that we visited the current node\n",
    "- $k$ is an hyperparameter that sets the number of simulation at which $Q$ and $RAVE$ are given the same weight\n",
    "\n",
    "![beta](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/beta.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "539f0487",
   "metadata": {
    "id": "539f0487"
   },
   "outputs": [],
   "source": [
    "class RAVEMonteCarloTreeSearchNode(MonteCarloTreeSearchNode):\n",
    "    '''Introduce RAVE optimization to MCTS'''\n",
    "    def __init__(self, state, n_simulations=N_SIMULATIONS, parent=None, parent_action=None, new_env_creator=new_environment_creator, k=3, gamma=0.78):\n",
    "        super().__init__(state, n_simulations, parent,parent_action,new_env_creator, gamma=gamma)\n",
    "        self.average_reward_rave = 0\n",
    "        self.number_of_visits_rave = 0\n",
    "        self.k = k\n",
    "\n",
    "    def rave(self, reward):\n",
    "        self.number_of_visits_rave += 1.\n",
    "        self.average_reward_rave = self.average_reward_rave + (1/self.number_of_visits_rave)*(reward-self.average_reward_rave)\n",
    "\n",
    "    def ucb1(self, child, c_param=0.8):\n",
    "        ucb =  float('inf') if (child.number_of_visits == 0 or self.number_of_visits == 0) else child.average_reward + \\\n",
    "        c_param*np.sqrt( \\\n",
    "          2*np.log(self.number_of_visits)/ child.number_of_visits \\\n",
    "        )\n",
    "        beta = np.sqrt(self.k / (3 * self.number_of_visits + self.k))\n",
    "        first_term = float('inf') if ucb == float('inf') \\\n",
    "                  else (1-beta)*ucb\n",
    "        second_term = float('inf') if child.number_of_visits_rave == 0 \\\n",
    "                  else beta*child.average_reward_rave\n",
    "        return first_term + second_term\n",
    "  \n",
    "    def best_child(self, c_param=0.8):\n",
    "        choices_weights = [self.ucb1(child, c_param) for child in self.children] \n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "  \n",
    "    def best_action(self):\n",
    "        for _ in range(self.n_simulations):\n",
    "          v = self._tree_policy()\n",
    "          reward, taken_actions = v.rollout()\n",
    "          v.backpropagate(reward, taken_actions)\n",
    "        return self.best_child(c_param=0.)\n",
    "  \n",
    "    def rollout(self):\n",
    "        state = self.state.env.unwrapped.state()\n",
    "        taxi_pos = state.taxis[0].location\n",
    "        passenger_pos = state.passengers[0].location\n",
    "\n",
    "        current_rollout_state = self.new_env_creator(taxi_pos,passenger_pos)\n",
    "\n",
    "        _reward = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "        taken_actions = []\n",
    "        while not done:\n",
    "            possible_moves = self.get_legal_actions()\n",
    "\n",
    "            action = self.rollout_policy(possible_moves)\n",
    "            current_rollout_state.step(action)\n",
    "            obs, reward, done, trunc, info = current_rollout_state.last()\n",
    "            _reward += reward*(self.gamma**step)\n",
    "            taken_actions.append(action)\n",
    "            step += 1\n",
    "        return _reward, taken_actions\n",
    "\n",
    "    def backpropagate(self, reward, taken_actions):\n",
    "        self.number_of_visits += 1\n",
    "        self.average_reward = self.average_reward + (1/self.number_of_visits)*(reward-self.average_reward)\n",
    "    \n",
    "        for action in set(taken_actions):\n",
    "            for child in self.children:\n",
    "                if child.parent_action == action:\n",
    "                    child.rave(reward)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(reward, taken_actions)\n",
    "  \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def create_all_children(self):\n",
    "        if self.is_fully_expanded():\n",
    "            raise Exception('Node is already fully expanded')\n",
    "\n",
    "        for action in self._untried_actions:\n",
    "\n",
    "            state = self.state.unwrapped.state()\n",
    "            taxi_pos = state.taxis[0].location\n",
    "            passenger_pos = state.passengers[0].location\n",
    "            new_env = self.new_env_creator(taxi_pos,passenger_pos)\n",
    "            new_env.step(action)\n",
    "            obs, reward, done, trunc, info = new_env.last()\n",
    "            child_node = RAVEMonteCarloTreeSearchNode(new_env, parent=self, parent_action=action, gamma=self.gamma, k=self.k, new_env_creator=self.new_env_creator)\n",
    "            child_node.terminal_state = done\n",
    "\n",
    "            self.children.append(child_node)\n",
    "\n",
    "    def _tree_policy(self):\n",
    "      \"\"\"\n",
    "      Selects node to run rollout.\n",
    "      \"\"\"\n",
    "      current_node = self\n",
    "      while not current_node.is_terminal_node():\n",
    "\n",
    "          if not current_node.is_fully_expanded():\n",
    "              current_node.create_all_children()\n",
    "              return current_node.best_child()\n",
    "          \n",
    "          current_node = current_node.best_child()\n",
    "      return current_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "497f052b",
   "metadata": {
    "id": "497f052b",
    "outputId": "24cf45f3-6b44-4991-989c-beadbb6a2f1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "+-----------------------+\n",
      "| : |F: | : | : | : |F: |\n",
      "| : | : : : | : | : | : |\n",
      "| : : : : : : : : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: : | : : : : : |\n",
      "| : : : : : | : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "| | :G| | | :G| | | : | |\n",
      "+-----------------------+\n",
      "Taxi0-YELLOW: Fuel: inf, Location: (3, 3), Engine: ON, Collided: False, Step: 1, ALIVE\n",
      "Passenger0-YELLOW: Location: Taxi0 (3, 3), Destination: (-1, -1)\n",
      "Env done: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#N_SIMULATIONS = 20\n",
    "env = new_environment_creator(taxi_pos=(0,0),passenger_pos=(3,3))\n",
    "root = RAVEMonteCarloTreeSearchNode(state = env, n_simulations = 20, k=10, gamma=0.78)\n",
    "root.state.unwrapped.render()\n",
    "selected_node = root.best_action()\n",
    "step = 1\n",
    "while not selected_node.terminal_state:\n",
    "    selected_node = selected_node.best_action()\n",
    "    clear_output(wait=True)\n",
    "    print(step)\n",
    "    selected_node.state.env.render()\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fac9868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluate_policy(algo ,simulations):\n",
    "    N_SIMULATIONS = simulations\n",
    "    if (algo==\"mcts\"):   \n",
    "        env = new_environment_creator(taxi_pos=(0,0),passenger_pos=(3,3))\n",
    "        root = MonteCarloTreeSearchNode(state = env, n_simulations=simulations, gamma=0.78)\n",
    "        root.state.unwrapped.render()\n",
    "        selected_node = root.best_action()\n",
    "        step = 1\n",
    "        while not selected_node.terminal_state:\n",
    "            selected_node = selected_node.best_action()\n",
    "            clear_output(wait=True)\n",
    "            print(step)\n",
    "            selected_node.state.env.render()\n",
    "            step += 1\n",
    "        return step\n",
    "    elif (algo==\"RAVE\"):\n",
    "        env = new_environment_creator(taxi_pos=(0,0),passenger_pos=(3,3))\n",
    "        root = RAVEMonteCarloTreeSearchNode(state = env, k=10, gamma=0.78)\n",
    "        root.state.unwrapped.render()\n",
    "        selected_node = root.best_action()\n",
    "        step = 1\n",
    "        while not selected_node.terminal_state:\n",
    "            selected_node = selected_node.best_action()\n",
    "            clear_output(wait=True)\n",
    "            print(step)\n",
    "            selected_node.state.env.render()\n",
    "            step += 1\n",
    "        return step\n",
    "    elif algo == 'mcc':\n",
    "        env = new_environment_creator((0,0), (3,3))\n",
    "        monte_carlo_control = MonteCarloControl(env, gamma=0.99, max_episode_length=1_000)\n",
    "        step = monte_carlo_control.step_and_update(max_number_of_simulated_episodes=simulations, max_number_of_steps=150)\n",
    "        return step\n",
    "    else:\n",
    "        raise ValueError('Choose an algorithm among mcts, RAVE and mcc!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5877ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "def plot_steps_per_simulations(algo, simulations_per_step, repeat_evaluation):\n",
    "\n",
    "    data_to_plot = [[] for _ in range(len(algo))]\n",
    "\n",
    "    \n",
    "    for sim in simulations_per_step:\n",
    "        # sample random actions for all buildings\n",
    "        for i,alg in enumerate(algo): \n",
    "            print(f\"{alg} for {sim} simulations\")\n",
    "            steps=0\n",
    "            for _ in range(repeat_evaluation):\n",
    "                steps+=create_evaluate_policy(alg ,sim)\n",
    "            data_to_plot[i].append(steps/repeat_evaluation)\n",
    "\n",
    "    #steps=simulations_per_step[sim]\n",
    "    # collect the data\n",
    "\n",
    "    print(data_to_plot)\n",
    "    # plotting the data collected\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 3))\n",
    "    ax.set_title(f\"steps taken for algorithm {algo}\")\n",
    "    for i, _ in enumerate(algo): \n",
    "        ax.plot(simulations_per_step, data_to_plot[i], '-o', label=algo[i])\n",
    "    ax.set_xlabel(\"simulations per step\")\n",
    "    ax.set_ylabel(\"Steps\")\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53f88283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_steps_per_simulations([\"mcc\", \"mcts\", \"RAVE\"],[10,20,30,50],10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06723efa",
   "metadata": {},
   "source": [
    "![benchmark](https://raw.githubusercontent.com/GabrieleSerussi/mcts/main/benchmark.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ec0f9d3",
   "metadata": {},
   "source": [
    "# TODO 1\n",
    "1. Define a new function which calls new_environment_creator with passenger_location = (6,7) with the DEFAULT_MAP\n",
    "2. Create a new environment with this new function\n",
    "3. Initialize MCTS node by setting as environment this environment, by setting new_env_creator to be the function created in step 1. and by setting n_simulations to be 20\n",
    "4. Run MCTS\n",
    "5. Repeat step 4-5 with n_simulations to be 100\n",
    "6. Repeat the previous steps for RAVE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48097737",
   "metadata": {},
   "source": [
    "# TODO 2\n",
    "1. Define a new function which calls new_environment_creator with taxi position = (2,0) and with domain_map=BIG_MAP\n",
    "2. Create a new environment with this new function\n",
    "3. Initialize MCTS node by setting as environment this environment, by setting new_env_creator to be the function created in step 1. and by setting n_simulations to be 20\n",
    "4. Run MCTS\n",
    "5. Repeat step 4-5 with n_simulations to be 100\n",
    "6. Repeat the previous steps for RAVE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ed4588b",
   "metadata": {},
   "source": [
    "# TODO 3 (Optional)\n",
    "1. Define a new function which calls new_environment_creator with taxi position = (2,0) with passenger_location = (6,7) and with domain_map=BIG_MAP\n",
    "2. Create a new environment with this new function\n",
    "3. Initialize MCTS node by setting as environment this environment, by setting new_env_creator to be the function created in step 1. and by setting n_simulations to be 20\n",
    "4. Run MCTS\n",
    "5. Repeat step 4-5 with n_simulations to be 100\n",
    "6. Repeat the previous steps for RAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cdb9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
